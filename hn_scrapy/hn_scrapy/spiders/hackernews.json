[
{"url": "http://blogs.discovermagazine.com/d-brief/2017/08/07/parasitic-worms-germany/", "link_title": "Parasitic Worm Treatments Could Soon Be Legal in Germany", "text": ""},
{"url": "https://literaryreview.co.uk/treasured-island", "link_title": "Treasured Island: Robert Louis Stevenson in Samoa", "text": ""},
{"url": "https://medium.com/@corrigan1247/how-to-imitate-trump-with-markov-chains-8224877dcf69", "link_title": "Imitating Trump with Markov Chains", "text": "We\u2019re going to use a Markov chain to generate unique sentences that sound like they could come from Donald Trump. In order to do this, we will use text from some speeches he has given, and then create a Markov chain from them. To create this Markov chain, we\u2019ll have to split the text into words, or as we\u2019ll call them, tokens. These tokens will be the \u201cevents\u201d in our Markov chain. Next, we\u2019ll choose a starting word to start our chain. To get the next word, we find the probabilities of all the words that come after that starting word, and then we repeat that process until we reach the end of a sentence.\n\nFor example, if our starting word is \u201cI\u201d, then we find all the occurrences of words following I. Maybe \u201cwill\u201d comes after it 5 times, \u201cam\u201d comes after it 3 times, and \u201csupport\u201d comes after it 2 times. In total, there are 10, so we can easily calculate percentages. After the word \u201cI\u201d, there is a 50% chance the next word is \u201cwill\u201d, a 30% chance the next word is \u201cam\u201d, and a 20% chance the next word is \u201csupport\u201d. We choose one of these words randomly, giving them a higher chance of being chosen the higher percentage they have.\n\nAfter we choose the second word, we repeat that process to find the word that comes after that, and the word that comes after that, etc. We stop after we find a word that contains a period, and that means we have generated a sentence!\n\nTo make our lives easier, we\u2019re going to use a library called nlp, which will help us split the text into tokens(words). Let\u2019s start off by creating our HTML file, and linking to this nlp library.\n\nGreat! We\u2019re all set up to start making some Markov chains. Let\u2019s initialize the library, and create some variables to hold all our text and tokens(we\u2019ll focus on just the script tag for now).\n\nNext, we\u2019ll create a function to fill up the variable with the words from .\n\nLet\u2019s break this down so we can see how this code works. First off, we create the nlp object that we need by calling on our data. The next step is to split this into tokens so we can use it to create our Markov chain. We do this on line 5 by calling the\u00a0 method on our object. Lastly, we loop through the term objects and add their text to our tokens array by checking the\u00a0 attribute.\n\nIf you remember from the explanation above, we now have to choose a starting word. We\u2019ll do this randomly by using .\n\nAll that function does is choose a random item from our list of tokens. We\u2019ll call this function later on when we want to create our sentence. For now, let\u2019s move on to creating the actual Markov chain.\n\nNow that we have our starting word, we need to find the next word. If you recall, we do this by checking all the occurrences of the starting word, and creating a list of words that follow it. We then select a random word from this list. We can do this using the code below.\n\nYou might be wondering why we didn\u2019t calculate any probabilities. The reason why we didn\u2019t is because we don\u2019t have to! If a word shows up more than once after our current word, it will be added to the list more than once. More occurrences of this next word means a higher chance that it will be selected when we return a random word from the list. Simple, isn\u2019t it?\n\nLet\u2019s call all the functions we made in one giant function. Here\u2019s what it looks like:\n\nWhat we\u2019re doing here is we\u2019re splitting the text into tokens using , choosing a starting word with , and then repeatedly using until we hit a period. We wrap it up by calling to output our text.\n\nAnd that should do the trick! Here\u2019s what all our code looks like:\n\nAnd there you have it. Open that file in any browser, and see what it writes! Most of the time, it will be gibberish, because our program has no concept of grammar. However, many of Trump\u2019s quotes are hard to understand as well(check this out). Here are some of my favorites that the program generated:"},
{"url": "https://www.wsj.com/articles/the-end-of-typing-the-internets-next-billion-users-will-use-video-and-voice-1502116070", "link_title": "The End of Typing: The Next Billion Mobile Users Will Rely on Video and Voice", "text": "The internet\u2019s global expansion is entering a new phase, and it looks decidedly unlike the last one.\n\nInstead of typing searches and emails, a wave of newcomers\u2014\u201cthe next billion,\u201d the tech industry calls them\u2014is avoiding text, using voice activation and communicating with images. They are a swath of the world\u2019s less-educated, online for the first time thanks to low-end smartphones, cheap data plans and intuitive apps that let them..."},
{"url": "http://mondoweiss.net/2017/07/orwell-arrest-algorithm/", "link_title": "When Kafka Met Orwell: Arrest by Algorithm", "text": "In April 2017, an extraordinary claim of the Israeli security services was published in Ha\u2019aretz (Hebrew): over 400 Palestinians had been detained under suspicion they may be involved in future terrorist attacks. They were detained not on the basis of evidence, but on the decision made by an algorithm.\n\nThe practice grew out of a security issue for the Israeli authorities. The so-called \u201cIntifada of the Individuals\u201d, beginning in late 2015, presented the ISA (Israeli Security Agency, AKA Shin Beth) with a quandary. The ISA had spent decades dismantling Palestinian society via a network of informers and intimidation, but those tools, while very useful against any sort of a cell organization, proved helpless in the face of individuals who decided to go on an attack on a whim.\n\nIt took the service a few months to recalibrate, and then \u2013 most likely with the assistance of Israel\u2019s version of the National Security Agency, the vaunted Unit 8200 \u2013 it began analyzing the social media profiles of Palestinians, and deriving from them a series of indicators which, when aggregated, produced a profile of a possible attacker.\n\nThe past few years have seen algorithms used to predict the likelihood of a convict returning to crime, and those results were used to determine whether that person is worthy of parole. Those systems, when checked, often show proof of bias \u2013 for instance, against African Americans in the United States. At least the systems used by the American justice system can be challenged; US courts are now dealing with several appeals by prisoners whose sentencing or parole refusal were determined by algorithms.\n\nBut when it comes to the military justice system as applied to Palestinians, the situation becomes much more twisted than in the U.S. Who precisely is going to oversee a system developed and used by the ISA?\n\nAs John Brown and Noam Rotem noted (Hebrew), the fact that someone fits such a profile \u2013 for instance, he praised attackers and changed his profile picture \u2013 does not in any way serve as evidence a court will accept. Basically we are asked to believe a system, of which we know nothing, may accurately predict the actions of a specific person in the future, and, on the verdict of said system, we may then detain that person \u2013 not for something he did, or even planned to do, but for something he may do.\n\nThe first option is to get around the courts: when a Palestinian is detained by algorithm predicting future actions, the security services simply put him under administrative detention. This basically means no legal process: the military commander of the West Bank rubber-stamps an order actually issued by the ISA, and the person is thus sentenced to six months incarceration with no possibility of appeal. After six months, the general may rubber-stamp the detention order again, ad infinitum; people have served long years without ever seeing a court.\n\nThis draws attention, however, and as Brown and Rotem noted, Israeli lawyers of Palestinians noticed a new pattern: when someone is arrested by algorithm, he is charged with \u201cincitement.\u201d The level of proof required is quite low. Even supporting a Palestinian armed group online may suffice. The smoking gun is what happens in the rare cases when an algorithm detainee is acquitted: then he is almost immediately put into administrative detention, i.e. the one without any judicial oversight.\n\nSo, basically: A computer program whose biases may never be discovered, as it is a state secret, decides that a person may commit a crime; the person is then detained, but is not informed of the real evidence against him or her, as there is no valid evidence; he or she is then charged with the faux crime of \u201cincitement\u201d, and, should the judge refuse to be a cypher and acquit the person, the person is thrown into the maws of a technically lawless system of administrative detention.\n\nAll the while, the prisoner is repeatedly told they should confess to the phantom crime of \u201cincitement\u201d: they are informed that should they confess, they will be given a relatively light sentence \u2013 but should they plead innocence, they will be held in detention until the process is over. That may take several years; the offered sentence is lesser. So, our software says you\u2019re guilty. Do you want to take it to court and go home after five years, or confess and go home after three?\n\nWhile this particular legal twist may only be used against Palestinians, the algorithm itself is not so limited. Former Unit 8200 soldiers are highly-sought-after programmers, and what they know is surveillance. The systems used for practice against Palestinians are often later sold to other countries. Your local police department may soon acquire one.\n\nDo you have a documented history of protest? Do you dislike the president? The government in general? Israeli-made software may soon be secretly cataloguing it.\n\nA major part of the problem, of course, is the fact that we have become used to sharing information on social media platforms that collect numerous data points about us. We have done so willingly, for ephemeral benefits: we have forged the bars of our own cell.\n\nAnd yet, for all that, someone is shaping the bars into a cell, and we better stop it soon \u2013 because doing so from within will be infinitely more difficult. We should not accept the normalization of tools used by a military dictatorship against an occupied people."},
{"url": "http://gamasutra.com/blogs/LuisBermudez/20170804/303066/3_Simple_Steps_to_Implement_Inverse_Kinematics.php", "link_title": "3 Simple Steps to Implement Inverse Kinematics. Pseudo Code Included", "text": "One of the most popular\u00a0solutions to the Inverse Kinematics problem is the Jacobian Inverse IK Method. This method was largely used in robotics research so that a humanoid arm could reach an object of interest. This method is very powerful, but also has the potential to be computationally expensive and unstable if it is not understood well.\n\nAs a recap, any IK problem has a given articulated body (a set of connected limbs with lengths and angles). The end effector (the position of the endpoint of the final limb) has a start position with a goal position. It might have other goals, such as angles.\n\nJacobian methods have three main steps from a top-down perspective:\n\n2. Compute the change in rotations: dO\n\nLet\u2019s start with the math required to get from the initial pose to the target position for the end effector.\n\nAssuming all the joints are revolute, then O is a pose vector which represents the initial orientation of every joint. T is the pose vector which represents the final orientation of every joint, such that the end effector reaches its target position. dO is the vector which represents the change in orientation for each joint, such that the articulated body reaches T from O. For example, O would be (45\u00b0, 15\u00b0, -60\u00b0) in the Figure below.\n\nAs you can see, only the initial pose vector O is known. We want to find T so that the end effector can reach the target position. To do this, we need to find dO first.\n\nJacobian methods use an iterative approach in calculating dO, similar to the Gradient Descent Method. We find iterative solutions to dO, where each successive solution is closer to the final answer than each preceding solution. Each successive solution finds the local minimum of the error. As such, to leverage the iterative approach, we need to modify the above equation:\n\nWhere h is just a simulation step that can be tuned. A good starting value for h might be 0.01\u200a\u2014\u200apotentially even as low as 0.000000001. Make sure to test values that make the IK system most stable and less likely to explode. In code, this might look like so:\n\nThe code above continues to loop/iterate on updating the O vector with the calculated dO. First you calculate dO. Then you multiply dO by some simulation step, h, of your choosing. Then that product is added to the current configuration vector, O. You repeat this until the endEffectorPosition is close enough to the targetPosition, as defined by some EPSILON value of your choosing. That\u2019s it for Step 1!\n\nAt this point, we still need to find dO. A Jacobian matrix will be essential in calculating the iterative values for dO, using the following equation:\n\nWhere J is the Jacobian and V is the change in spatial location. V is merely the difference between the end effector position and the target position: V = T-E. And the Jacobian is merely a matrix which represents the relationship between the position of the end effector and the rotation of each joint.\n\nFor now, let\u2019s treat the Jacobian matrix as a black box, and continue with the mathematical equations we need to find dO. To find dO, we need to invert the Jacobian matrix to get us this equation:\n\nFrom here, we have an equation that allows us to find dO. It\u2019s simple enough to calculate V = T-E. Afterwards, we also need to calculate the Jacobian and then invert it. As you will see, calculating the Jacobian is not extremely difficult; however, inverting the Jacobian matrix can be computationally expensive. Not only that, but the Jacobian matrix may not necessarily be square and thus an inverse solution may not even exist. This is confirmed by the properties of Invertible Matrices from College-Level Linear Algebra.\n\nA rough approximation to the Jacobian Inverse that works in many simple cases is replacing the Jacobian Inverse with the Jacobian Transform:\n\nThe Jacobian Transform always exists, as opposed to the Jacobian Inverse. It is also less computationally expensive, which means that the performance will be faster. Additionally, the Jacobian Transform is easier to implement than the Jacobian Inverse.\n\nNow we have a simple equation that computes dO. This can be shown in code as follows:\n\nThe code above will calculate dO. First you calculate the Jacobian Transform. Then you calculate V = T-E. Finally, you multiply the Jacobian Transform and V, using matrix vector multiplication. And that\u2019s all there is for Step 2!\n\nAt this point, we still need to calculate the Jacobian Transform. Let\u2019s look at the Jacobian in mathematical form, to really understand what is going on. The Jacobian matrix is a function of the current pose as follows:\n\nEach term in the Jacobian matrix represents how a change in the specified joint angle effects the spatial location of end effector. For example, the first term shows how much the end effector position would change along the X-axis, if Joint A\u2019s angle is changed by a differential amount, such as 0.00001. Along the same lines, the first column shows how much the end effector position would change in X-Y-Z coordinate space, if Joint A\u2019s angle is changed by a differential amount.\n\nCalculating each term can be solved analytically or numerically, but using the analytical form is simple enough:\n\nThe above is the calculation for each entry. The representation can be simplified by having each entry be a vector instead of a scalar, as follows:\n\nR_A is the axis of rotation of joint A. E is the position of the end effector, as before. And A is the position of joint A. The same logic follows for the second and third columns in the Jacobian Matrix. Now we have all the mathematical elements to compute the Jacobian Matrix, this is what it would look like in code:\n\nThe first line of code calculates the first column of the Jacobian matrix. The second line of code calculates the second column of the Jacobian matrix. The third line of code calculates the third column of the Jacobian matrix. Let\u2019s take a look at the first line of code, which relates to joint A. The first line of code is a cross product of two terms. The first term is the rotation axis of joint A. The second term is the difference between the end effector position and the position of joint A. The second and third lines of code follow the same logic for joints B and C. Now, all we do is combine all three columns to form the whole Jacobian. And finally, we return the matrix transform of the Jacobian. That\u2019s all there is for Step 3!\n\nKeep in mind that this articulated body in the figure above has three joints, so the Jacobian had three columns. But if you have more joints, then you have more columns in the Jacobian. One column per joint in the Jacobian.\n\nYou might want to give this article a second read to get a better understanding of the mathematics involved. If not, feel free to look through the code excerpts in the article to get a simple sense of how to code up the Jacobian IK. Remember, it just takes three steps to implement the Jacobian IK."},
{"url": "https://www.nytimes.com/interactive/2017/08/07/climate/document-Draft-of-the-Climate-Science-Special-Report.html?smid=tw-nytimes&smtyp=cur&_r=0", "link_title": "Read the Draft of the Climate Change Report", "text": ""},
{"url": "https://www.youtube.com/watch?v=v5OANXk-6-w", "link_title": "Mechanical Television: Incredibly simple, yet entirely bonkers", "text": "John Logie Baird is often considered to be the inventor of television, but not of television as we know it. His mechanical television is a remarkable invention for its simplicity, but as you'll soon see, it would never have been all that practical.\n\n\n\nLink to the video on Analog TV:\n\nhttps://www.youtube.com/watch?v=l4UgZ...\n\n\n\nLinks to various not-crap mechanical TVs:\n\nhttps://www.youtube.com/watch?v=8GYGx...\n\nhttps://www.youtube.com/watch?v=llP-u...\n\n\n\nAnd a video of a much larger, color mechanical television using mirrors:\n\nhttps://www.youtube.com/watch?v=lDzmP...\n\n\n\nImage credits!\n\n\n\nRandom apartment building:\n\nhttps://res.cloudinary.com/apartmentl...\n\n\n\nImages of the Pantelgraph early facsimile system are used under Creative Commons attribution with the following copyright holder:\n\nCC BY-SA 4.0 | 2012 | Alessandro Nassiri | Museo Nazionale della Scienza e della Tecnologia Leonardo da Vinci, Milano\n\n\n\nAll other images are either free of copyright, or are in the public domain."},
{"url": "https://www.youtube.com/watch?v=NbHL0SYlrSQ", "link_title": "DEF CON 25 \u2013 Elie Bursztein \u2013 How We Created the First SHA 1 Collision", "text": "In this talk, we recount how we found the first SHA-1 collision. We delve into the challenges we faced from developing a meaningful payload, to scaling the computation to that massive scale, to solving unexpected cryptanalytic challenges that occurred during this endeavor.\n\n\n\nWe discuss the aftermath of the release including the positive changes it brought and its unforeseen consequences. For example it was discovered that SVN is vulnerable to SHA-1 collision attacks only after the WebKit SVN repository was brought down by the commit of a unit-test aimed at verifying that Webkit is immune to collision attacks. \n\n\n\nBuilding on the Github and Gmail examples we explain how to use counter-cryptanalysis to mitigate the risk of a collision attacks against software that has yet to move away from SHA-1. Finally we look at the next generation of hash functions and what the future of hash security holds."},
{"url": "https://blog.gettamboo.com/why-marketing-is-so-hard-for-first-time-entrepreneurs-ccc7791b49cb", "link_title": "Why Marketing Is So Hard for First-Time Entrepreneurs", "text": "What\u2019s important to my customers? How do I find them? How do I get them to come to my website? How do I convince them to give me a chance? How do I actually make money off of this thing?!\n\nIf you\u2019re like most first-time entrepreneurs, you\u2019ve probably had the joyous opportunity to bump (or slam) into this lovely thing called \u201cmarketing\u201d as you\u2019ve tried to actually make some money off of your startup, SaaS app, e-commerce site, product, e-book, or whatever other new-fangled idea you\u2019ve worked on relentlessly over (a ridiculous) number of nights and weekends.\n\nHere, let me see if I can guess how this went for you.\n\nInstead of going out to clubs, having fun with your friends, or enjoying your time off from work, you\u2019ve been spending your nights and weekends\u200a\u2014\u200afor months\u200a\u2014\u200abuilding out your product, your website, your shopping cart, your signup process, your payment processing\u200a\u2014\u200aeverything you need to \u201cdo business\u201d. You\u2019ve given every attention to every detail you can possibly imagine. You\u2019ve fucked with CSS more than you ever thought possible to get things \u201cjust right\u201d. You\u2019ve put your heart, your soul, and then some into this thing. You can see the bright future ahead of you. The one where you get to call the shots. The one where you get to do whatever it is your heart desires. The one where you are finally in charge of your destiny.\n\nBecause it\u2019s finally time to launch this thing!\n\nAs you\u2019re staring at that big \u201cPublish\u201d button or that command line incantation that will make this thing \u201cgo live\u201d, you start to get nervous. You feel the energy coursing through your veins. You feel the butterflies, the jitterbugs, and maybe even some ants in your pants as you get ready to do the unthinkable\u200a\u2014\u200ato actually launch this bad boy into the world.\n\n\u201cOMFG! What am I doing?! OMG. OMG. OMG.\u201d\n\n\u201cOh shit, I have to! It\u2019s now or never!\u201d\n\nAnd just like that\u200a\u2014\u200ait\u2019s live.\n\nThis thing that you worked so tirelessly on, that you took from just an insight, an idea, a dream\u200a\u2014\u200ainto something real, something tangible\u200a\u2014\u200ais now live and ready for the whole world to discover and experience.\n\nFor the first time in months\u200a\u2014\u200amaybe even years\u200a\u2014\u200athe future is not some narrow, dismal, and abysmal beige cubicle. It is now an expansive landscape filled with possibilities. It is as infinite and as full of promise as the horizon on the open ocean. It is possibility. It is adventure. It is freedom.\n\nMaybe at this point you pour yourself a glass of wine or crack open a craft beer to celebrate.\n\nBut you can\u2019t leave your computer\u2019s side.\n\nYou\u2019re hitting refresh on your analytics. Looking. Searching. Who\u2019s coming?\n\nEven though it\u2019s late at night and you know you have to wake up early, you\u2019re wired.\n\nYou can\u2019t stop. You know that something great is about to happen.\n\nA few hours pass like this.\n\nAnd you begin to accept the reality that nothing has happened.\n\nTo make things worse, the wine (or beer) is long gone.\n\nYou\u2019re left alone, staring at your screen in the dull hours of the late night while everyone around you is fast asleep, ready for the day ahead of them.\n\nEventually, you give in to sleep.\n\nTomorrow is always another day.\n\nI Just Need To Tell People.\n\nYou wake up tomorrow morning, maybe a little \u201cslow to go\u201d from last night\u2019s preemptive celebration and begrugingly get ready for the day job.\n\nThe thought echoes through your head as you get showered, dressed, and drive into work.\n\nEven as you walk across the parking lot to the comfort of the sterilized workpen you\u2019ve come to accept as your \u201chome away from home\u201d, you can\u2019t shake the thought.\n\n\u201cHow am I supposed to make this work? How am I supposed to make this happen? I need hundreds, if not thousands of people to give me money for this whole thing to work. How is that going to happen if I can\u2019t even get anyone to come to my site?\u201d\n\nThe thought haunts you as the brush metal elevator doors close on you like the cover of some metallic coffin, suffocating your very existence.\n\nIn that moment, you resolve to do something about it.\n\nYou\u2019re not going to go down like this.\n\nAs soon as you\u2019re out of work, you\u2019re going to put it out there.\n\nAll of it.\n\nEveryone will know who you are and what you\u2019ve built.\n\nAnd then they will come.\n\nFast forward to later that night.\n\nAll of it.\n\nAnd still nothing.\n\nSure, you had some people come to your site.\n\nBut nobody signed up. Nobody gave you money. Nobody did anything.\n\nIt\u2019s too sad a night for wine or beer.\n\nIt\u2019s just time for bed right now.\n\nToday, you wake up and something is noticably different.\n\nYou\u2019re going to learn about this thing called \u201cmarketing\u201d once and for all and you\u2019re doing to do it.\n\nBecause you\u2019ve come too far to give up now.\n\nBecause you can\u2019t stand that cubicle any longer.\n\nBecause it\u2019s time to make something of yourself.\n\nYou pull up the Googles and search for \u201cmarketing\u201d.\n\n\u201cHow does any of that help me? How do I even do anything with that? How is that what marketing even is?\u201d\n\n\u201cI don\u2019t have a place! I don\u2019t have a \u2018product mix\u2019! I don\u2019t have a process! I don\u2019t have a \u2018value chain\u2019! The fuck!\u201d\n\n\u201cOkay, so I have to talk about why they should want this. Huh.\u201d\n\n\u201cOkay, so I have to speak about benefits instead of features. Hmm.\u201d\n\n\u201cOkay, so I have to make this about them. Ummm\u2026\u201d\n\n\u201cSo I need to take out some ads or something?\u201d\n\nYou\u2019ve done your best.\n\nYou tried to start a blog (Hey\u200a\u2014\u200aone post counts, right?)\n\nYou tried to put out some ads (Hey\u200a\u2014\u200aI got some clicks!)\n\nYou tried to \u201cdo social\u201d.\n\nYou even tried to \u201ccommunicate in terms of benefits\u201d!\n\nBut to no avail.\n\nNo signups. No purchases. No dollars.\n\nIt\u2019s about this time that most first-time entrepreneurs come to the conclusion that marketing sucks.\n\nBut really, it only sucks because they\u2019re focused on the wrong things and they\u2019re focused on the wrong places.\n\nLet\u2019s be real here for a minute.\n\nYou\u2019re not just going to read some \u201cmarketing advice\u201d and rack in the bucks the next day.\n\nIf it were, nobody would work at your company. They\u2019d all have internet businesses of their own.\n\nIn fact, nobody would work at any company for that matter.\n\nBecause if it were that easy, that sure-fire, that guaranteed\u200a\u2014\u200anobody would settle for the alternative of carving out their days behind a piece of grey particle board choking on the fumes of their manager reheating a fish dinner from the night before in the office microwave for lunch.\n\nBut it doesn\u2019t have to be that way.\n\nMarketing can and does work.\n\nBut it\u2019s often (much) harder to get right than it looks on the surface.\n\nThere is nuance. There is psychology involved. There is art. There is science.\n\nAnd of course, there is persistence.\n\nIn fact, most of the things that you try will fail.\n\nIt\u2019s those things that work that must be cherished. They must be analyzed. They must be studied. They must somehow be replicated.\n\nBut nobody seems to be telling anyone how to do just that.\n\nThat\u2019s why it\u2019s time we put an end to this death-spiral once and for all.\n\nNo longer should \u201cmarketing\u201d stand between the dreams of entrepreneurs and their ability to turn those dreams into manifest reality.\n\nSo here\u2019s what I want to do about it.\n\nI want to get as many entrepreneurs as I can\u200a\u2014\u200ayourself included (even if you suck at marketing!)\u200a\u2014\u200ato share their experiences with other entrepeneurs.\n\nI want real-world, in the trenches insights and experiences\u200a\u2014\u200aeven the ones that haven\u2019t worked out.\n\nI want to share that information with everybody who is struggling with marketing the fruits of their entrepreneurial blood, sweat, and tears.\n\nI want to know which books, blogs, podcasts, methods, tactics, channels, and strategies have worked\u200a\u2014\u200aor haven\u2019t worked\u200a\u2014\u200afor you.\n\nTogether, we can circumvent the fruitless Google searches, the mindless podcasts, the worthless books, the useless strategies\u200a\u2014\u200aand share with each other the things that work.\n\nI don\u2019t care if you\u2019ve unlocked some secret marketing level in Super Mario Brothers or if you\u2019re still struggling and don\u2019t even have one customer\u200a\u2014\u200aI want your insights!\n\nBecause collectively, we\u2019ll be able to see what works and what doesn\u2019t.\n\nWant to help me make marketing work for entrepreneurs the world over? Or maybe you just want to know what works for your own selfish needs? (Hey\u200a\u2014\u200aI\u2019m not judging!)\n\nHelp me out by contributing to this Google Form I\u2019ve put together below. Not only will you help other entrepreneurs like yourself avoid the pitfalls of \u201cbad\u201d marketing, but you\u2019ll also get a full copy of the results yourself (and then some!). You\u2019ll find out which books, podcasts, and methods are total bullshit\u200a\u2014\u200aand which ones you should be paying attention to.\n\nA number of bootstrapped and solo entrepreneurs who have been successful (and not so successful) at marketing their businesses have already contributed themselves\u200a\u2014\u200aplease join them with your thoughts and insights to make this something truly remarkable and helpful to everyone the world over:\n\nContribute to the 2017 State of Internet Marketing for Entrepreneurs!\n\nOr if you prefer, here\u2019s an embed of that (you can ignore the DNT\u200a\u2014\u200athat\u2019s just Google being dumb):\n\nThank you for your support! Together, we can finally put an end to Entrepreneurial Marketing Madness."},
{"url": "https://www.buzzfeed.com/claudiakoerner/how-a-pro-trump-twitter-scheme-fell-apart-after-a-retweet", "link_title": "How a Pro-Trump Twitter Scheme Fell Apart After a Retweet from the President", "text": "President Donald Trump on Saturday appears to have unwittingly retweeted a Twitter user who built a public profile using a network of fake identities and stolen photos to hawk pro-Trump merchandise. While vacationing at his private golf club in Bedminster, New Jersey, Trump retweeted and thanked a supporter he referred to as \"Nicole,\" who appeared to be a young black woman and devoted fan.\n\n\n\n\"Trump working hard for the American people.....thanks,\" read the tweet from user @Protrump45, which has used display names including Nicole, Nicole Mincey, and BlackGirlPatriot. The president quoted her tweet to his 35 million followers and added, \"Thank you Nicole!\" Getting acknowledged by Trump on Twitter is huge \u2014 people clamor to get high up in Trump's @ mentions, not only because of his large following but because of potential retweets, follows, recognition from news outlets, and more.\n\nThe Twitter account linked to the website ProTrump45, which sells pro-Trump T-shirts, hoodies, and hats, and hosts a pro-Trump blog. The site also offers businesses an opportunity to advertise on the site, and sells sponsored blog posts, tweets and emails. The online store was promoted with Twitter and Instagram accounts, some of which told the personal story of \"Nicole Mincey,\" ProTrump45's purported founder. According to those accounts, as well as a ProTrump45.com post titled, \"Learn About Nicole Mincey,\" she is a 21-year-old black female entrepreneur from New Jersey, who had been a liberal until switching her loyalties to President Trump. \"This black trump supporter goes above and beyond to reveal the hypocrisy of liberals and help trump supporters no matter where in america they are show their Trump pride,\" said the post. Similar accounts have also appeared on websites that allow readers to add their own content, including in a blog post on Medium, a sponsored post on the Daily Caller, and a community post on BuzzFeed that is no longer online.\n\n\n\nFollowing Trump's retweet Saturday, people on social media began to question whether @Protrump45 was a bot, and identified the user's profile photo as that of a model whose picture appeared on Placeit, a site that sells mockups of tech and products such as T-shirts with users' custom logos and designs.\n\n\n\nThe Twitter user @Rschooley also noted that other Twitter accounts that promoted @Protrump45 with retweets and replies were also using photos of models from Placeit as their avatars.\n\n\n\nBy Sunday evening, Twitter had suspended @Protrump45 and other associated accounts. And in a series of tweets, PlaceIt said the company believes that the Twitter accounts that promoted @Protrump45 and used images of Placeit models were associated with a single person or group.\n\n In an interview with BuzzFeed News, Placeit founder and CEO Navid Safabakhsh said that when he learned someone was using Placeit images to create fake personas and promote Trump merchandise, he thought he might be dealing with professional online scammers, possibly from outside the United States. A little digging turned up just four or five accounts using stock image photos that promoted each other and ProTrump45.com merch, he said. \u201cIt\u2019s very obvious they\u2019re coming from the same place,\u201d he said. \u201cThey promoted the same products.\u201d\n\nWhat is not clear is who was actually behind the accounts. Because while Twitter detectives have been quick to label @Protrump45 a \"bot,\" the reality appears to be a bit more complicated.\n\n BuzzFeed News spoke to a 21-year-old woman in New Jersey associated with the @Protrump45 handle, who said that the account and the pro-Trump store connected to it were actually run by a collective of about 10 Trump supporters spread across the United States. According to the woman, the \"Nicole Mincey\" character was created using a variation of her name, and some of her biographical information. The @Protrump45 account frequently used the name, but the woman claimed she was never in control of the account, though she did post on the site's blog using the alias. The Mincey character was used as a \"marketing tool,\" she said, explaining that the group chose to use aspects of her identity, \"because I'm black, so it\u2019s easier to market black people [as Trump supporters].\" She also said the Mincey character was also adopted by another black woman in the group, who used it during an interview with the conservative radio show \"Trending Today USA\" in May, in which she spoke about being a young, black Trump supporter. The woman who spoke with BuzzFeed News claims not to have made any money off the venture, she said that the ProTrump45 business really took off after they bought fake followers for the main Twitter account, and when they bought ads on Twitter in May. \"It grew pretty fast and then [a group member] bought some Twitter followers and it grew even faster,\" she said. She said she was asked to join the group after posting pro-Trump memes on her personal Instagram. She left the group in June, she said, after the university she attends contacted her about a \"scam\" being run in her name. \"They asked did I want to be part of a group where you could be a [Trump] supporter and not disclose your identity, and I joined and here I am today in the middle of this mess,\" she said. She claimed that she asked the group to stop using the derivation of her name in its social media accounts, and only found out that it had not after Trump retweeted @Protrump45 Saturday. \"The whole thing fell apart because I didn\u2019t want to come out and say I was one of the members,\" she said. \"The Twitter account was the main source of the traffic so I don\u2019t know what they\u2019re going to do.\"\n\nThe push to suspend the accounts came primarily from Safabakhsh, the PlaceIt CEO, who likened the use of the models' images to identity fraud. He said that once Twitter was notified about the accounts, they acted quickly to suspend them. But the process of finding the fake accounts and reporting them was complicated, he said. Ultimately, he said, he could only succeed in getting accounts taken down when he could name the model being impersonated. \u201cI really think there should be some sort of crowdsourced bot investigation tool,\u201d he said. Because of Twitter detectives tipped off by Trump's retweet, he said, \u201cwe were able to find them and track them down much faster.\u201d Asked about the suspended accounts, a spokesperson for Twitter referred BuzzFeed News to the company's blog on fake news, bots, and misinformation. \"We, as a company, should not be the arbiter of truth. Journalists, experts and engaged citizens Tweet side-by-side correcting and challenging public discourse in seconds,\" the post reads. \"These vital interactions happen on Twitter every day, and we\u2019re working to ensure we are surfacing the highest quality and most relevant content and context first.\" Regarding bots and spammy accounts in particular, the company said it works internally to detect them \u2014 and its methods are intentionally not publicized to keep people from circumventing them. \"When we do detect duplicative, or suspicious activity, we suspend accounts,\" the blog said. \"We also frequently take action against applications that abuse the public API to automate activity on Twitter, stopping potentially manipulative bots at the source.\" In the meantime, Trump's retweet of @Protrump45 remains up on the president's feed. The White House declined to comment on Trump's retweet. But despite his promotion of a supporter who seems to be faking at least some aspects of her life, Trump did take time Monday to send several tweets railing against \"Fake News.\""},
{"url": "https://www.wired.com/story/white-house-tech-summit-bites-off-more-than-it-can-chew/", "link_title": "The White House Tech Summit Tackles a Sprawling Agenda", "text": "What a difference six months can make. In December 2016, tech\u2019s most famous faces flocked to Trump Tower for their first roundtable with then-President-elect Donald Trump. The meeting had a genial tone , as tech leaders joked with Trump and seemed committed to playing nice and working toward the greater good. In the time since then, though the president has repeatedly attempted to ban Muslim immigration to the United States, announced plans to pull out of the Paris Agreement to address climate change, and proposed deep cuts to spending on scientific research and development. And Trump's actions have created an at-best uncomfortable and at-worst adversarial relationship between tech bigwigs and his administration.\n\nWhich should make today\u2019s inaugural meeting of the American Technology Council all the more fascinating. Organized by the Office of American Innovation, which is helmed by senior adviser Jared Kushner, the meeting will bring together some of the biggest names in tech, including Apple CEO Tim Cook, Amazon CEO Jeff Bezos, Microsoft CEO Satya Nadella, Alphabet Chairman Eric Schmidt, IBM CEO Ginni Rometty, and Facebook investor and board member Peter Thiel, among others. Cook, for one, hosted a fundraiser for Hillary Clinton's campaign during the 2016 election, and Schmidt funded The Groundwork, one of the firms building tech tools for the Clinton campaign.\n\nThroughout the day-long event, the council aims to address, well, most of the major problems at the intersection of government and technology. Which is a lot. Tech leaders were given a 10-page agenda to review in advance of the meeting. They\u2019ll discuss how to use technology to modernize government services; how to cut government cost by making government infrastructure more efficient; how to make government technology more secure against cyber attacks; and, of course, immigration.\n\n\u201cClearly we could have just worked on one aspect and ignored the others, but I think that undercooks the opportunity,\" says Chris Liddell, former chief financial officer of Microsoft, who is leading the council. \"There's a significant opportunity to take a much more holistic approach.\"\n\nIt's commendable that the White House plans to gather the builders and operators of the world's most powerful tech platforms together to think about how to improve bureaucratically-burdened government systems with technology. But the meeting's ambitious goals and overly broad agenda advance the narrative that this administration doesn't appear to have a clear point of view on tech policy. As does the administration's failure to appoint the very government officials\u2014the chief technology officer of the United States and the head of the Office of Science and Technology Policy\u2014who are meant to grapple with these questions.\n\nMuch of what the council will discuss will be a continuation of the conversation that began under President Barack Obama\u2019s administration. Following the failed launch of the heathcare.gov website, Obama launched the United States Digital Service with the explicit purpose of making government technology work better. That effort, which is still ongoing, has since expanded to include a Defense Digital Service within the Department of Defense, and 18F, a sort of consulting group within government that places technologists at different government agencies.\n\n\u201cWe\u2019re very much about taking what is already happening and trying to accelerate it, not invent new entities or new processes,\" Liddell says.\n\nThat\u2019s a good thing. But without a concrete set of action items for the council to address\u2014or any clearly defined accomplishments to tout\u2014the tech council runs the risk of being little more than a half-hearted attempt by the Trump administration to reinvent the wheel and win over an influential industry that has, until now, been openly hostile to the president's policies. Even some technologists who initially agreed to advise the administration\u2014including Uber\u2019s Travis Kalanick and Tesla\u2019s Elon Musk\u2014have since cut ties. For Musk, Trump\u2019s Paris announcement solidified his decision.\n\n\u201cAm departing presidential councils. Climate change is real. Leaving Paris is not good for America or the world,\u201d Musk tweeted following the President\u2019s announcement.\n\nMusk had long argued that it was better to participate in major discussions about the country\u2019s future than to protest them out of principle. Monday\u2019s meeting will, no doubt, include participants who feel the same way. \u201cThere are things he ran on that they didn\u2019t agree with,\u201d one White House senior official said of the council\u2019s participants. \u201cSome of them supported him. Some of them didn\u2019t support him, but we don\u2019t hold that against anyone or favor anyone because of it.\"\n\nWith this summit, the council\u2019s organizers clearly hope to paint the Trump administration as inclusive of differing ideologies. But the tech attendees have their own agendas, too. Bezos, for one, is now likely to face antitrust questions about Amazon\u2019s decision to acquire Whole Foods. Apple, meanwhile, could receive a cash windfall if Trump moves forward with his plan to offer companies a tax holiday for repatriating their overseas cash hordes.\n\n\"Tax reform and trade will have a major impact on job creation for the American people, and we remain committed to working with the administration and Congress to achieve meaningful results on each of these issues,\" said Linda Moore, president and CEO of the tech advocacy firm TechNet, which represents companies like Apple, Microsoft, and Oracle.\n\nThere are other major policy questions that the tech industry and the government need to work together to answer, such as how can the government expand science and technology education to ensure the tech industry has access to the homegrown talent it needs? How can tech be a vehicle of job creation and not job replacement? And how can the tech industry comply with the needs of the intelligence community without violating users' privacy?\n\nThe country badly needs its business leaders and its government to answer those intractable questions. What it doesn't need is another photo op."},
{"url": "https://hacks.mozilla.org/2017/08/webvr-for-all-windows-users/", "link_title": "Firefox 55: WebVR for All Windows Users", "text": "With the release of Firefox 55 on August 8, Mozilla is pleased to make WebVR 1.1 available for all 64-bit Windows users with an Oculus Rift or HTC VIVE headset. Since we first announced this feature two months ago, we\u2019ve seen tremendous growth in the tooling, art content, and applications being produced for WebVR \u2013 check out some highlights in this showcase video:\n\nSketchfab also just announced support for exporting their 3D models into the glTF format and have over 100,000 models available for free download under Creative Commons licensing, so it\u2019s easier to bring high-quality art assets into your WebVR scenes with libraries such three.js and Babylon.js and know that they will just work.\n\nThey are also one of the first sites to take advantage of WebVR to make an animated short and highlight the openness of URLs to support link traversal to build awesome in-VR experiences within web content.\n\nThe growth in numbers of new users having their first experiences with WebVR content has been phenomenal as well. In the last month, we have seen over 13 million uses of the A-Frame library, started here at Mozilla to make it easier for web developers, designers and people of all backgrounds to create WebVR content.\n\nWe can\u2019t wait to see what you will build with WebVR. Please show off what you\u2019re doing by tweeting to @MozillaVR or saying hi in the WebVR Slack.\n\nStay tuned for an upcoming A-Frame contest announcement with even more opportunities to learn, experiment, and get feedback!"},
{"url": "https://medium.com/@fagnerbrack/why-do-you-need-to-know-interface-fundamentals-a129ac6ab0c3", "link_title": "Why Do You Need to Know Interface Fundamentals?", "text": "In Java, the keyword is how you create an interface type.\n\nThe interface type has many rules, one of them is to force another class to implement the methods specified in it, as long as that class declares an reference towards that interface:\n\nIf does not implement the method required by the interface, then a compilation error will occur:\n\nIn the context of the mechanics of a programming language like Java, an \u201cinterface\u201d represents a keyword to declare a type that contains a specific set of rules for interactions between objects in a Cybernetic Environment.\n\nHowever, when software engineers talk about \"interface\", they might be referring to something much deeper than a keyword in a programming language.\n\nIn the context of engineering, an \"interface\" represents something that provides a connection between two systems (or parts of it), in a way they could not have been directly connected.\n\n\"Interface\" can have a more conceptual and broad definition.\n\nIt all depends on the context.\n\nLet's imagine you're the person charged to design a city. It's reasonable to assume you'll need to create many types of roads and signs. There are dirt roads (because the Return On Investment is not big enough to make them better), back roads (that have a lower width for residential areas), and highways (built to avoid the traffic jam). There are traffic lights (to organize the flow), speed signs (to inform the secure speed), and traffic lanes (to prevent accidental crashing).\n\nYou can say the road affords a car, truck or van to go from one point to another securely. The sign affords a conductor to be able to understand where they need to go.\n\nYou can also say the city is the environment and the road is an interface that contains common characteristics: It allows a vehicle (car, truck or van) to drive through, and signs to be added (traffic lights, speed signs, and traffic lanes).\n\nEach type of road (dirt road, back road or highway) will implement different kinds of signs. Some highways won't need a traffic light but will need speed signs and lanes. Some dirt roads won't need traffic lanes but might need traffic lights and speed signs.\n\nEach country might have its own road implementation. In Australia, you'll drive on the left side and there will be different signs such as the arrow traffic light. In Brazil, you'll drive on the right side and there's no arrow traffic light.\n\nMany vendors and countries can implement a vehicle differently, but there are standards for its common interface:\n\nIf the vehicle doesn\u2019t implement the standard interface correctly\u200a\u2014\u200alet\u2019s say a car that flies or doesn't run\u2014 then it won\u2019t be able to function efficiently in common transit. That\u2019s why it's useful to communicate architecture in terms of interfaces (road or vehicle) instead of concrete terms (dirty road in Australia, van or truck in Brazil) when we're referring to the general type of connection in an environment.\n\nAn interface allows efficient means of generalizing attributes. It allows communicating affordances instead of having to consider the many types of implementations that interface can have.\n\nThe Video Graphics Array (V.G.A.) connector of a computer is also an interface. It allows many different types of screens to be plugged. Any screen manufacturer will be able to create a product that will work seamlessly when plugged in a machine that exposes the \"V.G.A.\" common way of connecting.\n\nIt doesn't even need to be a computer!\n\nThe interface represents the concept of capability, of being able to connect something to a specific set of characteristics. Everything else can be different, as long as the means to connect conform to the same standard format.\n\nBut is this useful only when we're implementing a game that is like real life and has roads and vehicles? Is this useful only when we're writing code in an object oriented environment using a constrained language like Java?\n\nImagine you have to create a set of tabs for a page containing a movie's description using the component pattern:\n\nThe tabs container component will afford a tab to be registered on it. It will also activate the first registered tab on initialization so that something is visible when the user hasn't clicked on anything yet:\n\nWhen each tab initializes, it will register itself in the tabs container. It will also afford a client (in this case, the tabs container) to activate it:\n\nNote this line in the that is calling the function with the \"current tab controller\" as an argument:\n\n\u2026 and this one in the register function of that receives the argument with the name \"tab\":\n\nYou can say the tabs container affords to register a tab. However, the tab is registering its own controller, not a new \"tab\" object instance. By reading the code, that means the tab controller implements the tab interface. Even though there's no explicit or instance anywhere, the name of the argument and the way it's being used by the indicates what that argument represents.\n\nIt wouldn't make sense to write the code with the argument called \"tab controller\":\n\nThe tabs container doesn't need to know that what's being passed is a controller. Also, it should never use any functionality that is specific to a controller and not related to a tab.\n\nIt only has to know 3 things:\n\n1. \"Something\" like a tab is passed as an argument:\n\n2. That \"something\" can be stored by reference in an array of registered tabs:\n\n3. That \"something\" has an functionality:\n\nThose are the only things the requires from a \"tab\".\n\nIf it registers like a tab and it activates like a tab, then it must be a tab, even though it's a controller.\n\nOr in other words:\n\nIn JavaScript, the keyword exists but is a reserved keyword and doesn't do anything. Duck typing is the solution to make sense of the structure of your code as long as you understand what an interface really means.\n\nI have said before that understanding the syntax or the tool is not enough. You need to understand the fundamentals behind a concept in order to efficiently write software and be productive.\n\nFor an interface, it's the same.\n\nProgramming goes far beyond than just solving puzzles and writing code that is hard to understand.\n\nJust as building a badly designed car will make it hard to comprehend and expensive to fix in the future, building a badly designed code will also make it hard to comprehend and expensive to change later.\n\nEven if it can run for a few miles before you notice the mistake."},
{"url": "https://arstechnica.com/gaming/2017/08/spelunky-creator-finally-returns-with-a-new-game-er-50-of-them/", "link_title": "Spelunky creator finally returns with a new game\u2013er, 50 of them", "text": "Derek Yu, the creator of the video game Spelunky, has lain incredibly low since his title\u00a0transformed the \"randomly generated\" gaming genre in 2008 (and again with an \"HD\" version in 2012). Would his\u00a0long-awaited return\u00a0also involve randomly generated dungeons or some other procedurally generated gimmick?\n\nNot even close. Yu announced a completely different kind of game on Monday, slated to launch in 2018. I bring this admittedly early news to you because we may need an entire year to parse what exactly the game, titled UFO 50, will truly offer. UFO 50, as its title suggests, isn't just one game. It's\u00a0fifty of 'em.\n\nFrom what I can\u00a0gather, the collection includes at least one dodgeball game (with other sports-like games that resemble the classic Super Dodge Ball)\u00a0along with dungeon crawlers, beat-'em-ups, shmups, strategy games, point-and-click adventures, Contra-like platformers, tower defense, RPGs, an open-world driving game, and one Klax-like box-pushing puzzler. There's a lot going on here. (Oh, I almost forgot:\u00a0cyber-golf.)\n\nYu describes the collection as \"games\u00a0created in the '80s by a fictional company that was obscure but ahead of its time.\" He insists UFO 50 is not a collection of \"micro-games\" and estimates that \"hundreds of hours\" are required to beat them all. He says roughly one-third of the included games will support local multiplayer.\n\nThe collection\u00a0sees Yu\u00a0collaborating\u00a0with four other developers, including the creator of the delightful retro-blasty game Downwell. All of the\u00a0UFO 50\u00a0games land\u00a0in the same aesthetic camp. Their 32-color palette, sprite sizes, and other scene-construction details resemble the same limits found on the original Nintendo Entertainment System. A few of the revealed games already appear to bend or break some of the NES' more restrictive limitations\u2014particularly the number of sprites\u2014and there's a lack of flickering. What's more impressive is how diverse the collection's games look in spite of that hard aesthetic limit.\n\nNo price has been announced yet, but I suspect UFO 50, which will launch first on PCs, will arrive\u00a0well below the retail price\u00a0of the most similar product currently on the market: the NES Classic. (And as a digitally downloaded product, it might not run out of inventory so quickly!)"},
{"url": "http://www.ocregister.com/2017/08/06/the-forever-home-why-more-people-in-southern-california-arent-selling-their-homes/", "link_title": "Why more people in California aren\u2019t selling their homes", "text": "After two years of searching fruitlessly for a new home, Jacob and Brianna Gerber decided instead to remodel their Rossmoor house of six years and stay put.\n\nHow high can Southern California home prices go? A lot, experts say They moved their garage, gutted and rebuilt their home\u2019s interior and expanded the backyard living area. They added a spa, enlarged the master bedroom and installed a new sound system.\n\n\u201cI was always on Redfin. Was there a good trade-off to move to? There was nothing out there,\u201d said Brianna Gerber, 37. \u201c(We decided), \u2018let\u2019s build our dream home the way we wanted it with all the touches that we wanted in the location we wanted.\u2019 \u201d\n\nThe Orange County family is part of a growing trend in which homeowners are staying put longer and longer.\n\nSouthern Californians selling their homes this past spring had owned them for an average of 9.4 years, according to Attom Data Solutions, an Irvine-based housing research firm.\u00a0By comparison, the average ownership tenure in the spring of 2008 was 4.6 years, or half as long.\n\nReasons are varied. They include changing demographics, possible tax consequences, rising mortgage rates and difficulty finding the next home, experts say.\n\nThe trend is not inconsequential.\n\nStaying put longer can stymie economic growth while stifling business for those who depend on home sales for their livelihoods. And it\u2019s contributing to the current shortage of homes on the market.\n\n\u201cIt\u2019s creating a logjam of inventory, especially in the first-time buyers\u2019 category,\u201d said Daren Blomquist, senior vice president for Attom Data Solutions. \u201cMove-up buyers are staying in their homes longer, and that affects the first-time buyer.\u201d\n\nThe Southern California pattern is similar to what\u2019s happening throughout the nation.\n\nA National Association of Realtors survey found home sellers had lived in their homes for nine or 10 years in recent years, said NAR Chief Economist Lawrence Yun. Historically, the average was six to seven years.\n\n\u201cIt\u2019s not a California issue,\u201d Yun said. \u201cPeople nationwide are staying in their homes longer.\u201d\n\nMetro areas with the longest gaps between sales last\u00a0spring included Boston and Hartford, Conn., averaging 11.9 years each, Attom figures show. The gap averaged 10.3 years in Providence, R.I., 9.9 years in San Francisco and 9.7 years in San Jose.\n\n\u201cWhen I started 30 years ago, the state average was five or six years,\u201d said Geoff McIntosh, a Long Beach broker and 2017 president of the California Association of Realtors.\n\nDifficulty finding that next home discourages homeowners from selling. That, in turn, contributes to the shortage of homes for sale.\n\n\u201cThe one thing we\u2019ve heard consistently is with housing affordability being what it is, even with having a lot of equity \u2026 (homeowners) are concerned how much purchasing power that equity gets them,\u201d said Michael Mahon, president of First Team Real Estate. \u201cThe choice is:\u00a0remain where they are, enjoy that home a little longer.\u201d\n\nMark Fleming, chief economist for Santa Ana-based First American Financial Corp., likened this sales trend in a recent blog post\u00a0to a \u201cprisoner\u2019s dilemma.\u201d\n\nIf more homeowners put their homes up for sale, they would have more homes to choose from. But as more homeowners stay put, those who want to sell face a supply shortage, bidding wars and escalating prices.\n\n\u201cSellers face a prisoner\u2019s dilemma, a situation in which individuals don\u2019t cooperate with each other, even though it is seemingly in their best interest to do so,\u201d Fleming wrote. \u201d \u2026 If they both choose to sell, they both benefit because they increase the inventory of homes available, and collectively alleviate the supply shortage.\u201d\n\nIn addition, a recent rebound from historically low mortgage interest rates locks many owners into their current homes, he said. Fixed rates as low as 3.3 percent and adjustable rates as low as 2.4 percent are long gone.\n\n\u201cWhy move when it will cost more each month to borrow the same amount from the bank?\u201d Fleming wrote. \u201d \u2026 Existing homeowners are increasingly financially imprisoned in their own home by their historically low mortgage rate. It makes choosing a kitchen renovation seem more appealing than moving.\u201d\n\nCalifornia\u2019s Prop. 13 keeps property taxes low for homeowners \u2014 so long as they keep their current home and don\u2019t buy another. If they move, their taxes jump to current market rates, which can be two or three times higher than taxes for their old homes.\n\nBut the tax consequences don\u2019t stop there.\n\nOrdinarily, you can sell your home tax free if its value increased by $250,000 or less for unmarried owners and by $500,000 or less for married couples.\n\nIn volatile housing markets like California, however, value gains often exceed the exempted amount, meaning sellers face capital gains taxes when they sell.\n\n\u201cPrices have run up so dramatically,\u201d Yun said. \u201cThe capital gains implication has become an issue. \u2026 Some people say,\u00a0\u2018I don\u2019t want to pay money to the federal government.\u2019 \u201d\n\nAging baby boomers also are choosing to live out their days in their existing homes, keeping them off the market.\n\nSeventy-one percent of people aged 55 and older have been living in their homes for 18 years or longer, a California Association of Realtors analysis of U.S. Census figures show. Almost a fourth of them have been in the same home for the past 39 years.\n\nMcIntosh, CAR president, noted that in some cases, families are moving in with their aging parents, providing elder care and inheriting the home when their parents die.\n\nSo with more homeowners staying put, is that bad for business?\n\nAfter all, business is booming.\n\nFrom January through June, more than 95,500 homes changed hands in Orange, Los Angeles, Riverside and San Bernardino counties, the most for that period since 2006, CoreLogic figures show.\n\nInterviews with escrow officers and home inspectors from the Coachella Valley to the San Fernando Valley indicated their business also is up.\n\n\u201cIt\u2019s been as busy as we can take it,\u201d said Bill Schultz, a Coachella Valley home inspector. \u201cRight now, we\u2019re kind of in that same frenzy that we were 12 to 15 years ago.\u201d\n\nJohn LaRocca, president of Burbank-based LaRocca Inspection Associates, said his company hired six inspectors this year.\n\nNancy Glass, owner of Cardinal Escrow in Long Beach, hired three new people in the last four months.\n\nOn the other hand, how busy would things be if people moved more frequently? Wouldn\u2019t that trigger even more sales?\n\nSure, home sales are at an 11-year peak, the most transactions for the region since the days of the housing bubble. On the other hand, year-to-date home sales still are 24 percent below the year 2000, before the housing bubble and after a 14 percent increase in the region\u2019s population.\n\nIn 2000, almost 126,000 homes changed hand in the region during the first half of the year.\n\n\u201cThe impact is a slower-than-usual economic growth,\u201d Yun said. \u201cIf the inventory were larger, there would be much more home sales, and that would affect economic growth.\u201d\n\nOne sector of the market not suffering is home remodeling.\n\nAnnual expenditures on home remodeling projects in Southern California increased steadily since 2010, rising 38 percent to $2.2 billion in 2016, building permit figures compiled by the Construction Industry Research Board show.\n\nThe Gerbers in Rossmoor loved their home\u2019s location more than the house itself when they bought the 59-year-old Plymouth-style ranch home in December 2011. The 2,800-square-foot house sits in a quiet cul-de-sac not far from Brianna\u2019s parents, who help out with their two children, ages 3 and 6.\n\nBut the master bedroom closets were small and sat in the bathroom. The house was getting old, with leaky sinks, cracked stucco and an aging roof. It lacked insulation in the walls and didn\u2019t have air conditioning. The long, galley-style kitchen was out of date.\n\nBut they wanted to avoid the hassle of remodeling, so Brianna started looking for another house to buy. Nothing worked. Either the prices were too high, the locations were inferior or the homes weren\u2019t upgraded.\n\nSo they hired Rossmoor remodeling contractor Eddie Kesky and invested about $750,000 in renovations:\u00a0The family moved the master bedroom to the back, converted the detached garage in the backyard into an outdoor patio with a fireplace and dining area, built an attached garage at the front and opened up the kitchen so the family can gather around a new island counter for meals. They insulated the walls and installed air conditioning.\n\n\u201cNow we have the home that has the sound system and fits into our needs,\u201d said Jacob Gerber.\n\nIsrael Battres, owner of Battres Construction in Santa Ana, said the Gerbers\u2019 decision to stay put is common these days.\n\n\u201cMost of my homeowners are staying put and remodeling,\u201d Battres said. \u201cIt\u2019s a seller\u2019s market. If they go and sell their home, they\u2019re going to move into a place that needs remodeling. So they choose to stay in their home and make it their forever home.\u201d"},
{"url": "https://nctritech.wordpress.com/2017/03/07/zfs-wont-save-you-fancy-filesystem-fanatics-need-to-get-a-clue-about-bit-rot-and-raid-5/", "link_title": "ZFS won\u2019t save you: fancy filesystem fanatics need to get a clue about bit rot", "text": "tl;dr: Hard drives already do this, the risks of loss are astronomically low, ZFS is useless for many common data loss scenarios, start backing your data up you lazy bastards, and RAID-5 is not as bad as you think.\n\nI am absolutely sick and tired of people in forums hailing ZFS (and sometimes btrfs which shares similar \u201cadvanced\u201d features) as some sort of magical way to make all your data inconveniences go away. If you were to read the ravings of ZFS fanboys, you\u2019d come away thinking that the only thing ZFS won\u2019t do is install kitchen cabinets for you and that RAID-Z is the Holy Grail of ways to organize files on a pile of spinning rust platters.\n\nIn reality, the way that ZFS is spoken of by the common Unix-like OS user shows a gross lack of understanding of how things really work under the hood. It\u2019s like the \u201cknowledge\u201d that you\u2019re supposed to discharge a battery as completely as possible before charging it again which hasn\u2019t gone away even though that was accurate for old Ni-Cd battery chemistry and will destroy your laptop or cell phone lithium-ion cells far faster than if you\u2019d have just left it on the charger all the time. Bad knowledge that has spread widely tends to have a very hard time dying. This post shall serve as all of the nails AND the coffin for the ZFS and btrfs feature-worshiping nonsense we see today.\n\nSide note: in case you don\u2019t already know, \u201cbit rot\u201d is the phenomenon where data on a storage medium gets damaged because of that medium \u201cbreaking down\u201d over time naturally. Remember those old floppies you used to store your photos on and how you\u2019d get read errors on a lot of them ten years later? That\u2019s sort of like how bit rot works, except bit rot is a lot scarier because it supposedly goes undetected, silently destroying your data and you don\u2019t ever find out until it\u2019s too late and even your backups are corrupted.\n\nA certain category of people are terrified of the techno-bogeyman named \u201cbit rot.\u201d These people think that a movie file not playing back or a picture getting mangled is caused by data on hard drives \u201crotting\u201d over time without any warning. The magical remedy they use to combat this today is the holy CRC, or \u201ccyclic redundancy check.\u201d It\u2019s a certain family of hash algorithms that produce a magic number that will always be the same if the data used to generate it is the same every time.\n\nThis is, by far, the number one pain in the ass statement out of the classic ZFS fanboy\u2019s mouth and is the basis for most of the assertions that ZFS \u201cprotects your data\u201d or \u201cguards against bit rot\u201d or other similar claims. While it is true that keeping a hash of a chunk of data will tell you if that data is damaged or not, the filesystem CRCs are an unnecessary and redundant waste of space and their usefulness is greatly over-exaggerated by hordes of ZFS fanatics.\n\nEnter error-correcting codes (ECC.) You might recognize that term because it\u2019s also the specification for a type of RAM module that has extra bits for error checking and correction. What the CRC Jesus clan don\u2019t seem to realize is that all hard drives since the IDE interface became popular in the 1990s have ECC built into their design and every single bit of information stored on the drive is both protected by it and transparently rescued by it once in a while.\n\nHard drives (as well as solid-state drives) use an error-correcting code to protect against small numbers of bit flips by both detecting and correcting them. If too many bits flip or the flips happen in a very specific way, the ECC in hard drives will either detect an uncorrectable error and indicate this to the computer or the ECC will be thwarted and \u201crotten\u201d data will successfully be passed back to the computer as if it was legitimate. The latter scenario is the only bit rot that can happen on the physical medium and pass unnoticed, but what did it take to get there? One bit flip will easily be detected and corrected, so we\u2019re talking about a scenario where multiple bit flips happen in close proximity and in such a manner that it is still mathematically valid.\n\nWhile it is a possible scenario, it is also very unlikely. A drive that has this many bit errors in close proximity is likely to be failing and the the S.M.A.R.T. status should indicate a higher reallocated sectors count or even worse when this sort of failure is going on. If you\u2019re monitoring your drive\u2019s S.M.A.R.T. status (as you should be) and it starts deteriorating, replace the drive!\n\nNote that in most of these bit-flip scenarios, the drive transparently fixes everything and the computer never hears a peep about it. ZFS CRCs won\u2019t change anything if the drive can recover from the error. If the drive can\u2019t recover and sends back the dreaded uncorrectable error (UNC) for the requested sector(s), the drive\u2019s error detection has already done the job that the ZFS CRCs are supposed to do; namely, the damage was detected and reported.\n\nWhat about the very unlikely scenario where several bits flip in a specific way that thwarts the hard drive\u2019s ECC? This is the only scenario where the hard drive would lose data silently, therefore it\u2019s also the only bit rot scenario that ZFS CRCs can help with. ZFS with CRC checking will detect the damage despite the drive failing to do so and the damage can be handled by the OS appropriately\u2026but what has this gained us? Unless you\u2019re using specific kinds of RAID with ZFS or have an external backup you can restore from, it won\u2019t save your data, it\u2019ll just tell you that the data has been damaged and you\u2019re out of luck.\n\nIf your drive\u2019s on-board controller hardware, your data cable, your power supply, your chipset with your hard drive interface inside, your RAM\u2019s physical slot connection, or any other piece of the hardware chain that goes from the physical platters to the CPU have some sort of problem, your data will be damaged. It should be noted that SATA drive interfaces use IEEE 802.3 CRCs so the transmission from the drive CPU to the host system\u2019s drive controller is protected from transmission errors. Using ECC RAM only helps with errors in the RAM itself, but data can become corrupted while being shuffled around in other circuits and the damaged values stored in ECC RAM will be \u201ccorrect\u201d as far as the ECC RAM is concerned.\n\nThe magic CRCs I keep making fun of will help with these failures a little more because the hard drive\u2019s ECC no longer protects the data once the data is outside of a CRC/ECC capable intermediate storage location. This is the only remotely likely scenario that I can think of which would make ZFS CRCs beneficial.\n\n\u2026but again: how likely is this sort of hardware failure to happen without the state of something else in the machine being trashed and crashing something? What are the chances of your chipset scrambling the data only while the other millions of transistors and capacitors on the die remain in a functional and valid working state? As far as I\u2019m concerned, not very likely.\n\nData loss due to user error, software bugs, kernel crashes, or power supply issues usually won\u2019t be caught by ZFS CRCs at all. Snapshots may help, but they depend on the damage being caught before the snapshot of the good data is removed. If you save something and come back six months later and find it\u2019s damaged, your snapshots might just contain a few months with the damaged file and the good copy was lost a long time ago. ZFS might help you a little, but it\u2019s still no magic bullet.\n\nBy now, you\u2019re probably realizing something about the data CRC gimmick: it doesn\u2019t hold much value for data integrity and it\u2019s only useful for detecting damage, not correcting it and recovering good data. You should always back up any data that is important to you. You should always keep it on a separate physical medium that is ideally not attached to the computer on a regular basis.\n\nBack up your data. I don\u2019t care about your choice of filesystem or what magic software you write that will check your data for integrity. Do backups regularly and make sure the backups actually work.\n\nIn all of my systems, I use the far less exciting XFS on Linux with metadata CRCs (once they were added to XFS) on top of a software RAID-5 array. I also keep external backups of all systems updated on a weekly basis. I run S.M.A.R.T. long tests on all drives monthly (including the backups) and about once a year I will test my backups against my data with a tool like rsync that has a checksum-based matching option to see if something has \u201crotted\u201d over time.\n\nAll of my data loss tends to come from poorly typed \u2018rm\u2019 commands. I have yet to encounter a failure mode that I could not bounce back from in the past 10 years. ZFS and btrfs are complex filesystems with a few good things going for them, but XFS is simple, stable, and all of the concerning data loss bugs were ironed out a long time ago. It scales well and it performs better all-around than any other filesystem I\u2019ve ever tested. I see no reason to move to ZFS and I strongly question the benefit of catching a highly unlikely set of bit damage scenarios in exchange for the performance hit and increased management complexity that these advanced features will cost me\u2026and if I\u2019m going to turn those features off, why switch in the first place?\n\nA related category of blind zealot is the RAID zealot, often following in the footsteps of the ZFS zealot or even occupying the same meat-suit. They loudly scream about the benefits of RAID-6, RAID-10, and fancier RAID configurations. They scorn RAID-5 for having terrible rebuild times, hype up the fact that \u201cif a second drive dies while rebuilding, you lose everything!\u201d They point at 10TB hard drives and do back-of-the-napkin equations and tell you about how dangerous and stupid it is to use RAID-5 and how their system that gives you less space on more drives is so much better.\n\nStop it, fanboys. You\u2019re dead wrong and you\u2019re showing your ignorance of good basic system administration practices.\n\nI will concede that your fundamental points are mostly correct. Yes, RAID-5 can potentially have a longer rebuild time than multi-stripe redundant formats like RAID-6. Yes, losing a second drive after one fails or during a rebuild will lose everything on the array. Yes, a 32TB RAID-5 with five 8TB drives will take a long time to rebuild (about 50 hours at 180 MB/sec.) No, this isn\u2019t acceptable in an enterprise server environment. Yes, the infamous RAID-5 write hole (where a stripe and its parity aren\u2019t both updated before a crash or power failure and the data is damaged as as result) is a problem, though a very rare one to encounter in the real world. How do I, the smug techno-weenie advocating for dead old stupid RAID-5, counter these obviously correct points?\n\nA home or small business user is better off with RAID-5 if they\u2019re also doing backups like everyone should anyway. With a 7200 RPM 3TB drive (the best $/GB ratio in 7200 RPM drives as of this writing) costing around $95 each shipped, I can only afford so many drives. I know that I need at least three for a RAID-5 and I need double as many because I need to back that RAID-5 up, ideally to another machine with another identically sized RAID-5 inside. That\u2019s a minimum of six drives for $570 to get two 6TB RAID-5 arrays, one main and one backup. I can buy a nice laptop or even build a great budget gaming desktop for that price, but for these storage servers I haven\u2019t even bought the other components yet. To get 6TB in a RAID-6 or RAID-10 configuration, I\u2019ll need four drives instead of three for each array, adding $190 to the initial storage drive costs. I\u2019d rather spend that money on the other parts and in the rare instance that I must rebuild the array I can use the backup server to read from to reduce my rebuild time impact. I\u2019m not worried about a few extra hours of rebuild.\n\nNot everyone has thousands of dollars to allocate to their storage arrays or the same priorities. All system architecture decisions are trade-offs and some people are better served with RAID-5. I am happy to say, however, that if you\u2019re so adamant that I shouldn\u2019t use RAID-5 and should upgrade to your RAID levels, I will be happy to take your advice on one condition.\n\nBuy me the drives with your own money and no strings attached. I will humbly and graciously accept your gift and thank you for your contribution to my technical evolution.\n\nIf you can add to the conversation, please feel free to comment. I want to hear your thoughts. Comments are moderated but I try to approve them quickly."},
{"url": "https://github.com/danlm/QNial7", "link_title": "QNial7: The NIAL language environment", "text": "The Nial language was developed by Mike Jenkins and Trenchard More in a collaborative research project supported by Queen's University at Kingston and IBM Cambridge Scientific Center from 1979 to 1982. Mike's team at Queen's designed and implemented a portable C-based interpreter, Q'Nial that was initially released in 1983.\n\nThe language combines Trenchard More's theory of nested arrays with Mike's ideas on how to build an interactive programming system. The goal was to combine the strengths of APL array-based programming with implementation concepts borrowed from LISP, structured programming ideas from Algol, and functional programming concepts from FP. THe interpreter, originally developed for Unix, was small enough to run on the then newly released IBM PC and portable enough to execute on IBM mainframes computers.\n\nNial Systems limited licensed the interpreter from Queen's Uinversity and marketed it widely. Mike Jenkins continued to refine both the language and its implementation. In 2006 Mike released Version 6.3 as an open source project to encourage continued development of Nial.\n\nIn 2014 Mike started working with John Gibbons to develop a 64-bit version and to add capabilities that John needed for his work. The decision was made to target the open source for Unix-based platforms and release it on GitHub.\n\nThis version of Q'Nial is intended for people who want to integrate the functionality of Nial into projects that can take advantage of its powerful array computations for numeric, symbolic, or data analysis problems. The major changes are:\n\nYou can either use git to clone the repository or you can download a zipped version of the repository as provided on GitHub.\n\nThe repository is organised in a simple directory structure\n\nIf your computer is one of the supported platforms then all you need to do is the following:\n\nDownload the QNial7 repository to your $Home directory. Add the nial executable for your platform to your $PATH variable. For example, for OSX Test that the nial executable is working by running it interactively. A header of the following form appears: The cursor will be indented by 5 spaces. Test the executable by typing: will appear at the left margin. If you are new to using QNial, you can learn more about it by invoking the tutorial library function teach. Run nial interactively, then at the prompt type: Enter \"intro\" at the prompt and follow the instructions.\n\nThe directories BuildCore and BuildNial are used to build versions of QNial7. If no existing executable is available for the platform you are using then you need to build the nialcore executable using BuildCore. Follow the directions given in the README.md file in BuildCore.\n\nIf you are building for a new platform or want to add features to QNial7 that are not in the executable you have then you use directory BuildNial. The README.md in BuildNial describes the process for selecting additional features already available. There is also an explanation of how a new feature can be implemented and added to QNial7."},
{"url": "https://twitter.com/guillaume_che/status/894705063318474752", "link_title": "TensorFlow stars on GitHub by user profiles' public location", "text": "When you tweet with a location, Twitter stores that location. You can switch location on/off before each Tweet and always have the option to delete your location history. Learn more"},
{"url": "https://blog.neocities.org/blog/2017/08/07/ipfs-dns-support.html", "link_title": "IPFS DNS Support", "text": "I\u2019ve never been shy about our support for the distributed web. As the flaws of today\u2019s web continue to lead to increased centralization, and with Net Neutrality being gutted, the stakes for a good future for the web have never been more clear to me. That\u2019s why we were one of the first organizations to start doing implementation work with IPFS, which I continue to believe is the next version of the web, and the logical successor to HTTP.\n\nContinuing down this adoption path, I\u2019m happy to announce that Neocities now has support for IPFS DNS records! That means that most of the 140,000+ sites that have a neocities.org subdomain can now be accessed through IPFS via DNS.\n\nYou can see it in action at https://ipfs.io/ipns/testipfs.neocities.org, or if you have IPFS installed, you can view it locally at http://127.0.0.1:8080/ipns/testipfs.neocities.org.\n\nThis works by using TXT records with our DNS servers. You can run\n\n in a command prompt to see it in action:\n\nRight now we\u2019re only supporting IPFS immutable hashes (not IPNS/keypair ones), which means there is a short delay before DNS records update with site changes through IPFS, but we\u2019ll reduce this time lag soon. In the interim, this gives our IPFS implementation its first taste of mutability.\n\nThe distributed web is still in the foundation stages, and things are constantly changing. We\u2019ll keep testing new things and see how we can help improve the distributed web. Enjoy!"},
{"url": "https://github.com/rohitpaulk/simple-editor", "link_title": "Barebones text editor in 150 lines of Clojure", "text": "This is a bare-bones text editor written in Clojure. It's just my excuse to learn Clojure, zero real-world value.\n\nIt supports the following keys:"},
{"url": "https://www.youtube.com/watch?v=EZxP0i9ah8E", "link_title": "Paul Buchheit at Startup School 08 (2008)", "text": "The interactive transcript could not be loaded.\n\nRating is available when the video has been rented.\n\nThis feature is not available right now. Please try again later."},
{"url": "https://outlookzen.wordpress.com/2016/10/31/dangerous-ideas/", "link_title": "Dangerous Ideas", "text": "In communist China, those deemed to be promoting dangerous Capitalist ideas were subjected to severe crackdown under the Cultural Revolution.\n\nIn Catholic 16th century Europe, those who promoted radical ideas that undermined mainstream Judeo-Christian beliefs, were locked up as heretics.\n\nIn America during the Red Scare, those suspected of endorsing Socialist ideas were subjected to McCarthy hearings, casting pallor over their ideological purity and loyalty to country.\n\nEvents that might have been centuries and continents apart, but tied together through a single common thread. A shared belief. The worry that certain ideas are dangerous. That certain ideas, if left uncensored, will make the world a worse place. That such dangerous ideas need to be combated by any means necessary. That one of the most effective ways to do so, is by punishing and persecuting the idea\u2019s proponents, so as to send a message to anyone else who might dare to speak up.\n\nAnd now, in the midst of an increasingly turbulent world, we\u2019re starting to see whispers of the same ghosts appear once again. We\u2019re starting to see citizens in one of the most (supposedly) enlightened and progressed societies in the world, calling for the economic persecution and ostracism of a (idiotic) political candidate, his endorsers, and even the colleagues of his endorsers.\n\nLet\u2019s acknowledge one fact at the outset: to compare the magnitude of persecution experienced by the victims of the Cultural Revolution, to what Peter Thiel is experiencing, is simply absurd. Despite anything Peter Thiel might be going through, most people would swap places with him in a heartbeat.\n\nAnd yet, the essence of the comparison still stands. The magnitude of persecution being heaped upon Peter Thiel might be trivial compared to the Russian gulags, but the general principle behind both remains the same. That certain ideas are dangerous, and anyone who endorses such dangerous ideas is worthy of punishment.\n\nSome might argue that what Peter Thiel is experiencing is not persecution, because there is no government actions involved. No one is suing Peter Thiel in court, nor is anyone threatening to physical harm him in any way. And yet, such arguments miss the point entirely. Persecution isn\u2019t, and has never been, limited to government activity or physical threats. For centuries, Jews, Roma and Blacks around the world have been persecuted by the societies and citizens around them. The worst of these featured physical violence, but even when they didn\u2019t, the economic and social persecution endured by these victimized groups was horrendously damaging. Blacks who were denied entry or service in all varieties of Private Sector activities, found themselves economically marginalized. Individuals who were subjected to ostracism and exclusion found their lives dramatically altered, and the very threat of such social persecution kept many more from speaking their minds.\n\nTo think of human beings as animals, capable of experiencing suffering only when physically harmed, degrades the very essence of the human condition by ignoring the economic and social needs that every one of us yearns for. To claim that the economic and social persecution endured by its victims is not persecution at all, is a slap on the face to millions who\u2019ve experienced its cold pressing grips.\n\nBut let\u2019s return to the original premise that we started this journey on. The premise that certain ideas are so dangerous, that they need to be censored by persecuting its proponents. Does such a premise truly hold up? If you believe in the ideals of Free Speech, the answer is clearly no. Every western citizen has had the axioms of Free Speech drilled into their heads as kids, but why do we believe in Free Speech? Why do we support, the First Amendment? If ideas such as racism, sexism and bigotry are so dangerous, why do we guarantee legal immunity to their proponents? Why not simply lock them up in prison, as is done in other countries? Why not outright ban these dangerous ideas?\n\nThe Catholics banned the questioning of Judeo-Christian values, because surely no one can doubt the word of God. The Communists banned the advocating of Capitalist ideologies, because surely no one can defend the exploitation of the proletariat by the bourgeoisie. Why not take a page from their book and carve out exceptions in the First Amendment for ideas that we know incontrovertibly to be false?\n\nIf you\u2019re a supporter of Free Speech at least, the answer goes back to the Marketplace of Ideas. One where ideas spar with one another, and in the long run, false/bad ideas find themselves supplanted by superior alternatives. One where dangerous ideas will automatically find themselves discredited and bankrupt, without the need for censorships, personal attacks or persecution.\n\nConsider the example of Flat Earth beliefs. Some people through history long labored under the illusion that the Earth was flat, like a coin. With time, this belief found itself supplanted and replaced by the belief in a Spherical Earth, which then found itself supplanted by the belief in a Oblate Spheroid. Today, if I went around the country telling everyone who would listen that the Earth is flat, no one need bother persecuting me, or attacking me personally. The idea itself has lost, has been made bankrupt, and no degree of persecution is required to ensure the primacy of the Spheroid Earth belief. The Marketplace of Ideas has succeeded in weeding out false ideas, and supplanting them with superior ones, all without any need for persecution.\n\nTo quote a Justice of the Supreme Court, \u201cBut when men have realized that time has upset many fighting faiths, they may come to believe even more than they believe the very foundations of their own conduct that the ultimate good desired is better reached by free trade in ideas \u2014 that the best test of truth is the power of the thought to get itself accepted in the competition of the market, and that truth is the only ground upon which their wishes safely can be carried out. That, at any rate, is the theory of our Constitution.\u201d\n\nPerhaps such belief in the Marketplace of Ideas is misplaced. Perhaps such idealism is hopelessly naive, and human beings, as foolhardy as we are, are doomed to be entrapped by attractively false ideas. Perhaps our belief in the Marketplace of Ideas, is itself one such appealing and false idea. If so, disregard everything you\u2019ve read in this essay. Perhaps our blind trust in the First Amendment is misplaced, and the Catholics, Communists and McCarthyites were onto something after all. Perhaps some ideas are so dangerous, that the only rational approach is to fight them by persecuting anyone who attempts to spread them.\n\nBut I don\u2019t think so. I believe that the Marketplace of Ideas really does work. That to quote Lincoln, you can fool all of the people some of the time, or some of the people all of the time, but not all of the people all of the time. That the benefits to society of having Free Speech and a Marketplace of Ideas, where anyone can participate without fear of backlash, far outweigh any of the downsides. That when faced with dangerous ideas, we should focus our efforts on attacking the idea, not the person. That far more than fearing dangerous ideas, what we really have to fear is an environment that strives to censor them."},
{"url": "http://hintjens.com/blog:114", "link_title": "Why I've Quit Twitter(2016)", "text": "I've loved Twitter, the format and the transparency. Yet over the last year, and months, I've become increasingly concerned about it. We've all seen the divisive arguments over race and gender. These aren't just your usual Internet arguments. These are becoming a civil war and I'm stepping out of it.\n\nI think Twitter has become a recruiting tool for a number of toxic cults. These disguise themselves as 'left wing' and 'right wing' and other forms of activism. What they do in fact is spread hate in the name of hits, followers, income, and power. They use each other to troll and divide.\n\nLet me cite these by name and explain each. I'm not going to use any links. Explore further at your own risk.\n\nWe have the third wave feminists (3WF), promising a nirvana where maleness is a birth crime and femininity will Save the World. It has its mystical roots in Gaia the Earth Mother, and it appeals to an ancient conflict between men and women. The conflict expresses in every couple, every family, and within individuals.\n\n3WF collects women looking for a cause, often highly educated, under-challenged, and lonely. This is a generation who have learned that doing female things, and following the female biological imperatives, is \"bad.\" And yes, these are real things. If you hear a voice shouting \"social construct!\" in your mind, you are already under a form of mind control.\n\nHumanity is the only species capable of denying its own existence.\n\nIt is ironic, and sad, that 3WF accuses women that refuse to accept its message as suffering from \"internalized misogyny.\" That is, self-hate so deep they cannot see it. And yet it is 3WF members that hate their own femaleness. They take no pride in it. It disgusts them so much they must project that onto others.\n\n3WF also collects men, such as a few \"male feminists\" whom I've had to block to protect them from my need to always be right. Why do boys follow a girls' cult? Partly it's biology. The men go where the women are. We will say and do a lot of stupid things for the remotest chance at sex. But a lot of men have also grown up in the same education system, learning from a young age that gender is a social construct, and Male = Bad, Female = Good.\n\nI am an activist, and have spent decades of my life working pro-bono for the benefit of others. This seems a small price to pay for enjoying a prosperous and stable society. Yet 3WF is not pro-bono activism. It does not have any tangible impact on real problems facing the vulnerable in society: many women, and also children, the aged, and many men. Do I really need to cite statistics showing that most violence against children comes from women? I'd rather not, because it seems I'm excusing male violence, which I'm not.\n\nWhat 3WF clearly is, is a psychopathic cult. Its leaders, whom I gladly brand as psychopaths, gather significant power and money. They wield their followers like a weapon against opponents, to stop debate. They invent jargon and magical theories. An MTV talking head proudly states, \"'Mansplaining' is now in the dictionary. If that's not free speech, I don't know what is!\"\n\nWhy and how did feminism get hijacked? I'm not an expert so have asked some who are. Their answers: feminism in the West largely accomplished its goals by 2000 or so. There is no wage gap, no discrimination of note. As the core of the movement was based on conflict with maleness, it had to find new targets, and new leaders willing to aim at these targets. Hence its expansion into hate, and its domination by psychopaths.\n\nThe second cult is close and yet distinct, by definition. It was born as \"Black Lives Matter\" around 2014, and has grown into a national movement calling itself the \"Black Liberation Collective,\" or BLC. It wears the clothes and language of the Black Power movement of the 1950s. Yet while that movement was born in the ghettos, and fought real problems of racism, BLC seems obsessed with posture and gesture. It is a Facebook movement, narcissist to the core.\n\nBLC has spread rapidly into college campuses across the US. While college students seemed like Fair Game to aggressive pepper-spraying police a few years before, BLC raised the stakes. \"Hit one of us,\" they said, \"and you hit an entire people.\" As a threat, it was effective. Yet what has BLC done with that? It has turned campus after campus into scenes from Lord of the Flies. What happens when spoiled children get the feeling of power over others? Read thedemands.org.\n\nWhat does BLC promise its supporters, the children of wealthy families, freed from the burden of work or child care, freed even from the need to study? Not liberation, for sure. What I think it offers is appeasement for survivor guilt, someone to blame, and a rough answer.\n\nI've witnessed real racism in the States, black friends of mine who spent the weekends in gaol to pay off court fees for unjustified traffic fines. I've seen families destroyed by alcohol, drugs, and the sheer negligence of the state. There are real problems to solve. And there are many people working to solve these problems.\n\nBLC ignores such problems. Instead, it takes the guilt of the recently well-off, casts it into the worst kind of tribalism, and then promises a future where \"whiteness\" is eradicated. It casts the sins of the world into the pale-skinned tribes, then seeks to throw them over a cliff like the Biblical swine.\n\nHere is the final BLC demand to the University of Missouri:\n\nAnd in 2015, that university lost $20-$25 million in tuition as students voted with their feet. BLC starts to look less like a cure, and more like a fatal disease.\n\nThe goal here is not progress. It is chaos, destruction, the tabula rasa of revolution.\n\n3WF and BLC are similar in key ways. They both worship the god Ego, with different prophets. They both speak to the same pampered, indolent middle class. They both focus on the same villains: the white male establishment, aka the \"Patriarchy.\"\n\nAnd both cults go one step further: they claim the status of absolute victim, and in doing so they deny the real victims their place. 3WF ignores the spread of vulnerability across society, particularly children and the elderly. BLC ignores the mass of poverty that cannot claim one drop of African or Native American blood. It is not a small mass, as Donald Trump knows.\n\nYes, America is a divided society. Extraordinarily unfair in ways that can make no sense to the outside observer. Filled with anger, hate, and intolerance. Focused on tribalism and difference. These are real problems that can be solved, and probably will be solved, over time. Yet these two cults embrace the problems and amplify them, in their thirst for power.\n\nSince both cults are growing, perhaps at a time when more traditional cults like Scientology are shrinking, they work together. Soon enough they will be chewing at each others' throats. Yet for now they agree on the common enemy and their strategy of tension.\n\nWe see such cults of hate across the world, wherever there are unresolved social divisions caused by history. The natural tendency of many people is to choose a side, which worsens the divisions. Psychopaths love such opportunities. \"Hate the infidel,\" they shout. \"Bomb the Muslim!\" they demand. \"Kill the Jew,\" they whisper.\n\nBLC's difference is its embrace of the theory of race. Does this have a biological basis?\n\nLet me cite one study from Sarah Tishkoff. Her team found fourteen ancestral human genetic families. These match ancient linguistic families. Nine of these are in Africa, five in the rest of the world. Around 75% of African-Americans share a Niger-Kordofanian family origin.\n\nDoes this constitute two \"races\"? Or even two \"ethnicities\"? If you want to count, there are probably over a dozen, most of which are \"black.\" And all modern humans are a mongrel mix of these. Gene flow has no mercy, which is why Russian Jews look like Russians and Ethiopian Jews look Ethiopian, despite thousands of years of mixing taboos.\n\nThere is no \"black\" and there is no \"white,\" except by comparison and prejudice.\n\nWe falsified the theory of race at great cost in the last century. Yet BLC builds on it, and amplifies it. That makes it a dangerous cult, both in itself and in the way it enables others' destructive tendencies.\n\nWhich brings me to our third cult, the reactionary right, or RR. It is arguably the more dangerous of the three. The RR claims to address the damage that the 3WF and BLC do, yet it is profoundly racist, and just as divisive.\n\nThe 3WF and BLC have left a huge mass of ordinary men and women without voice in the debate over where our society should go. The RR finds this mass of people, and speaks to their vulnerability. It attacks 3WF and BLC eloquently, and without fear. Whereas most people shirk from confrontation with armies of sock puppets, RR loves the confrontation.\n\nIn general the only people who seek out psychopaths to argue with are worse psychopaths.\n\nRR attacks the \"gender is a social construct\" myth of 3WF and then attacks \"race is a social construct\" in the same breath. This may work for some listeners. To me, the switch from exposing intolerance to embracing it is shocking. Yet it's systematic, and often focuses on immigration and refugees.\n\nAfter the arrests in Brussels of a terrorist, days before the attacks, the RR was claiming that \"200 youths attacked police as they arrested an Islamic terrorist,\" in Molenbeek. The RR claim that Muslims in Europe fight for a Caliphate. They happily mix \"refugee\", \"immigrant\", \"terrorist\", and \"Muslim\" as if these mean the same thing.\n\nI think, though it's not yet clear to me, that the RR are trying to push people towards political extremism. That is, while 3WF is trying to control media and technology, and BLC wants to control education, the RR wants real political power. Donald Trump's bid for Presidential office is entirely in line with the RR world view.\n\nThe 21st century media is a different animal from old media. Gone are the careful, slow investigations and analyses. In their place we see click races and catch phrases. After a significant court trial such as that of Jian Ghomeshi, they prepare two pieces, one for each outcome. Each is designed to trigger maximum outrage. Seconds after the announcement, the pieces go live, and the tweets flow.\n\nTwitter is the mouthpiece of New Media. It takes outrage and multiplies it a thousand times. Every person in that human chain gets a small kick as they Retweet and Like. There is no responsibility.\n\nAnd there is no dialog. We get two sides shouting slogans at each other, each seeking for the most outrage they can express without being blocked and banned.\n\nIn the fight for clicks, all dialogue dies. In its place we get emotion, and appeal to emotion. In Ghomeshi's trial, the judge said (I paraphrase):\n\nWhich became, on Twitter:\n\nWhich a few dozen retweets later became:\n\nWhich 3WF picked up on and has turned into a new crusade against the inherent evil of maleness. The judge will probably be retired, for doing his job properly, and Ghomeshi will never clear his name. Check the #ghomeshi hashtag if you can stomach it. Only a few commentators ask, \"what if the accusers really were lying?\"\n\nAsking for thought and pause is asking to be ignored. 3WF does not need trials or evidence to know that maleness is guilty. It is a core tenet of the cult: every man is a rapist, it is a question of \"when\", not \"if\". The effect is to keep its members in an echo chamber of hate, isolated from balancing opinion.\n\nIronically, Ghomeshi was a long-standing member of the 3WF cult. As a public male figure, he was Fair Game, though. The use of sexual misconduct allegations against prominent men is a standard technique. It often follows the same pattern:\n\nReal victims do not take this route. They mostly say nothing, out of shame and self-recrimination. Then if they speak up, it is to the police, who mostly do nothing because such cases are so hard to prove. Victims may press civil complaints then, where the standards of evidence are lower, and the punishments less severe. Only when their case goes to trial, does the press learn about it.\n\nThose who use the media as their court tend to be liars. A person with evidence keeps it in reserve, and speaks through their lawyer.\n\nI'm forced to conclude that Twitter promotes liars. Worse, it promotes those who make a career out of lies. That is, psychopaths. Let me say this again: Twitter promotes psychopaths. It gives them power and status and credibility.\n\nI started building a Twitter profile some years ago. It all seemed innocent at the time. Get more followers, it's a good thing, right? Twitter let me promote my books, my writing, my talks. It was a way to keep in touch with people.\n\nAnd then there's that moment when you realize you're talking to an audience. Not friends, not colleagues. Rather, a group of mostly strangers. And if you say things they don't like, they'll walk away. You say the right things, and Twitter rewards you. New followers! Be too blunt, and people leave.\n\nThe problem with performance is that it rewards drama and emotion. This is fine for entertainment. It is toxic for dialog, for understanding. Combine Twitter's karma whoring model with its short text limit, and the result can be horrid.\n\nHorrid, that is, for those who seek truth and knowledge. The only way to get that is by careful argument of theory. Even then we only approximate. The process is everything. And Twitter promotes a process of anti-science.\n\nThere are some specific things Twitter could change to fix this. It would break Twitter as we know it, and thus will not happen:\n\nThe particular thread that kicked my brain into action was about (not) banning a conference speaker who holds racist views. I'm going to make an exception, and link to some of these tweets. Note the language and style used.\n\nConference organizers are of course free to invite whom they like. Notable about this particular event was that it has rules about how speakers are selected. These rules, a contract, exist specifically because 3WF demanded them. Papers are stripped of gender and identifying information, then selected on purely technical merit.\n\nSo the demand for selective enforcement is interesting. This is a classic cult technique: complex rules that apply in one direction only. The psychopath steals from everyone, yet is the first to demand harsh sentences for others' petty crimes.\n\nYet no matter the agreed rules, the demand to ban a speaker for their political views is indeed a call for censorship. This is text-book censorship: the suppression of speech which is inconvenient, sensitive, or politically incorrect.\n\nWhat's wrong with some censorship, you might ask? It's much like asking, \"why do we need due process?\" Surely censoring obvious hate speech is a good thing? Surely sending obvious rapists straight to prison without the cost and delay of a trial is a good thing?\n\nPeople who believe this (and there are a lot) have a common culture of lazy safety. They have never been arrested and locked up on false charges, by cops needing to make their quota. They have never been falsely accused by vindictive ex-partners. They have never lived in dictatorships where fighting the regime was classified as \"mental illness.\" They have never known a world where some knowledge was considered so dangerous, you could die for spreading it.\n\nOnce again, the loudest demands for rough justice come from the pampered children of the upper middle class, who believe the cornucopia of technology is their birthright. Who piss in drinkable water and think nothing of it. Who see an empty fridge as \"lazy\" and not \"poor.\"\n\nSince it is clearly not obvious, let me break down the reasons why censorship of Truly Dangerous Opinion (TDO) is bad:\n\nAs John Stuart Mill wrote in, \"On Liberty\":\n\nCensorship is a goals of all cults, because it gives them the Holy Grail of informal tribunals with unwritten rules. You'll see this demand made over and over: \"give us power to enforce our opinions over others\". It is really a call for dissolution of the State, and the handing of power to self-selected individuals. A form of anarchy where the strongest rule over the weak. It is the psychopath's wet dream.\n\nIt has become clear to me that Twitter is a lens that distorts. The economics in this case favor the wrong people: the psychopaths and their followers. It gives them space in which to grow, recruit, and market themselves. By using Twitter, I'm contributing to this.\n\nWe know where the politics of extremism takes us. We don't need to go back hundreds of years. Just a few days, to bombs exploding in my home city. Psychopaths produced those bombs, recruited the mules who carried them, and sent them on their way. One more cult of hate, seeking to divide and conquer.\n\nYes, I'm literally making an equivalence between the digital cults and Daesh. They lie at different points on a scale, yet it is one scale. Saying \"all men\" and \"all white men\" has the same intent and effect as saying \"all Muslims.\" It divides, isolates, and creates space for the cult to grow.\n\nNever doubt the harm that psychopaths will inflict on others. It is simply a pragmatic calculation of benefits against risks and costs. Allow cults to grow without sanction, and they will become more aggressive and more destructive.\n\nI'm in no place to stop these cults or even reason with them. However I can stop contributing to them. That means, stopping using Twitter. It's a painful decision to abandon years of building up a following. And yet it frees me to speak openly again.\n\nSo, I've deleted my account. Since literally deleting it confused some people, who thought I'd blocked them, I have removed my profile, and stopped tweeting. My old tweets remain, and people can message me.\n\nThere are other ways to talk with people. Other platforms that don't tilt so heavily in favor of extremism. Platforms where we can write more than a sentence. None is perfect. Yet some are better than others.\n\nThanks to everyone who supported me during the long years. I'm not walking away, just finding a better place to stand."},
{"url": "http://www.washingtonpost.com/sf/world/2017/08/07/perus-glaciers-have-made-it-a-laboratory-for-adapting-to-climate-change-its-not-going-well/", "link_title": "Peru\u2019s glaciers make it a test case for adapting to climate change", "text": "Lake Palcacocha is an example of the immediate threats Peru and other developing countries are facing from climate change. The country is especially vulnerable since it is home to 70 percent of the world\u2019s \u201ctropical glaciers\u201d \u2014 small, high-altitude ice caps found at the earth\u2019s middle latitudes. Their disappearance has made Peru something of a laboratory for human adaptation to climate change.\n\nSo far, it\u2019s not going very well.\n\n\u201cFor countries like Peru that are trying to climb out of poverty, there are major social, cultural and economic obstacles to adaptation,\u201d said Nelson Santill\u00e1n, a researcher at Peru\u2019s national water authority. \u201cIdentifying risks is one thing, but doing something about them is another.\u201d\n\nIn the weeks since President Trump announced the United States would\u00a0renege on its commitment\u00a0to the Paris climate accord, scientists have pointed to new signs the planet is edging closer to a precipice. Maximum temperature records continue falling. New cracks are opening at the polar ice caps.\n\nPeru\u2019s high-altitude glaciers are tiny by comparison, but millions of people depend on their runoff for water, food and hydroelectricity.\n\nSome of Peru\u2019s glaciers have\u00a0lost more than 90\u00a0percent\u00a0of their mass. While much of the water trickles harmlessly down the mountainside, in places like Lake Palcacocha, it is pooling in great big puddles of melted ice. Many of these new lakes are held back by glacial moraines, which are essentially mounds of compressed sediments. They may be structurally weak, and as the volume of water pushing on them increases, some will collapse."},
{"url": "http://www.minerva.com", "link_title": "First Reverse Merchant Processor", "text": "Where do the proceeds from the token crowdsale go?\n\nAs shown in the graph above, the token crowdsale is divided between multiple factions. 75% of the tokens will be distributed amongst crowdsale participants. 10% will be distributed amongst the founding team and advisors. 10% will be reserved for long term operational costs and new advancements. 2.5% will be reserved to be distributed to new partnerships in the form of signing bonuses. The remaining 2.5% will be reserved for our diligent bug bounty program. Crypto-assets transferred in exchange for Owl tokens during the crowdsale are the revenue of Blockstars."},
{"url": "https://semiengineering.com/advanced-packaging-moves-to-cars/", "link_title": "Advanced Packaging Moves to Cars", "text": "As automotive OEMs come up to speed on electrification of vehicles, each at their own pace, they are starting to embrace novel packaging approaches as a way to differentiate themselves in an increasingly competitive market.\n\nWirebond used to dominate this market, where most of the chips were relatively unsophisticated and product cycles were slow\u2014sometimes as long as five to seven years. But as automakers begin planning for an increasingly fast-moving driver-assisted or fully autonomous future, they have begun considering advanced packaging as a way to keep their technology current and to get products to market more quickly.\n\n\u201cWe are seeing an increase in new and different packaging choices and considerations for automotive applications,\u201d observed Edward Fontanilla, deputy director of product technology marketing at STATS ChipPAC. \u201cOne example is high-frequency radar devices for 77GHz Advanced Driving Assistance Systems (ADAS).\u00a0These high-frequency radar devices require a much tighter RF signal isolation and aggressive performance targets. Fan-out wafer level packaging (FOWLP), particularly embedded wafer-level ball-grid array (eWLB), is becoming a prevailing packaging choice.\u201d\n\nFontanilla pointed to a number of advantages to using a fan-out approach in automotive electronics. Among them:\n\n\u2022 It eliminates the need for a laminate substrate, replacing it instead with a copper redistribution layer (RDL). That RDL has a shorter connection distance, which in turn significantly reduces impedance.\n\n \u2022 As with many advanced packaging approaches, it also uses a smaller form-factor with lower parastics in the interconnection, which is critical for high-frequency applications.\n\n \u2022 A lower tolerance from wafer-level processing enables better yield, which is cost-effective for high-frequency applications;\n\n \u2022 And like all fan-outs, it provides more design flexibility because there is less routing interference and sufficient isolation for RF channels.\n\nOthers report a similar increase in attention by automotive manufacturers in advanced packaging.\n\n\u201cThere is a direct interest from car manufacturers themselves, and more specifically from Tier 1s,\u201d said Jean-Marc Yannou, senior technical director for ASE Europe. \u201cOEMs such as Audi recognize that differentiation in cars more and more comes from electronics because car manufacturers have solved most of the problems with internal combustion engines. So they go electric or hybrid. That\u2019s one differentiator. Another differentiator is that cars go higher-end all the time. Consumers wants a lot of gadgets in the car such as to be able to electronically adjust the seat, heat the seat, open the trunk or sunroof just by pressing a button.\u201d\n\nHe noted that automotive OEMs are well aware that vehicle differentiation is possible because of the electronic components. They also understand they can\u2019t differentiate by using a different CMOS technology, so it wouldn\u2019t make a lot of difference if they stay at an older node or move to 10/7nm. For automotive applications, the volumes are not high enough to see a significant price drop with device scaling.\n\n\u201cThe investment/CapEx levels are so high that very, very high volumes are needed for automotive makers to justify this move,\u201d Yannou said. \u201cThey will follow, but they follow well after the wireless industry. They are still about five years late on new CMOS technology node adoption.\u201d\n\nWhen it comes to packaging, though, it\u2019s a totally different story.\n\n\u201cIn the past it was the same as for silicon nodes,\u201d he said. \u201cWe were only following suit for the wireless industry. Now we do more and more things that are actually specific for automotive. Audi said all the differentiation of semiconductors, which in itself is the differentiating factor for electronics and for cars in general, will be packaging. Packaging will be used to bring the performance to a higher level with better thermal dissipation and lower electrical losses by shrinking the whole size.\u201d\n\nWhile the size of the electronics package can be a factor in design, given that there is plenty of space in a car, the real focus is on the ability to hide that circuitry. \u201cElectronics have to be invisible to the eye of the user,\u201d Yannou said. \u201cMiniaturization is one important factor.\u201d\n\nWhat kind of package?\n\n This isn\u2019t a simple swap out of one technology for another, however. There are a number of factors that need to be considered, such as how these devices will be used, the environment in which they will be used, and which type of packaging is best. Current bets are on fan-outs, but that also could shift to include other packaging types.\n\n\u201cThe environment it will be used in is critical,\u201d said Tom Salmon, vice president of collaborative technology platforms at SEMI. \u201cYou need to think about whether it will be under the hood, in the infotainment system, and how that will all look in 2025. Then, what are the materials that you will need for packaging? This may be very different because you no longer have Tier 1 and Tier 2 players defining things for the next 20 years like they did in the past.\u201d\n\nPackaging provides some flexibility because not all of the components need to be changed out for every new application. Working groups for the Heterogeneous Integration Road Map\u2014now being developed as a successor to the now-defunct International Technology Roadmap For Semiconductors\u2014working on a set of reference platforms for various markets such as automotive. Along with those markets different working groups are developing guidelines for 2.5D, 3D and fan-outs.\n\n\u201cThe goal is to create optimal platforms to develop the right components and still have the flexibility to use wafer-level packaging or chip-scale packages or whatever is optimized for a particular environment,\u201d Salmon said.\n\nFan-outs have been getting much of the attention lately. Jan Vardaman, president of TechSearch International, described fan-out wafer level packaging as a disruptive technology because there is no substrate and no traditional underfill, and all packaging can take place at the foundry or at the OSAT. But she noted that it does require chip-package co-design.\n\n\u201cFan-out wafer-level packaging is used for radar modules because of performance,\u201d said Vardaman. \u201cThis is RF, so lower parasitics are very important. There also is a lot of wirebond today and a lot of QFNs (quad-flat no-leads). The higher-pin-count microcontrollers are moving from wirebond to flip chip, and there are some system-in-package modules in various parts.\u201d\n\nReliability and the unknown\n\n Underlying this shift of how to put chips together is a concern for reliability, which always has been a concern in automotive and other safety-critical markets. This used to be a relatively straightforward discussion, but as more electronics are added to vehicles it has taken on a whole new level of complexity. Packaging is just one more facet of this discussion.\n\n\u201cThere needs to be a systematic approach to functional safety, but it\u2019s difficult to determine how we\u2019ll achieve that,\u201d said Robert Bates, chief safety officer for the Embedded Systems Division of Mentor, a Siemens Business. \u201cYou need to plan for the possibility of random failures. They do happen. And you need to do this with the understanding that both the hardware and the software are getting more complex. For machine learning and neural networking, the software is not directly implementing what it\u2019s doing. And for safety-critical hardware, typically this is a few generations old and you\u2019re adding unproven and different conditions into autonomous systems.\u201d\n\nWhile advanced packaging allows the ability to swap components in and out, the process is more predictable if those components were developed to work within those packages. The problem is that automakers are in the middle of a mad scramble to stay current with this technology, so they increasingly are making use of various commercially available components that were never developed for advanced packages and which have not been tested under extreme conditions for extended periods of time.\n\n\u201cThis puts Tier 1 and Tier 2 suppliers in a difficult spot,\u201d said Bates. \u201cThey need to provide more data.\u201d\n\nWhile that data can be simulated, there is far less data available from real-world testing, which in turn puts the onus on companies to do more verification and testing of the components.\n\n\u201cThere will be lots of testing as we \u2018follow the chips,'\u201d said Anil Bhalla, senior strategic marketing manager at Astronics. \u201cIt will be necessary to verify that designs and manufacturing operations produce solutions that are defect-free and safe according to regulatory standards.\u00a0Automotive will require the use of much more proven technology than consumer devices such as smart phones.\u00a0And this testing will need to happen both at the component and system-level.\u201d\n\nThat\u2019s just the starting point, too. \u201cSafety-critical applications will require additional testing as we better understand the defect mechanisms,\u201d said Bhalla. \u201cThe industry is assuming this will work based on early trials. More trials on a broader scale will support the gradual roll-out of this new technology.\u00a0The economics of autonomous driving is motivating the entire semiconductor ecosystem to evolve during this transition.\u201d\n\nSupply chain\u00a0shuffle\n\n The push toward autonomous vehicles and advanced electronics can be traced at least partly back to Tesla, which until several years ago was largely ignored by most big automakers as a niche player. Tesla\u2019s rollout of autonomous driving technology changed all of that, disrupting the supply chain and calling into question relationships that had been in place for decades. Carmakers suddenly had to scramble to stay relevant, ratcheting up their efforts in driver-assisted and fully autonomous engineering.\n\nBut rather than develop everything from existing components, they began searching for the best technology from wherever it was developed. This isn\u2019t just one or two components, though. It requires an assortment of electronic content, including RF sub-systems, power management units, motor controllers, a collection of sensors, and advanced logic that uses CPUs, GPUs, and a variety of hardware accelerators.\n\nMoreover, many of these components are being developed by companies that until recently had little or no interaction with each other. ASE\u2019s Yannou recalled one recent meeting set up by an OEM between ASE and the Tier 1, Valeo. \u201cThe Valeo people were a little bit upset in the beginning because they were asking what I was doing in the room with one of my sub sub sub contractors. The OEM told Valeo, \u2018You want to work with ASE, and together you will do this integrated module. And if you can bring everything on one side of it, then you can build the Wi-Fi module on the other side by freeing space on the second side of the PCB.\u2019\u201d This project is running today.\n\nThe OEMs also realize that today, building a car is like assembling modules together. \u201cWhen you look at the chassis of a Tesla vehicle it\u2019s overly simple. It\u2019s just tubes of aluminum welded together, a lot of batteries, an electrical motor, some electronics and that\u2019s it. Other car OEMs\u2014especially the European ones, but also the Toyotas and GMs and Fords of the world\u2014all used to be internal combustion engine specialists. But since the industry is moving away from those engines, things are changing. When you think about it now, a battery expert or an electronics expert can build a car as well as an engine expert like Ford, GM, Toyota, Renault, Peugeot, Volkswagen, Audi, BMW. All of these traditional car OEMs actually fear for their future business,\u201d Yannou said.\n\nThat has a direct effect on the packaging technologies being developed for automobiles. Wirebond is still used in a majority of automotive infotainment applications, such as the GPS, audio controller and USB devices, as well as in the MCUs used in electric and hybrid vehicles, the instrumentation, the power management systems, and body systems, which includes Ethernet, transceiver and interior lighting components. But for new systems, such as ADAS, as well as more current infotainment, instrumentation and body systems, fan-outs are beginning to gain traction, according to STATS\u2019 Fontanilla.\n\nIn addition, those packages are beginning to include the same kinds of chips that have been developed for the mobilility market. \u201cToday the dominant pitch is, \u2018If it works in smart phones, why wouldn\u2019t it work in cars,'\u201d said Yannou. \u201cAnd as a matter of fact, very often there\u2019s very little \u2014 if nothing \u2014 to be changed so that it works. It\u2019s just a matter of qualifying it long enough and hard enough. The technologies are quite robust actually.\u201d\n\nExactly how this will play out in the packaging world isn\u2019t obvious, though.\n\n\u201cLarge IDMs like Infineon or large Tier 1s like Continental tend to use mature technologies, which are very well proven as being robust for automotive like QFP (quad flat package) or even through-hole type of packaging,\u201d Yannou said. \u201cThey would refrain from using organic substrates BGAs, LGAs. They would refrain from using QFN because you don\u2019t see the leads, so you can\u2019t check on the solder connections to the board. And these are valid reasons. Smaller players may dare to use new technology, so that kind of shakes the whole supply chain forces large players into rethinking whether to move forward with some new technologies, as well.\u201d\n\nRelated Stories\n\n Advanced Packaging Picks Up Steam\n\n System-in-package technology is poised to roll out across multiple new markets.\n\n Tech Talk: ADAS\n\n What will change in automotive design on the road to autonomous vehicles.\n\n What\u2019s Missing In Advanced Packaging\n\n When it comes to multi-board and multi-chips-on-a-board designs, do engineers have all the tools they need?\n\n Advanced Packaging Goes Mainstream (blog)\n\n After decades of work, there are now plenty of commercial success stories"},
{"url": "https://www.linkedin.com/pulse/differences-between-men-women-vastly-exaggerated-adam-grant", "link_title": "Differences Between Men and Women Are Vastly Exaggerated", "text": "If you\u2019ve read the recent memo by a Silicon Valley engineer about diversity, you probably had a strong emotional reaction.\n\nIt\u2019s always precarious to make claims about how one half of the population differs from the other half\u2014especially on something as complicated as technical skills and interests. But I think it\u2019s a travesty when discussions about data devolve into name-calling and threats. As a social scientist, I prefer to look at the evidence.\n\nThe gold standard is a meta-analysis: a study of studies, correcting for biases in particular samples and measures. Here\u2019s what meta-analyses tell us about gender differences:\n\n1. When it comes to abilities, attitudes, and actions, sex differences are few and small.\n\nAcross 128 domains of the mind and behavior, \u201c78% of gender differences are small or close to zero.\u201d A recent addition to that list is leadership, where men feel more confident but women are rated as more competent.\n\nThere are only a handful of areas with large sex differences: men are physically stronger and more physically aggressive, masturbate more, and are more positive on casual sex. So you can make a case for having more men than women\u2026 if you\u2019re fielding a sports team or collecting semen.\n\n2. In the U.S., boys aren\u2019t better at math than girls.\n\nAcross nearly 4,000 studies, the average gender gap in math achievement is not statistically different from zero. The two sexes have very similar variances, with men showing slightly more variability. And there are just as many cases where girls outperform boys in math as vice-versa:\n\n3. Where male advantages in math ability exist, they\u2019re heavily influenced by cultural biases.\n\nGirls do as well as boys\u2014or slightly better\u2014in math in elementary, but boys have an edge by high school. Male advantages are more likely to exist in countries that lack gender equity in school enrollment, women in research jobs, and women in parliament\u2014and that have stereotypes associating science with males.\n\nIf you don\u2019t see bias there, try this: when teachers know students\u2019 names, boys do better on math tests. Yet when grading is anonymous, girls do better on math tests. And before a math test, reminding college students of their gender leads girls to perform 43% worse than boys. But if you just call it a problem-solving test, the gender gap in performance disappears.\n\nBiases affect men too. Women are stereotyped as more empathetic, and they do score higher than men when you test the ability to read other people\u2019s thoughts and feelings. But if you don\u2019t introduce it as an empathy test, the gender gap vanishes.\n\n4. There are sex differences in interests, but they\u2019re not biologically determined.\n\nThe data on occupational interests do reveal strong male preferences for working with things and strong female preferences for working with people. But they also reveal that men and women are equally interested in working with data.\n\nSo why are there so many more male than female engineers? Because women have systematically been discouraged from working with computers. Look at trends in college majors: since the 1980s, the proportion of female majors has gone up in science and medicine and law, but down in computer science.\n\nWe know that interests are highly malleable. Female students become significantly more interested in science careers after having a teacher who discusses the problem of underrepresentation. And at Harvey Mudd College, computer science majors were around 10% women a decade ago. Today they\u2019re 55%.\n\nIt\u2019s time to stop making mountains out of molehills. If men are from Mars, it looks like women are too.\n\nAdam Grant is an organizational psychologist, Wharton\u2019s top-rated professor, and a New York Times bestselling author who shares insights every month in GRANTED."},
{"url": "https://www.youtube.com/watch?v=fPC4TtEHaK4&feature=youtu.be", "link_title": "(13) Bitcoin live chart", "text": "Rating is available when the video has been rented.\n\nThis feature is not available right now. Please try again later."},
{"url": "http://www.ethingtoday.info/2017/08/watching-youtube-without-using-mouse.html", "link_title": "Watching YouTube Without Using Mouse", "text": "Have you ever used shortcut keys when watching videos on Youtube? In this post, I am going to share with you some extremely useful shortcut keys. They will help you to save much time when watching videos on Youtube because you do not have to use the mouse to interact with it. Note: This can only be applied on\u00a0PC. \n\n \n\n\n\nWhile watching videos, if you want to watch in full screen mode, you can press F to activate it. T F again or use the ESC\n\nTo view the video from the beginning, pressing\u00a0Home or Zero key on the keyboard could be useful in this case.\n\n\u2465 Go fast to the end of the video\n\nEnd key, it will move you to the next video. You should note one thing that for single video, it will move you to the last seconds of the video. However, if you are watching videos in a Playlist, when using thekey, it will move you to the next video.\n\n1 to 9 on the keyboard to rewind or go forward when watching videos. For example, when you press\u00a01, 2 or 3, the video will go forward\u00a0by\u00a010%,\u00a020% and 30% of the video respectively and so on until the end of the video. In this case, use the keys fromtoon the keyboard to rewind or go forward when watching videos. For example, when you pressthe video will go forward\u00a0byandof the video respectively and so on until the end of the video."},
{"url": "https://tyk.io/blog/migrating-monolith-apis-microservices/", "link_title": "Migrating from a Monolith to APIs and Microservices", "text": "Nearly every company must deal with legacy code. They can\u2019t just start over with a green field project. So, how can teams dealing with one or more monolithic applications migrate to APIs and microservices? And, how can you make this transition without halting all new feature development while everything is transitioned?\n\nThis was a recent discussion with one organization as part of my API training workshop. I am going to share with you a summary of my strategy for migrating your existing applications to an API-centric, microservice architecture.\n\nDevelopers familiar with migrating legacy code to a new technology or platform may have used the strangler pattern before. Martin Fowler describes the strangler application as:\n\nWe will use this technique, but rather than replacing an outdated ORM layer or some other library, we will use it to drive significant architectural change over time.\n\nNote: Your application may be considerable in size. When this is the case, apply this process to a smaller portion and keep iterating across the whole of the application. It is often a fool\u2019s errand to try and take on a large-scale application all at once, as requirements and priorities change.\n\nToo often, we get stuck focusing on the internal details of our solution: the various classes, database tables, and other components that make up our app. When migrating to an API-centric approach, we should start from the outside-in by considering how the world (i.e. our apps, partners, and public developers) needs to talk to our software. I cover this process in detail in my book and workshop.\n\nThe output of this process should be an OpenAPI definitions that capture each endpoint, including the request payload and response formats supported.\n\nOnce your API is defined, we want to start integrating the definition into your code a little at a time. Start by defining a clear interface that the application will use \u2013 commonly a facade. Facades are used to define the interface to the application, wrapping the existing legacy code that we eventually want to modify or remove behind it.\n\nInternally, it is still the same bunch of code that it has always been \u2013 no changes should occur here (yet). Externally to the code, it looks like a clean interface representing the ideal state of your design. By defining a clear interface upfront, we start to build boundaries around the areas where we will eventually need web APIs and microservices.\n\nUnlike some legacy modernisation practices, our facade best serves us when we try to mimic the web design request and response payloads. This doesn\u2019t mean that you need to pass JSON or XML inside your legacy app. Rather, do your best to provide a facade that accepts similarly structured input and output. The facade will have to do a little translation logic, but overall should serve as an adapter between your ideal API design and the internal legacy details. Ultimately, you will have one or more facades that look similar to your target API design, but operate within the same codebase.\n\nNow that we have facades that represents our API, the next step is to break each facade into smaller units of code that will represent our microservices. If you are not pursuing microservices at this time, then this step will still result in a more modular API.\n\nTo decompose the facades, examine each method/function that represents an equivalent API endpoint. Look for complex concerns that would benefit from being separated into microservices (or modules). For each one, define a service object that will represent the service interface, then migrate the related code to the service object.\n\nAfter you complete this step, you will have facades that hide the details of one or more service objects, each of which decompose the legacy application into smaller, bounded concerns. Yet, we have still not moved to APIs or microservices \u2013 this is the next step.\n\nYou have decomposed your application into smaller, more manageable modules that represent your target API design. The next step is to break out the code behind your facades into your desired web APIs. The legacy code will become a consumer of the API by changing each facade to call the API rather than calling the legacy code. The client code within the facade is responsible for catching errors, such as API token expiration, network failures, and other problems often encountered in production environments.\n\nWhen moving to web APIs, it is important to install an API gateway to provision API tokens for your legacy app, enforce security, support scaling of backend resources, and for generating log and analytics essential to managing your API properly.\n\nIf you are choosing to move to a microservices, then your web APIs will act as a facade to your microservices (just over HTTP rather than in-process). Each service object should be separated from the legacy code, much like the previous step separated the facades. Each service should be instrumented to track call chains, capture logs for troubleshooting, and other disciplines necessary for a microservice architecture.\n\nI have encountered some teams that have not heeded this advice. The result is two parallel initiatives, where legacy systems continue to evolve while other teams are attempting to build out web APIs that represent the state of the legacy application from months (or years) ago. The modernization process never catches up and the legacy code is never able to be sunset and eventually shutdown.\n\nInstead, find ways to creatively strangle the legacy code into management facades and service objects, migrate them to a web API, and perhaps decompose them further into microservices as desired. The result should be a modular, API-centric architecture that supports operations today and begins to open new opportunities for innovation on top of a well-design API."},
{"url": "http://fox40.com/2017/08/04/csu-eliminates-math-requirement-to-boost-graduation-rates/", "link_title": "CSU Eliminates Math Requirement to Boost Graduation Rates", "text": "SACRAMENTO -- For many college students, math is a dreaded subject.\n\n\"Taking all these unnecessary math classes is just unnecessary,\" Noah Rodriguez, a freshman at Sac State, told FOX40. \"If you are taking classes for your major, you can graduate on time.\"\n\nGetting students to graduate on on time is the reason behind the California State University system's recent decision to do away with a requirement for students to take intermediate algebra.\n\nFor years, that course was a prerequisite to complete the math requirement even for students weren't math or science majors.\n\n\"This previous idea of 'algebra for all,' is that really what we're trying to do? Is that necessary for everybody in every career? Their answer would be no,\" Student Affairs Vice President Ed Mills said.\n\nThe latest figures show just 21 percent of California State University students graduate in four years. The goal now is to get 40 percent of students in and out in that time frame.\n\nMills says the CSU curriculum changes are on the leading edge compared to most universities nationwide.\n\n\"It's a little radical. It's a change,\" Mills said. \"It's progressive but we think that it's really needed and it's going to be a big help to all of our students.\"\n\nThe new rule goes into effect in the fall, meaning Rodriguez still has to take the class."},
{"url": "https://www.villagevoice.com/2005/02/01/i-robot/", "link_title": "The Extropians (2005)", "text": "Harvard president Lawrence Summers is facing his latest\u2014and biggest\u2014public relations disaster. Ever since he suggested in a speech last month that the lack of top female scientists could be due to \u201cinnate differences\u201d in genetics and upbringing between men and women, the national outcry has been fierce. \u201cIt\u2019s pandemonium,\u201d says Harvard junior Simon Rich, president of The Harvard Lampoon, of the situation on campus. Some alumni are threatening to stop giving money. The New York Times ran a major article questioning Summers\u2019s leadership skills. The National Organization for Women has called for his resignation.\n\nI went to school down the street from Harvard, at M.I.T. While growing up, I was never made to feel that there were \u201cinnate differences\u201d between men and women when it came to anything; besides, both of my parents were scientists. My father, an eccentric chemistry professor, only once remarked that there were innate differences between me and my two brothers. \u201cYou\u2019re smarter than they are,\u201d he said. \u201cYou should be a scientist.\u201d He\u2019d sneak me into the nearby Institute for Advanced Study at Princeton University on weekends to do my homework as a kid and started me on thick textbooks about physics and organic chemistry as soon as I learned how to read. By the time I was 12, he had me proofreading the scientific manuscripts he was preparing for publication, and at age 13 I applied to work in my first chemistry lab. I had just bought my first Kraftwerk record\u2014The Man-Machine\u2014and decided that robots were the future and that I wanted to be one.\n\nThat summer I worked in my first laboratory, teaching a giant magnet connected to a robot arm how to conduct chemistry experiments. I met a kid there who was a year or two older than me, and he was applying to a place called M.I.T., where, according to what I\u2019d heard, everyone liked robots if they weren\u2019t robots already, and where everyone believed technology was the future. I looked up to him, partially because he seemed to know more about robots than I did, and we became fast friends. We fell out of contact, but a few years later, he\u2014by then a student at M.I.T.\u2014encouraged me to apply there, and I did.\n\nA few months before I got to campus in the autumn of 1997, I got a letter from a tiny group calling itself the M.I.T. Extropians. They had mailed an inflammatory\u2014and wholly unauthorized\u2014eight-page pamphlet to the entire incoming freshman class, myself included. In it, they praised hoary teenage standbys like Ayn Rand, Beethoven, and Nietzsche; waxed philosophical about life extension, cybernetics, and neural networks; and disturbingly issued several sweeping statements about women and minorities, lashing out against affirmative action and M.I.T.\u2019s liberal diversity policies. These guys were much more strident and extreme than Summers could ever be accused of being, even considering his worst verbal gaffes.\n\nThe pamphlet included choice lines like \u201cThe average woman or \u2018underrepresented minority\u2019 at M.I.T. is less intelligent, less intellectual, and less ambitious. . . . The average woman majors in the softer, less mathematical majors, by contrast with the average man, who majors in the harder, more mathematical majors.\u201d The examples they offered of these \u201csofter\u201d majors were biology, my chosen field of chemistry, materials science, architecture, and civil and environmental engineering. It went on to say, \u201cAsk upperclasswomen, better yet ask a sorority (who set out to rush 40% of the freshwomen every year), how often a group of women will sit down on the weekend, or Friday night, to discuss what Bell\u2019s Theorem and the Aspect Experiment imply for a hidden variables interpretation of quantum mechanics.\u201d\n\nThe pamphlet ended with a ludicrous \u201cOpen Letter to the Prometheans, Class of 2001,\u201d a list of recommendations that included reading bad sci-fi and listening to chestnuts like Mahler\u2019s Fifth Symphony, and\u2014finally and most crucially\u2014the signatures of all three of the M.I.T. Extropians. Finally I could see who these morons were.\n\nOne of the three names was my buddy from high school\u2014the one who encouraged me to come to M.I.T. in the first place. I felt like I\u2019d been kicked in the face.\n\nWhen I got to campus, I confronted him. \u201cHow could you do this?\u201d\n\nHe looked bemused. \u201cI didn\u2019t mean you,\u201d he said. \u201cYou deserve to be here. We meant, like, other girls.\u201d\n\nI wanted to punch him but restrained myself. Besides, I\u2019d never punched anyone in my life. And he was taller.\n\nThat\u2019s the only time in my life that I\u2019ve felt discriminated against for being a female in science. Happily, M.I.T. was an open, democratic system\u2014no one cared if you were male or female, black or white, robot or nonrobot, as long as you could do the work. Being a great scientist or engineer has little to do with those superficial human designations. As students, we were joined by a single bond: We were all nerds. Besides, there were plenty of other girls at M.I.T., and I was relieved to find out that nearly all of my classmates thought that the Extropians were completely out of their minds. They got into major trouble with the administration, and M.I.T. refused to recognize them as a legitimate student group. A few guys from the M.I.T. humor magazine even dubbed themselves the M.I.T. Entropians, and handed out hundreds of copies of their own pamphlet, a ferocious line-by-line parody of the original.\n\nYears later, after I graduated, I ran into two of the three M.I.T. Extropians in New York City. What had happened to those three weirdos who fantasized about being lone misunderstood geniuses, of being Ender in Orson Scott Card\u2019s sci-fi classic Ender\u2019s Game? The ones that were going to cryogenically freeze themselves for life after death, who were going to upload their brains onto computers, who were investigating the deepest issues in artificial intelligence? None of them were actively doing science anymore. My former friend had discovered raves and told me excitedly that he\u2019d spent the past year getting wildly immersed in San Francisco\u2019s psychedelic trance scene.\n\nGeeta Dayal is a writer living in Brooklyn. \u201cThe Acid Test,\u201d her story on LSD research, appeared last month in the Voice Education Supplement."},
{"url": "https://www.facebook.com/BankofAmerica/posts/1411974255523473?pnref=story", "link_title": "Bank of America just helped 25 fraud transactions happen to students", "text": ""},
{"url": "https://web.archive.org/web/20111207091647/http://www.webcrunchers.com:80/stories/index.html", "link_title": "The Real Captain Crunch", "text": "How I got into phone phreaking\n\n This is how it all got started. \n\n \n\n My first visit with the blind kids \n\n How the toll switching system works.\n\nFirst bust \n\n I knew it was coming.\n\n \n\n My arrest\n\n Details of what they did to me."},
{"url": "http://newsplank.com/", "link_title": "Google News Blows Now Try These Guys.. :o)", "text": "Switch to our automated Mobile Version if you wish! \n\n\n\n"},
{"url": "https://thisbitofcode.com/managing-localization-rails/", "link_title": "Managing localization files in Rails", "text": "In large applications, managing localization files can become a behemoth of a task, especially if you\u2019re working with external translation teams who will be providing you with proper translations of YAML files (or XML, Ruby hash, files, etc.).\n\nMost Rails applications that have an international user base will have localization features built in. This means that in your views, rather than having something like this\n\nYou\u2019ll have something like this instead\n\nThis method will search for the matching key in your localization file and display the matching string based on your app\u2019s locale setting. Most of how to set your app-wide locale can be easily found by Googling. This blog post will cover the lessons I\u2019ve learned with to manage localization in a more organized way while working on a large Rails application.\n\nThe first and most important thing when working in a team environment on a large Rails application is to agree upon a formatting rule when adding in new localization texts. There are several reasons for this.\n\nLet\u2019s go over these one by one.\n\nIt\u2019s not uncommon to see thousands of localization keys in a large Rails application. It may be easy to find duplicate strings when your application is small, but when your localization files contain thousands of strings, it can be difficult to search for the correct key you want to use when building out your HTML pages. Also, not having a team wide naming convention set can lead to duplicate strings, which can cause hard to catch bugs in your views.\n\nLet\u2019s say that for example, you have two keys in your YAML file that goes like this:\n\nAll three keys will display the text \u201cWelcome back\u201d with a few differences. The keys \u201cwelcome_back\u201d and \u201clabel_welcome_back\u201d will both display \u201cWelcome back!\u201d while the key \u201ctext_welcome_back\u201d will display the same text with an exclamation point.\n\nLet\u2019s say you have an application with multiple developers working on it. If you have a localization file like the one above, different developers may start using different keys based on what their \u201cFind\u201d function in their editor will locate first. Depending on what text you\u2019re supposed to display, some developers may end up displaying the incorrect version of the text. For example, what if you\u2019re not supposed to display the exclamation point in the view, but you end up doing so because you accidentally used \u201ctext_welcome_back\u201d?\n\nThe issue becomes even more transparent when you have to start changing the actual text in the views. To change the text of the keys, you\u2019ll have to go into your localization files, try to figure out which text you\u2019re supposed to change, and hope that changing one of the \u201cwelcome_back\u201d texts won\u2019t negatively affect all the other views that might be incorrectly using that key. For example, if you change the text for \u201cwelcome_back\u201d to close a ticket for your specific page, it has a chance of creating a bug for all other pages that weren\u2019t supposed to be using that specific \u201cwelcome_back\u201d key.\n\nTherefore, it\u2019s important to set team wide rules on how to name localization keys. It really doesn\u2019t matter how you do it. The key is to set a ground rule and have your team be consistent with it when adding new keys. I\u2019ve seen it done in different ways. Some teams have duplicate texts, but have different keys based on where they\u2019re supposed to use that specific version of the key. For example, \u201ctext_welcome_back\u201d may just be a simple text while \u201clabel_welcome_back\u201d may only be used as a label on a form. I\u2019ve seen teams just stick with \u201cwelcome_back\u201d and that\u2019s the only key for that text that they\u2019re allowed to have.\n\nOne final important rule that you should set (because I always have trouble keeping this consistent) is whether you should include punctuation in your keys or not, and if you do, how you should name your keys. For example, if you really do need to have \u201cWelcome back!\u201d as the text, would you leave the exclamation point in your localization text or hardcode it into your HTML? If you have it in your localization file, should you name your keys differently like \u201cwelcome_text_exclamation\u201d? Personally, I think you should have the exclamation point inside your localization files because certain languages like Spanish will have the upside down exclamation point before the first word. In these cases, it\u2019ll make translating your app into other languages much easier since punctuation will already be accounted for in your localization files for languages that treat them differently.\n\nI mentioned one potential issue that may arise from having inconsistent naming conventions where it may become difficult to figure out which locale string you\u2019re supposed to be using and/or changing as you code. Another benefit of having consistent naming conventions is that you eliminate duplicate keys/value pairs in your localization files. For example, when I search for a text to use in a localization file, I\u2019ll usually just use my text editor\u2019s search feature to look for which key I should be using. Sometimes, I\u2019ll find two key/value pairs with the same exact texts but different keys. Sometimes I\u2019ll find similar texts but with different keys. This always makes me go, \u201cOkay\u2026 which one am I supposed to be using?\u201d. \u00a0Not having any duplicate keys and texts in your localization files reduces this unnecessary mental road block.\n\n#3 \u2013 Reduces chance for Rails app to fail booting due to errors in formatting\n\nIf your localization files have improperly formatted strings, your Rails app will fail to boot. Worst of all, the logs will not tell you exactly which of your localization file is causing the issue nor will it tell you which line. This is incredibly frustrating to debug and always has me performing a manual binary search (temporarily delete first half of a YAML file, see if app boots up, if not repeat) through a large localization file(s) to figure out which key/value pair is causing the issue. And yes, sometimes it\u2019s literally one key/value pair that\u2019s blocking the entire app from booting.\n\nThe cause for improper string formatting can vary. Sometimes, it can even be caused by inconsistent string formatting between various key/value pairs. Therefore, it\u2019s much more sane to keep a consistent rule when it comes to formatting the string values for your texts. I like to follow this rule when it comes to Rails apps.\n\nIf you follow these two rules consistently, you will have zero (or very few) issues when it comes to experiencing boot up errors in your Rails apps. In addition, having these set of rules will make working with third party translation vendors much easier.\n\n#4 \u2013 Establishes clear set of rules when working with vendors\n\nAs mentioned in the last point, if you\u2019re working with professional third party translation vendors, having a clear set of formatting rules will make your life 10 times easier. If you don\u2019t set some sort of ground rules with your translation vendor, they may send you huge YAML files with inconsistent formatting rules, causing your Rails app to crash. However, if your team already has a clearly defined set of rules, you\u2019ll have a more smooth experience working with third party vendors.\n\nIt is entirely possible to have your app translated into different languages automatically via services like Google Translate and Yandex. Sounds magical? Well, there are advantages and disadvantage to this method.\n\nThe way this works is that you\u2019ll write a script that you run as a rake task that will grab all key/values of your locale texts, loop through them, send a request to a translation API (Google Translate, Yandex, etc.), and build up a new YAML file for the language of your choice. This sounds great until you see the results which is in the disadvantages list.\n\nYep, the resulting translation usually is terrible, especially if it\u2019s a language that\u2019s difficult to translate into. For example, machine translation from English to Spanish may work \u201cok\u201d, but from English to something like Chinese probably will not. This is where working with professional translation services come in if your app requires proper translations.\n\nIf you want your app to read well (or even make sense) in other languages, you\u2019ll have to bring in a professional translation vendor. Most of the time, what you\u2019ll be doing is handing over your main YAML (or XML, Ruby hash file, or etc.) to them and then they\u2019ll send you over the YAML files in the languages you requested. You\u2019ll then take these YAML files and then merge them back into your codebase.\n\nI\u2019ve already mentioned that you should set text formatting rules with your vendor so that you don\u2019t have any problems with your app booting up. Assuming you have this part taken care of, I\u2019ll list a few things that can make your life working with vendors much easier.\n\n#1 \u2013 Setting up a workflow for your translation files\n\nThere are multiple ways that you can work with translation vendors, but the way I\u2019ve worked with them involved me providing the English YAML files (usually named \u201cen.yml\u201d) and then having the translation vendor send the translated files back to me (includes something like \u201cko.yml\u201d, \u201ces.yml\u201d, \u201cfr.yml\u201d and more). I would then take these files and merge them back into the codebase, and then start the local Rails server to check that the app boots up properly.\n\nYou can utilize different ways of managing the sending and receiving of the YAML files. I\u2019m sure platforms like Dropbox and Google Drive work fine (heck, I\u2019m sure you can even email them back and forth, although I wouldn\u2019t recommend it since managing that would get confusing real quick), but I found that using Github repos work even better. With Github repos, you can easily merge files back and forth between your main code repository and your \u201clocale files\u201d repository, and use git to resolve any differences.\n\nWith Github repos, anytime there were changes in the main YAML file (in my case, English), I would sync that file into the locale files repository on Github. The translation vendor would then pull down the latest \u201cen.yml\u201d file, translate the texts, and then push the latest set of YAML files in different languages to the repository. I would then pull down the latest YAML files, and then merge them back into the app\u2019s codebase.\n\nOf course, there are different software solutions out there to manage this process, but I found using git to be the most simple and economical to manage localization files when working with translation vendors.\n\nSometimes, you might run into issues where you\u2019re displaying a text that exists in your default YAML file (en.yml for me) but that key doesn\u2019t exist in the YAML files provided by your translation vendor. This can happen since the translation vendors are humans at the end of the day, they might miss translating some texts. If this happens, it\u2019s possible to \u201cfall back\u201d on your autogenerated locales if you happened to have those lying around.\n\nTo do this, simply have two sets of translation files for each languages. First set is the custom ones from the translation vendor and the second set is the autogenerated ones that you built with making API calls to services like Google Translate.\n\nYou want to then organize your translations files in your folder.\n\nI like to put all of the autogenerated localization files under and then the custom translated localization files under folder.\n\nAnd then in your\n\nWhat this will do, line, is that it\u2019ll load up all of the localization files in your locales folder, and then sort them alphabetically. And since the word \u201ccustom\u201d comes after \u201cauto_generated\u201d, when Rails looks up the key/value pairs of locales, it\u2019ll first look for the locale key in the auto_generated one and then in the custom one. And since the custom translation comes after the auto_generated one, the custom translation will be utilized rather than the auto_generated one. And if for some reason the custom translation does not exist, the auto_generated translation will be utilized instead since it has already been loaded by Rails.\n\nThis way, you can always have a fallback option in case there are missing locale keys in your custom localization files.\n\nWhen working with translation vendors, there is a chance that the localization files that you get back from them will have typos. The typo I\u2019ve seen most often have been that the language key that\u2019s on the top of the YAML file would be a different one.\n\nFor example, here\u2019s a typical YAML file that would represent a Korean translation.\n\nSee that \u201cko\u201d definition? Rather than that, sometimes I would get back files with some other language key instead, like \u201ces\u201d which is usually used for \u201cSpanish\u201d. I\u2019ve had times where I would merge this typo into the codebase, deploy and then have all the Korean users have their version of the app displayed in Spanish.\n\nThis is one of those typos that\u2019s easy to miss and can cause all sorts of annoyance for your international users. Thus, I like to write tests that check for the integrity of the YAML files before you merge and deploy your app. Here\u2019s are some typical tests I like to write to check for the integrity of the YAML files.\n\nAll this test does is it loads up the individual YAML files, and then check for the integrity of the key of that YAML file. For example, it\u2019ll load up a French YAML file, and then make sure that the key definition of that file is set to \u201cfr\u201d, so that French users won\u2019t end up accidentally seeing Russian (it\u2019s happened once\u2026). This is just an example of a test you can write to check for the integrity of your localization files. I\u2019m sure that there are different types of tests that you can write depending on your situation.\n\nI wrote in the first sentence of this post that managing localization in large apps can be a behemoth of a task. It is if you don\u2019t have a process set up for managing it. However, if you set up a organization wide process for managing it in a systematic way, it becomes one of those 5 minute tasks that\u2019s painless to do. I\u2019m hoping this post can be helpful for those with apps that need to support multiple languages."},
{"url": "https://arstechnica.com/gadgets/2017/08/radio-navigation-set-to-make-global-return-as-gps-backup-because-cyber/", "link_title": "Radio navigation set to make global return as GPS backup", "text": "Way back in the 1980s, when I was a young naval officer, the Global Positioning System was still in its experimental stage. If you were in the middle of the ocean on a cloudy night, there was pretty much only one reliable way to know where you were: Loran-C, the hyperbolic low-frequency radio navigation system. Using a global network of terrestrial radio beacons, Loran-C gave navigators aboard ships and aircraft the ability to get a fix on their location within a few hundred feet by using the difference in the timing of two or more beacon signals.\n\nAn evolution of World War II technology (LORAN was an acronym for long-range navigation), Loran-C was considered obsolete by many once GPS was widely available. In 2010, after the US Coast Guard declared that it was no longer required, the US and Canada shut down their Loran-C beacons. Between 2010 and 2015, nearly everyone else shut down their radio beacons, too. The trial of an enhanced Loran service called eLoran that was accurate within 20 meters (65 feet) also wrapped up during this time.\n\nBut now there's increasing concern about over-reliance in the navigational realm on GPS. Since GPS signals from satellites are relatively weak, they are prone to interference, accidental or deliberate. And GPS can be jammed or spoofed\u2014portable equipment can easily drown them out or broadcast fake signals that can make GPS receivers give incorrect position data. The same is true of the Russian-built GLONASS system.\n\nOver the past few years, the US Coast Guard has reported multiple episodes of GPS jamming at non-US ports, including an incident reported to the Coast Guard's Navigation Center this June that occurred on the Black Sea. South Korea has claimed on several occasions that North Korea has jammed GPS near the border, interfering with aircraft and fishing fleet navigation. And in the event of a war, it's possible that an adversary could take out GPS satellites with anti-satellite weapons or some sort of cyber-attack on a\u00a0satellite network.\n\nAs\u00a0Director of National Intelligence Dan Coates told the Senate Select Committee on Intelligence\u00a0in May:\n\nThe risk to GPS has caused a number of countries to take a second look at terrestrial radio navigation. Today there's broad support worldwide for a new radio navigation network based on more modern technology\u2014and the system taking the early lead for that role is eLoran. As Reuters reports, South Korea is preparing to bring back radio navigation with eLoran as a backup system for GPS, and the United States is planning to do the same.\n\nThe eLoran system gets its enhanced accuracy in much the same way that enhanced GPS gear squeezes greater accuracy out of the civil GPS signal for tasks such as surveying and mapping\u2014by using differential correction. A stationary receiver at a known fixed location checks the signal arriving from the beacon and measures the difference between its actual distance from the beacon and the distance calculated from the signal (based on the difference between the signal's timestamp and the time it was actually received).\n\nIn differential GPS, the differential information is broadcast by a base station at the known differential point; in eLoran, the data is fed back to the eLoran transmitter, and the transmitter applies the differential correction to its own signal. Since eLoran is regional, the differential calculation remains relatively accurate for its entire coverage area.\n\nBecause it uses low-frequency radio waves (in the 90 to 110 kHz range), it's not likely that you'll see eLoran integrated into your smartphone. While the antenna required for receiving eLoran signals is relatively small (about two inches square), that's a fairly massive amount of real estate for a smartphone to dedicate to a backup navigation system. But that size could be reduced with some investment in antenna miniaturization. And while eLoran only works in two dimensions (it doesn't provide altitude data) and only works regionally (with a range of 800 miles), it has one major advantage over GPS: its powerful low-frequency signals are far less susceptible to jamming or spoofing. The signal from eLoran beacons is 1.3 million times stronger than GPS signals. A 2006 MITRE study found that attempts to jam or spoof eLoran would be highly unlikely to work.\n\n\"[eLoran] is a deterrent to deliberate jamming or spoofing, since such hostile activities can be rendered ineffective,\" said\u00a0Brad Parkinson, the retired US Air Force colonel who managed the original GPS development program, according to\u00a0Reuters. A report Parkinson contributed to for an Institute for Defense Analyses Independent Assessment Team in 2014 found that \"eLoran is the only cost-effective backup for national needs.\"\n\nThe administrations of both George W. Bush and Barack Obama pushed for a national eLoran system, but their efforts were never funded by Congress. However, the version of the Department of Homeland Security funding bill for 2018 just passed by the House of Representatives in July includes language calling for DHS to fund the construction and maintenance of a new eLoran system \"as a complement to, and as a backup for\" the GPS system. And the South Korean government already has pushed forward plans to have three active eLoran beacons by 2019\u2014that's enough to provide accurate fixes for all shipping in the region should North Korea (or anyone else) attempt to block GPS again."},
{"url": "http://thechoice.one/concept-of-god-in-major-religions-video/", "link_title": "Concept of God in Major Religions", "text": "There has always been a question in the minds of people about the existence of God. All major religions have believed in one god or another. Thus, believing in a supreme being that is omnipotent and omniscience is essential to the existence of mankind.\n\nThis video lecture (with Q&A Session) by Dr. Zakir Naik\u00a0tackles the issue of existence of God from a different perspective including starting off with how different people view God. It first categorizes the belief of the major religions into 5 types of religions. Then, it describes the concept of God for world\u2019s several major religions including Hinduism, Sikhism, Judaism, and others. The lecture ends with a detailed analysis of concept of God in Islam and how it perfects the concept of God."},
{"url": "http://www.paulgraham.com/say.html", "link_title": "What You Can't Say (2004)", "text": "\n\n\n\n \n\n\n\n \n\n\n\n January 2004\n\n\n\n Have you ever seen an old photo of yourself and been embarrassed at the way you looked? Did we actually dress like that? We did. And we had no idea how silly we looked. It's the nature of fashion to be invisible, in the same way the movement of the earth is invisible to all of us riding on it.\n\n\n\nWhat scares me is that there are moral fashions too. They're just as arbitrary, and just as invisible to most people. But they're much more dangerous. Fashion is mistaken for good design; moral fashion is mistaken for good. Dressing oddly gets you laughed at. Violating moral fashions can get you fired, ostracized, imprisoned, or even killed.\n\n\n\nIf you could travel back in a time machine, one thing would be true no matter where you went: you'd have to watch what you said. Opinions we consider harmless could have gotten you in big trouble. I've already said at least one thing that would have gotten me in big trouble in most of Europe in the seventeenth century, and did get Galileo in big trouble when he said it\u2014that the earth moves. [1]\n\n\n\n It seems to be a constant throughout history: In every period, people believed things that were just ridiculous, and believed them so strongly that you would have gotten in terrible trouble for saying otherwise.\n\n\n\nIs our time any different? To anyone who has read any amount of history, the answer is almost certainly no. It would be a remarkable coincidence if ours were the first era to get everything just right.\n\n\n\nIt's tantalizing to think we believe things that people in the future will find ridiculous. What would someone coming back to visit us in a time machine have to be careful not to say? That's what I want to study here. But I want to do more than just shock everyone with the heresy du jour. I want to find general recipes for discovering what you can't say, in any era.\n\n\n\nThe Conformist Test\n\n\n\nLet's start with a test: Do you have any opinions that you would be reluctant to express in front of a group of your peers?\n\n\n\nIf the answer is no, you might want to stop and think about that. If everything you believe is something you're supposed to believe, could that possibly be a coincidence? Odds are it isn't. Odds are you just think whatever you're told.\n\n\n\nThe other alternative would be that you independently considered every question and came up with the exact same answers that are now considered acceptable. That seems unlikely, because you'd also have to make the same mistakes. Mapmakers deliberately put slight mistakes in their maps so they can tell when someone copies them. If another map has the same mistake, that's very convincing evidence.\n\n\n\nLike every other era in history, our moral map almost certainly contains a few mistakes. And anyone who makes the same mistakes probably didn't do it by accident. It would be like someone claiming they had independently decided in 1972 that bell-bottom jeans were a good idea.\n\n\n\nIf you believe everything you're supposed to now, how can you be sure you wouldn't also have believed everything you were supposed to if you had grown up among the plantation owners of the pre-Civil War South, or in Germany in the 1930s\u2014or among the Mongols in 1200, for that matter? Odds are you would have.\n\n\n\nBack in the era of terms like \"well-adjusted,\" the idea seemed to be that there was something wrong with you if you thought things you didn't dare say out loud. This seems backward. Almost certainly, there is something wrong with you if you don't think things you don't dare say out loud.\n\n\n\nTrouble\n\n\n\nWhat can't we say? One way to find these ideas is simply to look at things people do say, and get in trouble for. [2]\n\n\n\nOf course, we're not just looking for things we can't say. We're looking for things we can't say that are true, or at least have enough chance of being true that the question should remain open. But many of the things people get in trouble for saying probably do make it over this second, lower threshold. No one gets in trouble for saying that 2 + 2 is 5, or that people in Pittsburgh are ten feet tall. Such obviously false statements might be treated as jokes, or at worst as evidence of insanity, but they are not likely to make anyone mad. The statements that make people mad are the ones they worry might be believed. I suspect the statements that make people maddest are those they worry might be true.\n\n\n\nIf Galileo had said that people in Padua were ten feet tall, he would have been regarded as a harmless eccentric. Saying the earth orbited the sun was another matter. The church knew this would set people thinking.\n\n\n\nCertainly, as we look back on the past, this rule of thumb works well. A lot of the statements people got in trouble for seem harmless now. So it's likely that visitors from the future would agree with at least some of the statements that get people in trouble today. Do we have no Galileos? Not likely.\n\n\n\nTo find them, keep track of opinions that get people in trouble, and start asking, could this be true? Ok, it may be heretical (or whatever modern equivalent), but might it also be true?\n\n\n\nHeresy\n\n\n\nThis won't get us all the answers, though. What if no one happens to have gotten in trouble for a particular idea yet? What if some idea would be so radioactively controversial that no one would dare express it in public? How can we find these too?\n\n\n\nAnother approach is to follow that word, heresy. In every period of history, there seem to have been labels that got applied to statements to shoot them down before anyone had a chance to ask if they were true or not. \"Blasphemy\", \"sacrilege\", and \"heresy\" were such labels for a good part of western history, as in more recent times \"indecent\", \"improper\", and \"unamerican\" have been. By now these labels have lost their sting. They always do. By now they're mostly used ironically. But in their time, they had real force.\n\n\n\nThe word \"defeatist\", for example, has no particular political connotations now. But in Germany in 1917 it was a weapon, used by Ludendorff in a purge of those who favored a negotiated peace. At the start of World War II it was used extensively by Churchill and his supporters to silence their opponents. In 1940, any argument against Churchill's aggressive policy was \"defeatist\". Was it right or wrong? Ideally, no one got far enough to ask that.\n\n\n\n We have such labels today, of course, quite a lot of them, from the all-purpose \"inappropriate\" to the dreaded \"divisive.\" In any period, it should be easy to figure out what such labels are, simply by looking at what people call ideas they disagree with besides untrue. When a politician says his opponent is mistaken, that's a straightforward criticism, but when he attacks a statement as \"divisive\" or \"racially insensitive\" instead of arguing that it's false, we should start paying attention.\n\n\n\nSo another way to figure out which of our taboos future generations will laugh at is to start with the labels. Take a label\u2014\"sexist\", for example\u2014and try to think of some ideas that would be called that. Then for each ask, might this be true?\n\n\n\nJust start listing ideas at random? Yes, because they won't really be random. The ideas that come to mind first will be the most plausible ones. They'll be things you've already noticed but didn't let yourself think.\n\n\n\nIn 1989 some clever researchers tracked the eye movements of radiologists as they scanned chest images for signs of lung cancer. [3] They found that even when the radiologists missed a cancerous lesion, their eyes had usually paused at the site of it. Part of their brain knew there was something there; it just didn't percolate all the way up into conscious knowledge. I think many interesting heretical thoughts are already mostly formed in our minds. If we turn off our self-censorship temporarily, those will be the first to emerge.\n\n\n\nTime and Space\n\n\n\nIf we could look into the future it would be obvious which of our taboos they'd laugh at. We can't do that, but we can do something almost as good: we can look into the past. Another way to figure out what we're getting wrong is to look at what used to be acceptable and is now unthinkable.\n\n\n\nChanges between the past and the present sometimes do represent progress. In a field like physics, if we disagree with past generations it's because we're right and they're wrong. But this becomes rapidly less true as you move away from the certainty of the hard sciences. By the time you get to social questions, many changes are just fashion. The age of consent fluctuates like hemlines.\n\n\n\nWe may imagine that we are a great deal smarter and more virtuous than past generations, but the more history you read, the less likely this seems. People in past times were much like us. Not heroes, not barbarians. Whatever their ideas were, they were ideas reasonable people could believe.\n\n\n\nSo here is another source of interesting heresies. Diff present ideas against those of various past cultures, and see what you get. [4] Some will be shocking by present standards. Ok, fine; but which might also be true?\n\n\n\nYou don't have to look into the past to find big differences. In our own time, different societies have wildly varying ideas of what's ok and what isn't. So you can try diffing other cultures' ideas against ours as well. (The best way to do that is to visit them.)\n\n\n\nYou might find contradictory taboos. In one culture it might seem shocking to think x, while in another it was shocking not to. But I think usually the shock is on one side. In one culture x is ok, and in another it's considered shocking. My hypothesis is that the side that's shocked is most likely to be the mistaken one. [5]\n\n\n\nI suspect the only taboos that are more than taboos are the ones that are universal, or nearly so. Murder for example. But any idea that's considered harmless in a significant percentage of times and places, and yet is taboo in ours, is a good candidate for something we're mistaken about.\n\n\n\nFor example, at the high water mark of political correctness in the early 1990s, Harvard distributed to its faculty and staff a brochure saying, among other things, that it was inappropriate to compliment a colleague or student's clothes. No more \"nice shirt.\" I think this principle is rare among the world's cultures, past or present. There are probably more where it's considered especially polite to compliment someone's clothing than where it's considered improper. So odds are this is, in a mild form, an example of one of the taboos a visitor from the future would have to be careful to avoid if he happened to set his time machine for Cambridge, Massachusetts, 1992.\n\n\n\nPrigs\n\n\n\nOf course, if they have time machines in the future they'll probably have a separate reference manual just for Cambridge. This has always been a fussy place, a town of i dotters and t crossers, where you're liable to get both your grammar and your ideas corrected in the same conversation. And that suggests another way to find taboos. Look for prigs, and see what's inside their heads.\n\n\n\nKids' heads are repositories of all our taboos. It seems fitting to us that kids' ideas should be bright and clean. The picture we give them of the world is not merely simplified, to suit their developing minds, but sanitized as well, to suit our ideas of what kids ought to think. [6]\n\n\n\nYou can see this on a small scale in the matter of dirty words. A lot of my friends are starting to have children now, and they're all trying not to use words like \"fuck\" and \"shit\" within baby's hearing, lest baby start using these words too. But these words are part of the language, and adults use them all the time. So parents are giving their kids an inaccurate idea of the language by not using them. Why do they do this? Because they don't think it's fitting that kids should use the whole language. We like children to seem innocent. [7]\n\n\n\nMost adults, likewise, deliberately give kids a misleading view of the world. One of the most obvious examples is Santa Claus. We think it's cute for little kids to believe in Santa Claus. I myself think it's cute for little kids to believe in Santa Claus. But one wonders, do we tell them this stuff for their sake, or for ours?\n\n\n\nI'm not arguing for or against this idea here. It is probably inevitable that parents should want to dress up their kids' minds in cute little baby outfits. I'll probably do it myself. The important thing for our purposes is that, as a result, a well brought-up teenage kid's brain is a more or less complete collection of all our taboos\u2014and in mint condition, because they're untainted by experience. Whatever we think that will later turn out to be ridiculous, it's almost certainly inside that head.\n\n\n\nHow do we get at these ideas? By the following thought experiment. Imagine a kind of latter-day Conrad character who has worked for a time as a mercenary in Africa, for a time as a doctor in Nepal, for a time as the manager of a nightclub in Miami. The specifics don't matter\u2014just someone who has seen a lot. Now imagine comparing what's inside this guy's head with what's inside the head of a well-behaved sixteen year old girl from the suburbs. What does he think that would shock her? He knows the world; she knows, or at least embodies, present taboos. Subtract one from the other, and the result is what we can't say. \n\n\n\nMechanism\n\n\n\nI can think of one more way to figure out what we can't say: to look at how taboos are created. How do moral fashions arise, and why are they adopted? If we can understand this mechanism, we may be able to see it at work in our own time.\n\n\n\nMoral fashions don't seem to be created the way ordinary fashions are. Ordinary fashions seem to arise by accident when everyone imitates the whim of some influential person. The fashion for broad-toed shoes in late fifteenth century Europe began because Charles VIII of France had six toes on one foot. The fashion for the name Gary began when the actor Frank Cooper adopted the name of a tough mill town in Indiana. Moral fashions more often seem to be created deliberately. When there's something we can't say, it's often because some group doesn't want us to.\n\n\n\nThe prohibition will be strongest when the group is nervous. The irony of Galileo's situation was that he got in trouble for repeating Copernicus's ideas. Copernicus himself didn't. In fact, Copernicus was a canon of a cathedral, and dedicated his book to the pope. But by Galileo's time the church was in the throes of the Counter-Reformation and was much more worried about unorthodox ideas.\n\n\n\nTo launch a taboo, a group has to be poised halfway between weakness and power. A confident group doesn't need taboos to protect it. It's not considered improper to make disparaging remarks about Americans, or the English. And yet a group has to be powerful enough to enforce a taboo. Coprophiles, as of this writing, don't seem to be numerous or energetic enough to have had their interests promoted to a lifestyle.\n\n\n\nI suspect the biggest source of moral taboos will turn out to be power struggles in which one side only barely has the upper hand. That's where you'll find a group powerful enough to enforce taboos, but weak enough to need them.\n\n\n\nMost struggles, whatever they're really about, will be cast as struggles between competing ideas. The English Reformation was at bottom a struggle for wealth and power, but it ended up being cast as a struggle to preserve the souls of Englishmen from the corrupting influence of Rome. It's easier to get people to fight for an idea. And whichever side wins, their ideas will also be considered to have triumphed, as if God wanted to signal his agreement by selecting that side as the victor.\n\n\n\nWe often like to think of World War II as a triumph of freedom over totalitarianism. We conveniently forget that the Soviet Union was also one of the winners.\n\n\n\nI'm not saying that struggles are never about ideas, just that they will always be made to seem to be about ideas, whether they are or not. And just as there is nothing so unfashionable as the last, discarded fashion, there is nothing so wrong as the principles of the most recently defeated opponent. Representational art is only now recovering from the approval of both Hitler and Stalin. [8]\n\n\n\nAlthough moral fashions tend to arise from different sources than fashions in clothing, the mechanism of their adoption seems much the same. The early adopters will be driven by ambition: self-consciously cool people who want to distinguish themselves from the common herd. As the fashion becomes established they'll be joined by a second, much larger group, driven by fear. [9] This second group adopt the fashion not because they want to stand out but because they are afraid of standing out.\n\n\n\nSo if you want to figure out what we can't say, look at the machinery of fashion and try to predict what it would make unsayable. What groups are powerful but nervous, and what ideas would they like to suppress? What ideas were tarnished by association when they ended up on the losing side of a recent struggle? If a self-consciously cool person wanted to differentiate himself from preceding fashions (e.g. from his parents), which of their ideas would he tend to reject? What are conventional-minded people afraid of saying?\n\n\n\nThis technique won't find us all the things we can't say. I can think of some that aren't the result of any recent struggle. Many of our taboos are rooted deep in the past. But this approach, combined with the preceding four, will turn up a good number of unthinkable ideas.\n\n\n\nWhy\n\n\n\nSome would ask, why would one want to do this? Why deliberately go poking around among nasty, disreputable ideas? Why look under rocks?\n\n\n\nI do it, first of all, for the same reason I did look under rocks as a kid: plain curiosity. And I'm especially curious about anything that's forbidden. Let me see and decide for myself.\n\n\n\nSecond, I do it because I don't like the idea of being mistaken. If, like other eras, we believe things that will later seem ridiculous, I want to know what they are so that I, at least, can avoid believing them.\n\n\n\nThird, I do it because it's good for the brain. To do good work you need a brain that can go anywhere. And you especially need a brain that's in the habit of going where it's not supposed to.\n\n\n\nGreat work tends to grow out of ideas that others have overlooked, and no idea is so overlooked as one that's unthinkable. Natural selection, for example. It's so simple. Why didn't anyone think of it before? Well, that is all too obvious. Darwin himself was careful to tiptoe around the implications of his theory. He wanted to spend his time thinking about biology, not arguing with people who accused him of being an atheist.\n\n\n\nIn the sciences, especially, it's a great advantage to be able to question assumptions. The m.o. of scientists, or at least of the good ones, is precisely that: look for places where conventional wisdom is broken, and then try to pry apart the cracks and see what's underneath. That's where new theories come from.\n\n\n\nA good scientist, in other words, does not merely ignore conventional wisdom, but makes a special effort to break it. Scientists go looking for trouble. This should be the m.o. of any scholar, but scientists seem much more willing to look under rocks. [10]\n\n\n\nWhy? It could be that the scientists are simply smarter; most physicists could, if necessary, make it through a PhD program in French literature, but few professors of French literature could make it through a PhD program in physics. Or it could be because it's clearer in the sciences whether theories are true or false, and this makes scientists bolder. (Or it could be that, because it's clearer in the sciences whether theories are true or false, you have to be smart to get jobs as a scientist, rather than just a good politician.)\n\n\n\nWhatever the reason, there seems a clear correlation between intelligence and willingness to consider shocking ideas. This isn't just because smart people actively work to find holes in conventional thinking. I think conventions also have less hold over them to start with. You can see that in the way they dress.\n\n\n\nIt's not only in the sciences that heresy pays off. In any competitive field, you can win big by seeing things that others daren't. And in every field there are probably heresies few dare utter. Within the US car industry there is a lot of hand-wringing now about declining market share. Yet the cause is so obvious that any observant outsider could explain it in a second: they make bad cars. And they have for so long that by now the US car brands are antibrands\u2014something you'd buy a car despite, not because of. Cadillac stopped being the Cadillac of cars in about 1970. And yet I suspect no one dares say this. [11] Otherwise these companies would have tried to fix the problem.\n\n\n\nTraining yourself to think unthinkable thoughts has advantages beyond the thoughts themselves. It's like stretching. When you stretch before running, you put your body into positions much more extreme than any it will assume during the run. If you can think things so outside the box that they'd make people's hair stand on end, you'll have no trouble with the small trips outside the box that people call innovative.\n\n\n\nPensieri Stretti\n\n\n\nWhen you find something you can't say, what do you do with it? My advice is, don't say it. Or at least, pick your battles.\n\n\n\nSuppose in the future there is a movement to ban the color yellow. Proposals to paint anything yellow are denounced as \"yellowist\", as is anyone suspected of liking the color. People who like orange are tolerated but viewed with suspicion. Suppose you realize there is nothing wrong with yellow. If you go around saying this, you'll be denounced as a yellowist too, and you'll find yourself having a lot of arguments with anti-yellowists. If your aim in life is to rehabilitate the color yellow, that may be what you want. But if you're mostly interested in other questions, being labelled as a yellowist will just be a distraction. Argue with idiots, and you become an idiot.\n\n\n\nThe most important thing is to be able to think what you want, not to say what you want. And if you feel you have to say everything you think, it may inhibit you from thinking improper thoughts. I think it's better to follow the opposite policy. Draw a sharp line between your thoughts and your speech. Inside your head, anything is allowed. Within my head I make a point of encouraging the most outrageous thoughts I can imagine. But, as in a secret society, nothing that happens within the building should be told to outsiders. The first rule of Fight Club is, you do not talk about Fight Club.\n\n\n\nWhen Milton was going to visit Italy in the 1630s, Sir Henry Wootton, who had been ambassador to Venice, told him his motto should be \"i pensieri stretti & il viso sciolto.\" Closed thoughts and an open face. Smile at everyone, and don't tell them what you're thinking. This was wise advice. Milton was an argumentative fellow, and the Inquisition was a bit restive at that time. But I think the difference between Milton's situation and ours is only a matter of degree. Every era has its heresies, and if you don't get imprisoned for them you will at least get in enough trouble that it becomes a complete distraction.\n\n\n\nI admit it seems cowardly to keep quiet. When I read about the harassment to which the Scientologists subject their critics [12], or that pro-Israel groups are \"compiling dossiers\" on those who speak out against Israeli human rights abuses [13], or about people being sued for violating the DMCA [14], part of me wants to say, \"All right, you bastards, bring it on.\" The problem is, there are so many things you can't say. If you said them all you'd have no time left for your real work. You'd have to turn into Noam Chomsky. [15]\n\n\n\nThe trouble with keeping your thoughts secret, though, is that you lose the advantages of discussion. Talking about an idea leads to more ideas. So the optimal plan, if you can manage it, is to have a few trusted friends you can speak openly to. This is not just a way to develop ideas; it's also a good rule of thumb for choosing friends. The people you can say heretical things to without getting jumped on are also the most interesting to know.\n\n\n\nViso Sciolto?\n\n\n\nI don't think we need the viso sciolto so much as the pensieri stretti. Perhaps the best policy is to make it plain that you don't agree with whatever zealotry is current in your time, but not to be too specific about what you disagree with. Zealots will try to draw you out, but you don't have to answer them. If they try to force you to treat a question on their terms by asking \"are you with us or against us?\" you can always just answer \"neither\".\n\n\n\nBetter still, answer \"I haven't decided.\" That's what Larry Summers did when a group tried to put him in this position. Explaining himself later, he said \"I don't do litmus tests.\" [16] A lot of the questions people get hot about are actually quite complicated. There is no prize for getting the answer quickly.\n\n\n\nIf the anti-yellowists seem to be getting out of hand and you want to fight back, there are ways to do it without getting yourself accused of being a yellowist. Like skirmishers in an ancient army, you want to avoid directly engaging the main body of the enemy's troops. Better to harass them with arrows from a distance.\n\n\n\nOne way to do this is to ratchet the debate up one level of abstraction. If you argue against censorship in general, you can avoid being accused of whatever heresy is contained in the book or film that someone is trying to censor. You can attack labels with meta-labels: labels that refer to the use of labels to prevent discussion. The spread of the term \"political correctness\" meant the beginning of the end of political correctness, because it enabled one to attack the phenomenon as a whole without being accused of any of the specific heresies it sought to suppress.\n\n\n\nAnother way to counterattack is with metaphor. Arthur Miller undermined the House Un-American Activities Committee by writing a play, \"The Crucible,\" about the Salem witch trials. He never referred directly to the committee and so gave them no way to reply. What could HUAC do, defend the Salem witch trials? And yet Miller's metaphor stuck so well that to this day the activities of the committee are often described as a \"witch-hunt.\"\n\n\n\nBest of all, probably, is humor. Zealots, whatever their cause, invariably lack a sense of humor. They can't reply in kind to jokes. They're as unhappy on the territory of humor as a mounted knight on a skating rink. Victorian prudishness, for example, seems to have been defeated mainly by treating it as a joke. Likewise its reincarnation as political correctness. \"I am glad that I managed to write 'The Crucible,'\" Arthur Miller wrote, \"but looking back I have often wished I'd had the temperament to do an absurd comedy, which is what the situation deserved.\" [17]\n\n\n\nABQ\n\n\n\nA Dutch friend says I should use Holland as an example of a tolerant society. It's true they have a long tradition of comparative open-mindedness. For centuries the low countries were the place to go to say things you couldn't say anywhere else, and this helped to make the region a center of scholarship and industry (which have been closely tied for longer than most people realize). Descartes, though claimed by the French, did much of his thinking in Holland.\n\n\n\nAnd yet, I wonder. The Dutch seem to live their lives up to their necks in rules and regulations. There's so much you can't do there; is there really nothing you can't say?\n\n\n\nCertainly the fact that they value open-mindedness is no guarantee. Who thinks they're not open-minded? Our hypothetical prim miss from the suburbs thinks she's open-minded. Hasn't she been taught to be? Ask anyone, and they'll say the same thing: they're pretty open-minded, though they draw the line at things that are really wrong. (Some tribes may avoid \"wrong\" as judgemental, and may instead use a more neutral sounding euphemism like \"negative\" or \"destructive\".)\n\n\n\nWhen people are bad at math, they know it, because they get the wrong answers on tests. But when people are bad at open-mindedness they don't know it. In fact they tend to think the opposite. Remember, it's the nature of fashion to be invisible. It wouldn't work otherwise. Fashion doesn't seem like fashion to someone in the grip of it. It just seems like the right thing to do. It's only by looking from a distance that we see oscillations in people's idea of the right thing to do, and can identify them as fashions.\n\n\n\nTime gives us such distance for free. Indeed, the arrival of new fashions makes old fashions easy to see, because they seem so ridiculous by contrast. From one end of a pendulum's swing, the other end seems especially far away.\n\n\n\nTo see fashion in your own time, though, requires a conscious effort. Without time to give you distance, you have to create distance yourself. Instead of being part of the mob, stand as far away from it as you can and watch what it's doing. And pay especially close attention whenever an idea is being suppressed. Web filters for children and employees often ban sites containing pornography, violence, and hate speech. What counts as pornography and violence? And what, exactly, is \"hate speech?\" This sounds like a phrase out of 1984.\n\n\n\nLabels like that are probably the biggest external clue. If a statement is false, that's the worst thing you can say about it. You don't need to say that it's heretical. And if it isn't false, it shouldn't be suppressed. So when you see statements being attacked as x-ist or y-ic (substitute your current values of x and y), whether in 1630 or 2030, that's a sure sign that something is wrong. When you hear such labels being used, ask why.\n\n\n\nEspecially if you hear yourself using them. It's not just the mob you need to learn to watch from a distance. You need to be able to watch your own thoughts from a distance. That's not a radical idea, by the way; it's the main difference between children and adults. When a child gets angry because he's tired, he doesn't know what's happening. An adult can distance himself enough from the situation to say \"never mind, I'm just tired.\" I don't see why one couldn't, by a similar process, learn to recognize and discount the effects of moral fashions.\n\n\n\nYou have to take that extra step if you want to think clearly. But it's harder, because now you're working against social customs instead of with them. Everyone encourages you to grow up to the point where you can discount your own bad moods. Few encourage you to continue to the point where you can discount society's bad moods.\n\n\n\nHow can you see the wave, when you're the water? Always be questioning. That's the only defence. What can't you say? And why?\n\n\n\n\n\n\n\nNotes\n\n\n\nThanks to Sarah Harlin, Trevor Blackwell, Jessica Livingston, Robert Morris, Eric Raymond and Bob van der Zwaan for reading drafts of this essay, and to Lisa Randall, Jackie McDonough, Ryan Stanley and Joel Rainey for conversations about heresy. Needless to say they bear no blame for opinions expressed in it, and especially for opinions not expressed in it.\n\n\n\n\n\n \n\n Re: What You Can't Say\n\n \n\n What You Can't Say Will Hurt You\n\n \n\n \n\n"},
{"url": "http://startupstunned.blogspot.com/2017/08/want-to-start-marketplace-startup.html", "link_title": "Want to start marketplace startup? Better buckle up as it's not going to be easy", "text": "These type of businesses which includes giants like Amazon, AirBNB, Uber, Etsy to name a few are almost always huge in terms of network, profit, net worth and several other\u00a0such factors. They are also one of the hottest types among the investors and venture capitalists and a lot of them are looking to invest more and more here.\n\nThough this all sounds highly intriguing, the only thing which nobody wants to hear is that building a Marketplace business is as hard as anything. There are a lot of struggles founders have to face while starting one and even when there business is stable.\n\nIt includes a lot more complications which are not easy to solve and often comes out as the cause of company's failure. Some difficulties which marketplace businesses face almost everyday are:\n\nThe founders have to maintain a system which would prevent them from getting defeated by any of the difficulties mentioned above. There are certain well-proven strategies one can follow to tackle all these hardships.\n\nFirst of all you have to start with a niche portion (like Amazon used to sell only books when it started) and in a small area so that you can scale easily and effectively.\n\nAs said by Marco Zappacosta, CEO of Thumbtack, one should first focus on either supply or demand side. He preferred focusing on supply side first for building an infrastructure which would be handy when demands start pouring in. According to him, building such a infrastructure is again really tough and requires certain offers or features which would lure the suppliers to come on-board without giving it a second thought. These features differs in different types of marketplaces and you have to come up with your own based on the type you are working.\n\nAfter getting supply side on-board, you have to start worrying about the demand side and according to Marco, the best way for promoting a marketplace in it's early days is by word of mouth. As you have to start small initially, you don't need a big marketing budget and you should not go that big in the start because you'll not be able to provide good service to the user.\n\nAnother problem you are going to face is the 'platform leakage'. Platform leakage is what happens when a buyer and seller agree to circumvent the marketplace and continue transacting outside the platform. The transaction still happens, but the marketplace loses a revenue stream. It is one of the worst thing that can happen to a marketplace and have no fixed solution. Engaging users in the app, providing ratings and points on successful tasks to both users and suppliers, providing payment as well as order management tools to both users and suppliers, providing overall excellent service to users and excellent business to suppliers are a few strategies which have been tried by the marketplace giants.\n\nMust reads for learning some great lessons by marketplace giants and genius minds:\n\nSo, as you can see, it is really a hard nut to crack but once you have cracked it successfully, you are a big business with millions if not billions in revenue.\n\n\n\n Subscribe to this blog as I'd be rolling out more such articles.\n\n \n\n Till then, STAY HUNGRY. STAY FOOLISH."},
{"url": "http://www.cbc.ca/news/canada/montreal/airbnb-study-montreal-1.4237710", "link_title": "Property management companies Airbnb's biggest winners", "text": "A small number of commercial property managers\u00a0generate a majority of Airbnb's overall revenue, eating\u00a0up available housing stock and driving\u00a0up rent in Canada's three biggest cities, a new study from Montreal's McGill University concludes.\n\n\"Just 10 per cent of hosts account for a majority of the revenue and the nights booked on\u00a0Airbnb\u00a0consistently in Toronto, Vancouver and Montreal,\" said the study's lead author, David\u00a0Wachsmuth,\u00a0a McGill\u00a0professor of\u00a0urban planning,\u00a0in an interview on CBC Montreal's\u00a0Daybreak.\n\nThe study, titled Short-term Cities: Airbnb's Impact on Canadian Housing Markets,\u00a0will be published Tuesday.\n\n\"What we're seeing with short-term rentals is that the right of property owners to make money is trampling on the right of people to afford houses,\" Wachsmuth said.\n\nWachsmuth and his colleagues found that all three Canadian cities studied have seen a 50 per cent year over year increase in the number of listings on\u00a0Airbnb\u00a0\u2014 most of them, entire homes or apartments, and not just rooms in occupied homes.\n\nThey looked at three years of data provided by the\u00a0analytics firm AirDNA, which offers data to users looking to increase their Airbnb success. The firm collected information on all public transactions, gathering about\u00a080 million data points.\n\nWachsmuth said it was the best information publicly available.\n\nOf the three Canadian cities, Montreal has the most active listings on Airbnb, because of the city's popularity with tourists.\n\nIn Montreal, the Plateau\u2013Mont-Royal and\u00a0Ville-Marie\u00a0boroughs have the most listings.\n\nWachsmuth and his team found that\u00a0two or three per cent of the housing stock in those\u00a0neighbourhoods is now being\u00a0run by property management companies for short-term rentals.\n\n\"There's one in Montreal that has about 160 properties,\" said Wachsmuth. \"They make a couple of million dollars a year on\u00a0Airbnb,\u00a0is our estimate.\"\n\n\"They're basically running hotels ...\u00a0that are split across multiple apartments,\" he said.\n\nAccording to Wachsmuth, the most listings\u00a0in Montreal are from\u00a0the San Francisco-based property management company\u00a0Sonder, which also offers listings in Vancouver and across the U.S.\n\nOne Sonder property in the Plateau starts at $120 a night, for up to four people, while another loft space in the Plateau,\u00a0which can accommodate up to 10 people, starts at $249 a night.\n\n\"Montreal has quite cheap rents which means that we found \u2014 much more so than Toronto and Vancouver\u00a0\u2014\u00a0it's a lot easier to out-compete\u00a0the amount of income you can earn as a landlord from a normal rental by listing on Airbnb,\" he said.\n\n\"The author of this study has a history of manipulating scraped data to misrepresent\u00a0Airbnb\u00a0hosts, the vast majority of whom are\u00a0middle-class Canadian families sharing their homes to earn a bit of additional income to help pay the bills,\" wrote spokesperson\u00a0Lindsey\u00a0Scully\u00a0in an\u00a0emailed\u00a0statement.\n\n\"A\u00a0very small percentage of the entire housing stock in Montreal\u00a0is rented frequently enough to\u00a0out-compete\u00a0a long-term rental, undercutting the author's baseless conclusions about housing units removed.\"\n\nAirbnb\u00a0says more than\u00a080 per cent\u00a0of hosts on the site are sharing their primary residences and are doing so three to four nights a month to earn a little extra income.\n\n\u200bIn December\u00a02015, the Quebec government passed Bill 67 to regulate\u00a0short-term\u00a0rentals in the province.\n\nPeople who regularly rent \u00a0properties need to get a certificate from the province's tourism ministry, pay a lodging tax and advise their landlords that they will be renting to tourists.\n\nThe legislation also increased the number of provincial inspectors from two to 18 to help enforce the new rules.\n\nBut Waschmuth said the\u00a0research showed the legislation hasn't had an impact in Montreal on short-term listings. He proposed three steps to regulate services like Airbnb:\n\n1. Hosts should only be allowed to rent their primary residences, so no multiple listings.\n\n2. No full-time rentals, that is no properties that solely exist to be rented on Airbnb.\u00a0For example, Amsterdam caps rentals at\u00a060 days of the year, while London limits rentals to 90 days.\n\n3. Airbnb and other short-term rental platforms need to be required to enforce their regulations themselves, because it will be too difficult for city inspectors to do that."},
{"url": "https://vadosware.io/post/diversity-ez-fix", "link_title": "Fully anonymized interviews as an easy/easier fix to the Diversity problem", "text": ""},
{"url": "https://www.bloomberg.com/news/articles/2017-08-08/google-fires-employee-behind-controversial-diversity-memo", "link_title": "Google Fires Employee Behind Controversial Diversity Memo", "text": "Alphabet Inc.\u2019s Google has fired an employee who wrote an internal memo blasting the web company\u2019s diversity policies, creating a firestorm across Silicon Valley.\n\nJames Damore, the Google engineer who wrote the note, confirmed his dismissal in an email, saying that he had been fired for \u201cperpetuating gender stereotypes.\u201d\n\nThe imbroglio at Google is the latest in a long string of incidents concerning gender bias and diversity in the tech enclave.\u00a0Uber Technologies Inc. Chief Executive Officer Travis Kalanick lost his job in June amid scandals over sexual harassment, discrimination and an aggressive culture. Ellen Pao\u2019s gender-discrimination lawsuit against Kleiner Perkins Caufield & Byers in 2015 also brought the issue to light, and more women are speaking up to say they\u2019ve been sidelined in the male-dominated industry, especially in engineering roles.\n\nEarlier on Monday, Google CEO Sundar Pichai sent a note to employees that said portions of the memo \u201cviolate our Code of Conduct and cross the line by advancing harmful gender stereotypes in our workplace.\u201d But he didn\u2019t say if the company was taking action against the employee. A Google representative, asked about the dismissal, referred to Pichai\u2019s memo.\n\nDamore\u2019s 10-page memorandum accused Google of silencing conservative political opinions and argued that biological differences play a role in the shortage of women in tech and leadership positions. It circulated widely inside the company and became public over the weekend, causing a furor that amplified the pressure on Google executives to take a more definitive stand.\n\nRead more: Bloomberg QuickTake on women and tech\n\nAfter the controversy swelled, Danielle Brown, Google\u2019s new vice president for diversity, integrity and governance, sent a statement to staff condemning Damore\u2019s views and reaffirmed the company\u2019s stance on diversity. In internal discussion boards, multiple employees said they supported firing the author, and some said they would not choose to work with him, according to postings viewed by Bloomberg News.\n\n\u201cWe are unequivocal in our belief that diversity and inclusion are critical to our success as a company,\u201d Brown said in the statement. \u201cWe\u2019ll continue to stand for that and be committed to it for the long haul.\u201d\n\nThe memo and surrounding debate comes as Google fends off a lawsuit from the U.S. Department of Labor alleging the company systemically discriminates against women. Google has denied the charges, arguing that it doesn\u2019t have a gender gap in pay, but has declined to share full salary information with the government. According to the company\u2019s most recent demographic report, 69 percent of its workforce and 80 percent of its technical staff are male.\n\nFollowing the memo\u2019s publication, multiple executives shared an article from a senior engineer who recently left the company, Yonatan Zunger. In the blog post, Zunger said that based on the context of the memo, he determined that he would \u201cnot in good conscience\u201d assign any employees to work with its author. \u201cYou have just created a textbook hostile workplace environment,\u201d he wrote. He also said in a email, \u201cCould you imagine having to work with someone who had just publicly questioned your basic competency to do your job?\u201d\n\nStill, some right-wing websites had already lionized the memo\u2019s author, and firing him could be seen as confirming some of the claims in the memo itself \u2013 that the company\u2019s culture makes no room for dissenting political opinions. That outcome could galvanize any backlash against Alphabet\u2019s efforts to make its workforce more diverse.\n\nIn her initial response to the memo, Brown, who joined from Intel Corp. in June, suggested that Google was open to all hosting \u201cdifficult political views,\u201d including those in the memo. However, she left open the possibility that Google could penalize the engineer for violating company policies. \u201cBut that discourse needs to work alongside the principles of equal employment found in our Code of Conduct, policies, and anti-discrimination laws,\u201d she wrote.\n\nThe subject of Google\u2019s ideological bent came up at the most recent shareholder meeting, in June. A shareholder asked executives whether conservatives would feel welcome at the company. Executives disagreed with the idea that anyone wouldn\u2019t.\n\n\u201cThe company was founded under the principles of freedom of expression, diversity, inclusiveness and science-based thinking,\u201d Alphabet Chairman Eric Schmidt said at the time. \u201cYou\u2019ll also find that all of the other companies in our industry agree with us.\u201d"},
{"url": "http://www.pewinternet.org/2016/10/04/public-views-on-climate-change-and-climate-scientists", "link_title": "48% of U.S. adults say climate change is due to human activity", "text": "There is a host of ways Americans\u2019 opinions about climate issues divide. The divisions start with views about the causes of global climate change. Nearly half of U.S. adults say climate change is due to human activity and a similar share says either that the Earth\u2019s warming stems from natural causes or that there is no evidence of warming. The disputes extend to differing views about the likely impact of climate change and the possible remedies, both at the policy level and the level of personal behavior.\n\nRoughly four-in-ten Americans expect harmful effects from climate change on wildlife, shorelines and weather patterns. At the same time, many are optimistic that both policy and individual efforts to address climate change can have an impact. A narrow majority of Americans anticipate new technological solutions to problems connected with climate change, and some 61% believe people will make major changes to their way of life within the next half century.\n\nOn all of these matters there are wide differences along political lines with conservative Republicans much less inclined to anticipate negative effects from climate change or to judge proposed solutions as making much difference in mitigating any effects. Half or more liberal Democrats, by contrast, see negative effects from climate change as very likely and believe an array of policy solutions can make a big difference.\n\nAmericans who are more deeply concerned about climate issues, regardless of their partisan orientation, are particularly likely to see negative effects ahead from climate change, and strong majorities among this group think policy solutions can be effective at addressing climate change.\n\nRoughly two-thirds of Americans say climate scientists should have a major role in policy decisions about climate matters, more than say that the public, energy industry leaders, or national and international political leaders should be so involved.\n\nBut, overall, majorities of Americans appear skeptical of climate scientists. No more than a third of the public gives climate scientists high marks for their understanding of climate change; even fewer say climate scientists understand the best ways to address climate change. And, while Americans trust information from climate scientists more than they trust that from other groups, fewer than half of Americans have \u201ca lot\u201d of trust in information from climate scientists (39%).\n\nA minority of Americans perceive that the best available scientific evidence is driving climate research findings most of the time. And a roughly equal share says other, more negative, factors influence climate research.\n\nPeople\u2019s trust and confidence in climate scientists varies widely depending on their political orientation. Liberal Democrats are much more trusting of climate scientists\u2019 understanding of the issue and disclosure of full and accurate information about it. Republicans, particularly conservatives, are highly critical of climate scientists and more likely to ascribe negative rather than positive motives to the influences shaping scientists\u2019 research.\n\nThis chapter provides an overview of Americans\u2019 attitudes about climate change and climate scientists. It then details the divides in these views among political groups and among those who are more or less concerned about climate issues. Americans who care more about the issue of climate change, regardless of political orientation, are more trusting of climate scientists, more likely to expect negative effects to occur because of climate change, and more likely to believe that both individual efforts and policy actions can be effective in addressing climate change.\n\nRoughly half of adults (48%) say climate change is mostly due to human activity; roughly three-in-ten say it is due to natural causes (31%) and another fifth say there is no solid evidence of warming (20%).\n\nThe share saying human activity is the primary cause of climate change is about the same as Pew Research Center surveys in 2014 (50%) and 2009 (49%). Center surveys from 2006 to 2015 using somewhat different question wording found a similar share expressing this view (45% in the most recent, 2015 survey).\n\nLarge majorities of Americans think global warming will lead to an array of negative effects for the Earth\u2019s ecosystems. At least three-quarters of Americans say that harm to animal habitats and plant life is very or fairly likely to occur. A similar share expects storms to become more severe and damage to shorelines or more frequent droughts to occur.\n\nAmericans who believe global climate change is the result of human activity are far more likely than other Americans (those who believe climate change results from natural patterns or that there is no evidence of global warming) to say each of these effects is very likely.\n\nA 61% majority of the public expects Americans will make major changes to their ways of life in order to address problems from climate change within the next half century, while 38% do not expect this to occur. The public, as a whole, sides to optimism (55% to 44%) that new technological solutions will arise within the next 50 years that can solve most of the problems from climate change.\n\nThere are a number of different proposals to address climate change. The Pew Research Center survey explored people\u2019s views about whether each of several policy and individual actions can be effective at addressing climate change.\n\nAmericans are largely optimistic that restrictions on power plant emissions (51%) and international agreements to limit carbon emissions (49%) can make a big difference to address climate change. The Obama administration announced stricter limits on power plant emissions in 2015. This year, more than 175 countries, including the U.S., have signed the Paris Agreement, which aims to reduce carbon emissions around the world.\n\nPublic assessments of other policy proposals are similar. Some 46% say tougher fuel efficiency standards for cars and trucks can make a big difference in addressing climate change; 45% say corporate tax incentives that encourage businesses to reduce carbon emissions caused by their actions can too.\n\nAbout four-in-ten Americans (41%) say having more hybrid and electric vehicles on the road can have a big effect; 38% think people\u2019s efforts to reduce their own \u201ccarbon footprints\u201d as they go about daily living can make a big difference, while another 44% say this can make a small difference.\n\nA majority of Americans say that climate scientists should have a role in policy decisions about climate issues. Two-thirds (67%) of U.S. adults say climate scientists should have a major role and 23% say they should have a minor role. Just 9% say climate scientists should have no role in policy issues regarding global climate change.\n\nFollowing scientific experts on the list, 56% of adults say the general public should have a major role in policy decisions about climate issues, followed by 53% that name energy industry leaders.\n\nBy comparison, fewer Americans believe elected officials should have a major role in climate policy decisions. In all, 44% of U.S. adults say elected officials should have a major role, another four-in-ten (40%) say elected leaders should have a minor role in climate policy-making.\n\nPublic views about the role of elected officials in policy decisions on climate issues may tie with deep public cynicism about the federal government, generally. Or, as shown later in this chapter, those beliefs could tie to distrust that elected officials provide full and accurate information about the causes of climate change.\n\nPeople\u2019s normative views about the place of international leaders in these decisions are similar to that for U.S. leaders.\n\nScientists first noted the possibility that the burning of greenhouse gases, such as fossil fuels, could increase temperatures back in the 1800s. A report from National Academy of Sciences in 1977 warned that the burning of fossil fuels could result in average temperatures increases of 6 degrees Celsius by the year 2150.\n\nThe Intergovernmental Panel on Climate Change (IPCC), which reflects scientific opinion on the topic, stated in the forward to its 2013 report, \u201cthe science now shows with 95 percent certainty that human activity is the dominant cause of observed warming since the mid-20th century.\u201d And, several analyses of scholarly publications suggest widespread consensus among climate scientists on this point.\n\nSimilarly, a Pew Research Center survey of members of the American Association for the Advancement of Science (AAAS) found 93% of members with a Ph.D. in Earth sciences (and 87% of all members) say the Earth is warming mostly because of human behavior.\n\nBut, in the public eye, there is considerably less consensus. Just 27% of Americans say that \u201calmost all\u201d climate scientists hold human behavior responsible for climate change. Another 35% say more than half of climate scientists agree about this, while an equal share says that about fewer than half (20%) or almost no (15%) scientific experts believe that human behavior is the main contributing factor in climate change.\n\nConsistent with previous Pew Research Center studies, people\u2019s perceptions of consensus among climate scientists are closely related to their beliefs about global climate change. Among those who say climate change is due to human activity, many more say scientists are in agreement on the main cause of climate change.\n\nAmericans appear to harbor significant reservations about climate scientists\u2019 expertise and understanding of what is happening to the Earth\u2019s climate. One-in-three adults (33%) say climate scientists understand \u201cvery well\u201d whether climate change is occurring, another 39% say scientists understand this \u201cfairly well\u201d and some 27% say scientists don\u2019t understand this \u201ctoo well\u201d or don\u2019t understand it at all.\n\nJust over a quarter of the public \u2013 28% \u2013 says climate scientists have a solid understanding of the causes of climate change. And even fewer, 19%, of adults say the same about climate scientists\u2019 understanding of the best ways to address climate change.\n\nAmericans hold relatively positive views about climate scientists, compared with other groups, as credible sources of information. Far more Americans say they trust information from climate scientists on the causes of climate change than say they trust either energy industry leaders, the news media or elected officials. But in absolute terms, public trust in information from climate scientists is limited.\n\nSome 39% of Americans say they trust climate scientists a lot when it comes to providing information about the causes of climate change. About a fifth of Americans (22%) hold no trust or not too much trust in information from climate scientists. Another 39% report \u201csome\u201d trust in climate scientists to give a full and accurate portrait of the causes of climate change.\n\nPublic trust in information from the news media, energy industry leaders and elected officials is significantly lower, however. A majority of Americans report having not too much or no trust in information from these groups about the causes of climate change.\n\nThis survey included a series of questions that tapped into Americans\u2019 beliefs of potential influences on climate research, and the findings suggest some skepticism and mixed assessments from the public. A minority of 32% of Americans say climate research is influenced by the best available evidence \u201cmost of the time,\u201d 48% say this occurs some of the time and 18% take a decidedly skeptical view that the best evidence rarely or never influences research findings.\n\nA similar share of Americans say that scientists\u2019 career aspirations influence their research most of the time (36%). A smaller share of adults say scientists\u2019 political leanings (27%) or their desires to help related industries (26%) influence climate research findings most of the time. But majorities say these less germane motivations influence results at least some of the time.\n\nWhile most Americans say the public\u2019s best interest factors into climate change research at least some of the time, only 23% of Americans say climate research is influenced by concern for public interests most of the time. Overall, 28% say this occurs not too often or never and 48% of Americans take a middle position, saying this sometimes influences climate research findings.\n\nPolitical divides are dominant in public views about climate matters. Consistent with past Pew Research Center surveys, most liberal Democrats espouse human-caused climate change, while most conservative Republicans reject it. But this new Center survey finds that political differences over climate issues extend across a host of beliefs about the expected effects of climate change, actions that can address changes to the Earth\u2019s climate, and trust and credibility in the work of climate scientists. People on the ideological ends of either party, that is liberal Democrats and conservative Republicans, see the world through vastly different lenses across all of these judgments.\n\nAs with previous Pew Research Center surveys, there are wide differences among political party and ideology groups on whether or not human activity is responsible for warming temperatures. A large majority of liberal Democrats (79%) believe the Earth is warming mostly because of human activity. In contrast, only about one-in-six conservative Republicans (15%) say this, a difference of 64 percentage points. A much larger share of conservative Republicans say there is no solid evidence the Earth is warming (36%) or that warming stems from natural causes (48%).\n\nPew Research Center surveys have found these kinds of wide political gaps in previous years. In the 2015 Center survey, using somewhat different question wording, there was a 41-percentage-point difference between partisans; 64% of Democrats said climate change was mostly due to human activity, compared with 23% among Republicans.\n\nPeople\u2019s beliefs about the likely effects of climate change are quite uniformly at odds across party and ideological lines. About six-in-ten or more of liberal Democrats say it is very likely that climate change will bring droughts, storms that are more severe, harm to animal and plant life, and damage to shorelines from rising sea levels. By contrast, no more than about two-in-ten conservative Republicans say each of these possibilities is \u201cvery likely\u201d; about half consider these possibilities not too or not at all likely.\n\nThere are more modest differences when it comes to people\u2019s expectations that technological breakthroughs will solve climate problems in the future or that the American people will make major changes to their way of life as a result of climate change. A majority of Democrats think technological changes will help address climate change within the next 50 years; views among moderate/liberal Republicans are similar. Some 46% of conservative Republicans think this will probably or definitely occur. Similarly, about half of conservative Republicans (49%) expect Americans to make major changes to their way of life to address climate issues within the next five decades, as do majorities of other party and ideology groups.\n\nThere is wide gulf between liberal Democrats and conservative Republicans when it comes to beliefs about how to effectively address climate change. Liberal Democrats are optimistic that a range of policy actions can make \u201ca big difference\u201d in addressing climate change including: power plant emission limits, international agreements about emissions, tougher fuel efficiency standards for vehicles, and corporate tax incentives to encourage businesses to reduce emissions resulting from their activities. And, at least half of liberal Democrats say that both personal efforts to reduce the carbon footprint of everyday activities and more people driving hybrid and electric vehicles can make a big difference in addressing global warming.\n\nBy contrast, conservative Republicans are largely pessimistic about the effectiveness of these options. Most conservative Republicans say each of these actions would make a small difference or have no effect on climate change. About three-in-ten or fewer conservative Republicans say each would make a big difference.\n\nMore than three-quarters of Democrats and most Republicans (69% among moderate or liberal Republicans and 48% of conservative Republicans) say climate scientists should have a major role in policy decisions related to climate issues. Few in either party say climate scientists should have no role in these policy decisions.\n\nBut there some differences among party and ideology groups in their relative priorities about this. Conservative Republicans give a higher comparative priority to the general public in policy decisions about climate change issues. Democrats and moderate/liberal Republicans prioritize a role for climate scientists.\n\nRelative to other groups rated, fewer Americans think elected officials should have a major say in climate policy. Conservative Republicans stand out as being disinclined to support a major role for elected officials or leaders from other nations in climate policy.\n\nPeople\u2019s assessments of scientific understanding about climate also ties strongly to their political perspectives. Most liberal Democrats rate climate scientists as understanding \u201cvery well\u201d whether climate change is occurring (68%) and about half say scientists understand \u201cvery well\u201d the causes of climate change (54%). By contrast, just 11% of conservative Republicans judge climate scientists as understanding very well the sources of climate change. Fully 63% of this group says climate scientists understand the causes of climate change \u201cnot too\u201d or \u201cnot at well.\u201d\n\nFewer in either party think climate scientists understand ways to address climate change. Some 36% of liberal Democrats say climate scientists understand this \u201cvery well\u201d and 49% say scientists understand this \u201cfairly well.\u201d Conservative Republicans are particularly skeptical of climate scientists\u2019 understanding of ways to address climate change; just 8% say scientists understand how to address climate change \u201cvery well,\u201d 28% say \u201cfairly well\u201d and 64% rate scientific understanding of this as \u201cnot too well\u201d or \u201cnot at all well.\u201d\n\nAmerican\u2019s perceptions of scientific consensus on climate change are also related to political divides, as has also been found in past Pew Research Center surveys.\n\nLiberal Democrats are far more likely than any other party or ideology group to see strong consensus among climate scientists. Some 55% of liberal Democrats say almost all climate scientists agree that human behavior is mostly responsible for climate change.\n\nMuch smaller shares of other groups see widespread consensus among climate scientists. Some 29% of moderate/conservative Democrats say almost all climate scientists agree that human behavior is responsible for climate change, while some 16% of conservative Republicans and 13% of moderate/liberal Republicans say the same.\n\nPeople\u2019s perceptions of scientific consensus, even among liberal Democrats, are at odds with the near unanimity expressed in climate research publications that human activity is mostly responsible for climate change, however.\n\nPublic trust in information from climate scientists about the causes of climate change varies widely among political groups. Seven-in-ten (70%) liberal Democrats trust climate scientists a lot to provide full and accurate information about this, another 24% report some trust in information from climate scientists. In contrast, just 15% of conservative Republicans say they trust climate scientists a lot to give full and accurate information, four-in-ten (40%) report some trust and 45% have not too much or no trust in information from climate scientists. Moderate or liberal Republicans and moderate or conservative Democrats fall in the middle between these two extremes in their level of trust.\n\nAmerican\u2019s judgments about the credibility of climate research findings are also tied with people\u2019s political party and ideological orientations. At least half of liberal Democrats (55%) say climate research is influenced by the best available evidence most of the time, and 39% say this occurs some of the time. By contrast, just 9% of conservative Republicans say the best evidence influences climate research most of the time, though 54% say this occurs some of the time.\n\nConservative Republicans are particularly skeptical about the factors influencing climate research. Some 57% of conservative Republicans say climate research is influenced by researchers\u2019 career interests most of the time and 54% say the scientists\u2019 own political leanings influence research findings most of the time. A much smaller share of liberal Democrats say either of these factors influence scientific research most of the time, although many say scientists\u2019 career interests or personal political leanings influence the findings some of the time (54% for each).\n\nThe public\u2019s level of concern about climate matters varies. The Pew Research Center survey finds 36% of Americans particularly concerned, saying they care a great deal about the issue of global climate change. An additional 38% express some interest, while 26% say they care not too much or not at all about the issue of climate change.\n\nNot surprisingly, those who care a great deal about global climate change issues are more attentive to climate news. Some 26% of those who care about climate issues a great deal follow climate news reports very closely, compared with just 3% among those less concerned about these issues.\n\nThose most concerned about climate issues come from all gender, age, education, race and ethnic groups. Those more concerned about climate issues are slightly more likely to be women than men (55% vs. 45%). And, they are more likely to be Hispanic than the population as whole.\n\nPolitically, those who care more deeply about climate issues tend to be Democrats. They include about equal shares of moderate or conservative Democrats (37%) and liberal Democrats (35%). Some 24% are Republicans.\n\nPolitical party affiliation and ideology are not the only factors that shape people\u2019s views about climate issues and climate scientists. People who say they care a great deal about this issue are far more likely to believe the Earth is warming because of human activities, to believe negative effects from climate change are likely, and that proposals to address climate change will be effective. This group also holds more positive views about climate scientists and their research, on average. Differences between those more concerned and less concerned occur among both Republicans and Democrats.\n\nAbout three-quarters of Americans who care deeply about climate change say the Earth is warming because of human activity (76%), this compares with 48% among those who care some and just 10% among those who do not care at all or not too much about this issue.\n\nDifferences between those who care more and less about climate change issues occur among both Republicans and Democrats. Some 44% of Republicans who care a great deal about climate issues believe human behavior is causing temperatures to rise, compared with just 17% of Republicans who care some or less about this issue. Similarly, among Democrats, 87% of those who care a great deal about climate issues believe human activity is mostly responsible for global climate change, compared with 52% among those who care some or less about the issue of climate change.\n\nLarge majorities of those who care most about this issue think it is very likely that climate change will hurt the environment. Roughly three-quarters of those deeply concerned about climate issues think climate change will very likely bring harm to animal life (74%), damage to forests and plants life (74%), more droughts (73%), more severe storms (74%), and damage to shorelines from rising sea levels (74%). By contrast, roughly a third of those who care \u201csome\u201d about this issue say each of these possible effects is very likely. Many of those who do not care at all or not too much about the issue of climate change say the evidence of warming is uncertain; this group is particularly skeptical that any of these harms will come to pass. Differences among the more and less concerned about climate issues occur both among Republicans and Democrats alike.\n\nThere are smaller differences when it comes to people\u2019s expectations that Americans will make major changes to their way of life in order to address climate change. About two-thirds of those who care a great deal about climate issues (67%) expect this to occur within the next 50 years, as does a similar share of those who care some about this issue (70%) and 42% of those who do not care at all or not too much about the issue of climate change. And, 63% of the more climate-engaged Americans expect new technological solutions to address most problems stemming from climate change. Those who care some about climate issues hold similar views; 62% expect technological solutions. Those who have little personal concern about the issue of climate change are more skeptical; 34% expect technological solutions, 64% do not.\n\nMajorities of climate-engaged Americans are optimistic that a range of both policy and individual actions can make a big difference in addressing climate change. Those less personally concerned about climate issues are considerably more pessimistic, by comparison.\n\nAbout eight-in-ten of those more deeply concerned about climate issues say restrictions on power plant emissions (80%) and an international agreement to limit carbon emissions (78%) can make a big difference in addressing climate change. Some 73% of this group says tougher fuel efficiency standards for cars and trucks can make a big difference, and seven-in-ten (70%) say the same about corporate tax incentives to encourage businesses to reduce the carbon emissions stemming from their activities. By contrast, no more than two-in-ten American who are not at all or not too personally concerned about climate issues think each of these policy actions can make a big difference, although a sizeable minority among this group says each can make a small difference. Those who care \u201csome\u201d about the issue of climate change fall in between these two extremes; roughly four-in-ten of this group say each of these policy actions can make a big difference; a roughly similar share says each can make a small difference.\n\nThe same pattern occurs when it comes to individual efforts to address climate change. Among those who care deeply about climate issues, 63% believe individual efforts to reduce the \u201ccarbon footprint\u201d linked with one\u2019s daily activities can make a big difference. Among those who care some about this issue, about half as many say this can make a big difference (33%), and most (58%) say it can make a small difference. Just 12% of those with little personal concern about climate change say individual efforts of this sort can make a big difference, 42% says this can make a small difference, and 43% says this will have almost no effect. Similarly, some 63% of those personally concerned about climate issues say more people driving hybrid and electric vehicles can make a big difference in addressing climate change, compared with 40% among those who care some about climate issues and just 13% among those who do not care at all or not too much about climate issues.\n\nPeople who care more deeply about climate issues are also more likely than others in the general public to see climate scientists\u2019 and their work in a positive light.\n\nNearly all (90%) Americans who are deeply concerned about climate change issues support a major role for climate scientists in related policy decisions, as do 68% of Americans with some personal concern about climate issues. About a third (34%) of those with not too much or no personal concern about climate issues say climate scientists should have a major role, and 41% say scientists should have a minor role in climate policy.\n\nThis pattern holds among both Democrats and Republicans. For example, some 87% of Republicans who care a great deal about climate issues say climate scientists should have a major role in climate policy. This compares with 48% among other Republicans.\n\nThose who care a great deal about climate issues are much more likely than other Americans to say climate scientists understand very well whether change is occurring (64% vs. 23% among those care some and 7% among those do not care at all or not too much about this issue). About half of those deeply concerned about climate issues (52%) say climate scientists understand very well the causes of climate change, compared with just 19% among those with some personal concern and just 8% among those with no or not too much personal concern about this issue.\n\nMore Americans who care a great deal about climate issues say scientists understand the best ways to address climate change very well (37%) or fairly well (48%). Many fewer of less climate-concerned adults say the same. Just 13% of those with some personal concern about climate issues say scientists understand very well how to address climate change, while 56% say scientists understand this fairly well. And, just 5% of those with no or little personal concern about climate issues say scientists understand very well how to address climate change, 25% say scientists understand this fairly well and 68% say scientists do not understand this at all or not too well. Differences over climate scientists\u2019 understanding occur among both Democrats and Republicans who are relatively more and less concerned about climate change.\n\nSimilarly, people who care more about climate issues are more inclined to see consensus among scientists about the causes of climate change. Some 48% of the climate-concerned public says that almost all climate scientists agree that human activity is responsible for climate change; this compares with just 19% saying almost all scientists agree about this among those who care some about climate issues and 12% among those who do not care at all or not too much about climate issues.\n\nThose more concerned about global climate issues are far more trusting of information from climate scientists than are those less concerned about these issues. Two-thirds of the public who cares a great deal about climate issues (67%) say they trust climate scientists a lot to provide full and accurate information on the causes of global climate change. In contrast, 33% of those who care some about climate issues trust scientists\u2019 information a lot, while 53% trust it some. Just 9% of those with little or no personal concern about climate issues trust scientists\u2019 information a lot, 36% trust it some and 55% have not too much or no trust in information from climate scientists about this.\n\nDemocrats and Republicans who care a great deal about climate issues are more than twice as likely as their fellow partisans to hold a lot of trust in information from climate scientists. Among Republicans who care about climate issues, 46% trust climate scientists\u2019 information a lot compared with 16% among other Republicans. Among Democrats, fully 76% of those who care about climate issues a great deal say they trust climate scientists\u2019 information a lot compared with 34% among other Democrats.\n\nAmericans who are more concerned with climate issues are inclined to think research findings on climate are influenced by the best available evidence; about half of this group (51%) says research is influenced by the best evidence most of the time and 39% say this occurs some of the time. In contrast, three-in-ten (30%) of those with some personal concern about climate issues say the best evidence influences climate research findings most of the time, 60% say this occurs some of the time. Just 9% of those with no or not too much personal concern about climate issues say the best evidence influences climate research findings most of the time, 42% say this occurs some of the time and 45% say this occurs not too often or never.\n\nBy the same token, there are similar differences in views about negative influences on research between those who care deeply about climate issues and those who do not; the climate-concerned public is less inclined to see such research as influenced by scientists\u2019 personal political leanings, a desire to help their industries or their careers.\n\nThe news media are a key source of information about climate issues. The Pew Research Center survey finds only a small minority (11%) of Americans follow news about climate matters very closely. Another 44% follow somewhat closely, and an equal share follows news not too (32%) or not at all closely (12%).\n\nOverall, Americans are closely divided in their assessments of media coverage on climate issues. Some 47% say the news media do a very or somewhat good job, while 51% say they do a bad job covering climate issues.\n\nThese findings stand in contrast to American\u2019s views about the media overall. As shown elsewhere in this report, just 5% say they have a great deal of confidence in the news media, generally, to act in the public interest. A 2013 Pew Research Center report documents the steep decline in public regard for media accuracy, fairness and independence over the past two decades.\n\nPeople who say they closely follow climate news tend to give the media somewhat higher marks for coverage in this area as do those who say care a great deal about climate issues.\n\nPublic views about media performance also tend to divide along political lines. Conservative Republicans are especially critical of media coverage on climate issues with 71% of this group saying the media do a bad job. Moderate and liberal Republicans are closely divided in their overall evaluations of news coverage on climate (47% say they do a good job and 52% say they do a bad job). The balance of opinion is more positive among moderate and conservative Democrats (64% good to 34% bad) though liberal Democrats are closely divided (48% to 51%) on this issue. This pattern is broadly consistent with other Pew Research Center studies on views of the media.\n\nThe public divide over media performance in this area could link to the balance of coverage on climate issues. The Pew Research Center survey included two additional questions exploring people\u2019s views about news coverage.\n\nOverall, some 35% of Americans say the media exaggerate the threat from climate change while a roughly similar share (42%) of adults says the media do not take the threat seriously enough. Two-in-ten (20%) adults says the media are about right in their reporting about climate.\n\nThe same pattern occurs on a question about the balance of attention to those skeptical of climate change. Four-in-ten (40%) adults say the media give too little attention to skeptics, while a slightly smaller share (32%) says the media give skeptics too much attention. A quarter of Americans (25%) say the media are about right in their coverage of those skeptical about climate change.\n\nIn keeping with the wide political divides on beliefs about climate issues, there are strong political differences in views about media coverage of climate change. A majority of conservative Republicans (72%) say the media exaggerate the threat of climate change, while some 64% of liberal Democrats say the media do not take the threat seriously enough.\n\nOpinions about media coverage of skeptics follow a similar pattern. Some 59% of conservative Republicans say the media give too little attention to skeptics of climate change. In contrast, about half of liberal Democrats (54%) say the media give too much attention to skeptics of climate change."},
{"url": "https://www.cyphort.com/threat-insights/adwind-rat/", "link_title": "Adwind RAT Analysis", "text": "Adwind is a backdoor written in JAVA and arrives thru spam email. \u00a0It\u2019s a cross-platform Remote Access Tool (RAT)\u00a0that can run on Windows, Mac OS, Linux and Android platforms. This malware is found to be sold in the dark web and was previously used by hackers to target banks. It has different names such as AlienSpy, Frutas, Unrecom, Sockrat, JSocket, and jRat.\n\nBelow is the list of capabilities of the Adwind malware:\n\n1. This malware arrives as an attachment in a spam Email:\n\n2. When executed, it creates a random name folder in the user directory and drops and executes a copy of itself like the example below:\n\n3. \u00a0It also creates an Autostart key for Persistence:\n\n4. \u00a0It adds the following registry entries to prevent AV processes or tools from starting:\n\nThe %Filename of Target Exe% is a string variable. Below is the list of strings it uses which are found to be AV related:\n\n5. It also tries to terminate the said processes if found running.\n\n6. It communicates with its C2 using TLSv1.2\n\n7. Once communication is established, it will exfiltrate data and may download additional payload such as other malware on the infected system.\n\nCyport detects Adwind and its C2 communication as TROJAN_ADWIND.CY."},
{"url": "http://quillette.com/2017/08/07/google-memo-four-scientists-respond/", "link_title": "The Google Memo: Four Scientists Respond", "text": ""},
{"url": "https://www.nytimes.com/interactive/2017/08/07/upshot/music-fandom-maps.html", "link_title": "What Music Do Americans Love the Most? 50 Detailed Fan Maps", "text": "If you feel as if you\u2019ve seen a version of this map before, you probably have: Squint and you may recall a viral map of the places that account for 50 percent of the country\u2019s gross domestic product. Or a map of the counties where Hillary Clinton beat Donald Trump (54 percent of the population, but just 15 percent of the land mass). Justin Timberlake\u2019s fan map is indeed like those in the sense that it is largely a map of American cities. Yet there is a notable exception: Southern cities do not particularly care for J-T. Even his hometown, Memphis, does not burn particularly brightly for him."},
{"url": "https://figshare.com/articles/Stinging_the_Predators_A_collection_of_papers_that_should_never_have_been_published/5248264", "link_title": "Stinging the Predators: Papers that should never have been published", "text": "This ebook collects thirteen papers that were intended to be unpublishable. All were submitted to predatory journals to expose non-existent peer review and exploitative practices. Each paper has a brief introduction. Short essays round out the collection.<br><br>169 pages.<br><br>Version 1.0 released 26 July 2017.<br>Version 2.0 released 28 July 2017 (two new entries).<br>Version 3.0 released 31 July 2017 (two new entries).<br>Version 3.1 released 7 August 2017 (error corrected)."},
{"url": "http://www.filfre.net/2017/08/a-tale-of-the-mirror-world-part-8-life-after-tetris/", "link_title": "A Tale of the Mirror World, Part 8: Life After Tetris", "text": "As the dust settled from the battle over Tetris and a river of money started flowing back to the Soviet Union, the Soviet Academy of Sciences made a generous offer to Alexey Pajitnov. In acknowledgment of his service to the state, they told him, they would buy him an IBM PC/AT \u2014 an obsolete computer in Western terms but one much better than the equipment Pajitnov was used to. This would be the only tangible remuneration he would ever receive from them for making the most popular videogame in the history of the world.\n\nBut the Soviet Union was changing rapidly, and Pajitnov intended to change with it. He formed a little for-profit company \u2014 such a thing was now allowed in the Soviet Union \u2014 called Dialog with a number of his old friends and colleagues from the Moscow Computer Center, among them his friendly competitor in game-making, Dmitry Pavlovsky, and the first of many psychologists who would be fascinated by the Tetris Effect, Vladimir Pokhilko. The name of the company was largely a reflection of a pet project cooked up by Pajitnov and Pokhilko, a sort of cross between Eliza and Alter Ego with the tentative name of Biographer. \u201cThis kind of software can help you to understand your life and change it,\u201d Pajitnov believed. \u201cThis will help people. This is what I would call a \u2018constructive game.'\u201d Such a game would be, needless to say, a dramatic departure from Tetris, demanding far more time to develop than had that exercise in elegant minimalism.\n\nIn the meantime, though, Tetris was huge. Henk Rogers, now well-established as Pajitnov\u2019s mentor in the videogame business, advised him strongly to make a sequel, and to do it soon \u2014 for if you don\u2019t strike soon, he told him, someone else will. Pajitnov therefore came up with a game he called Welltris, a three-dimensional Tetris in which the shapes fell down into a \u201cwell\u201d \u2014 thus the name \u2014 which the player viewed from above. While Welltris lacked the immediate, obvious appeal of Tetris, some players would come to love it for its subtle complexity. Pajitnov had no problem selling it to Spectrum Holobyte in North America, who despite their affiliation with the hated Mirrorsoft had managed by dint of luck and cleverness to remain in his good graces. In Europe, the game was picked up by the French publisher Infogrames. It was released for personal computers on both continents before 1989 was through.\n\nPajitnov greeted a new year and a new decade by taking his first trip to the United States. Paid for by Spectrum Holobyte, whose resources were limited, it was an oddly austere promotional junket for the designer of the most popular videogame in the world. He made the trip alone but for a translator to help out with his still-broken English. His first stop was Las Vegas, for the Winter Consumer Electronics Show \u2014 quite the introduction to American excess! He sat gaping in wonder at the food piled in mounds before him at the hotel\u2019s $3.69 all-you-can buffet, flicked his \u201cI Love Vegas\u201d cigarette lighter, and remarked, \u201cSo this is a typical American city.\u201d After Vegas, he traveled around the country in what a Boston Globe reporter described as \u201calmost an underground manner,\u201d \u201capartment to apartment, computer friend to computer friend.\u201d But he was hardly a man of expensive tastes anyway; out of all the food he ate on the trip, it was Kentucky Fried Chicken he liked best.\n\nSpectrum Holobyte did spring for a lavish press reception in San Francisco\u2019s St. Francis Hotel. The day after that event, he made time for a \u201cU.S./Soviet Personal Computer Seminar\u201d at San Francisco State University out of his desire to \u201cgrow up the game life in Moscow. I want to help, with advice.\u201d He visited Minoru Arakawa and Howard Lincoln at Nintendo of America\u2019s headquarters in Seattle; got to see in person paintings at the Museum of Modern Art and the Metropolitan in New York which he\u2019d known only from the books in his parents\u2019 library; visited MIT\u2019s Media Lab to view cutting-edge research into virtual reality, trying not to compare the technology that surrounded him there too closely to the spartan desk he had to share with others back at the Moscow Computer Center. And between all these glimpses of American life, he gave interview after interview to an endless stream of journalists eager to get their whack at one of the great current human-interest stories. He made time for everyone, from the slick reporters from the major newspapers and magazines to the scruffy nerds from the smallest of the trade journals. One and all treated him as living proof of the changing times, a symbol of the links that were being forged between East and West in this new, post-Cold War order.\n\nHis odyssey wound up in Hawaii with Henk Rogers, swimming and kayaking and drinking mai tais. The two friends were a long, long way from the gray streets of Moscow where they had met, but the bond they had forged there endured. Over drinks one gorgeous starlit evening, Rogers asked Pajitnov if he would be interested in leaving the Soviet Union permanently to work on games in the West. Torn between the wonders he had just seen and everyone\u2019s natural love for the place he came from, he could only shrug for now: \u201cI do not have an answer for that question.\u201d\n\nRogers had had good reason for asking it. Ever ambitious, he had used the first influx of cash from Tetris to establish a new branch of Bullet-Proof Software in Seattle, conveniently close to Nintendo of America\u2019s headquarters. Broadly speaking, his intention was to do in the North American Nintendo market what he had been doing in Japan: find games in other countries and on other platforms that would work well on the Nintendo Entertainment System and/or the Game Boy, license the rights, and port them over. His first big North American release, a puzzle game called Pipe Dream that had already been a hit on home computers in Europe, would do very well on the NES and Game Boy as well.\n\nYet Rogers was also eager to do original games with Pajitnov. He had passed on Welltris, whose 3D graphics were a little more than the Nintendo machines were realistically capable of, but kept cajoling Pajitnov to come up with yet another, more Nintendo-friendly Tetris variant. The result was Hatris, where the falling shapes of Tetris were replaced with falling hats which had to be stacked atop one another according to style. Although it presaged the later craze for matching games in the casual-game market even more obviously than had Tetris, it wasn\u2019t all that great a game in its own right. Even on his American publicity tour, when it was still in the works, Pajitnov described it without a lot of enthusiasm. The sub-genre he had created was already in danger of being run into the ground.\n\nBut the industry, inevitably, was just getting started. The next several years would bring heaps more variations on the Tetris template, a few of them crediting their design fully to Pajitnov, some of them crediting him more vaguely for the \u201cconcept,\u201d some of them not crediting him at all. Some ran on computers, some ran on consoles from Nintendo and others. Some were very playable, some less so. Personally, I have a soft spot for 1991\u2019s Wordtris, a game designed by two of Pajitnov\u2019s Russian partners at Dialog where you have to construct words, Scrabble-style, out of falling letters. In addition to its other merits, it became another casual pioneer, this time of the sub-genre of word-construction games. But then, I\u2019m far better at verbal puzzles than spatial ones, so my preference for the wordy Wordtris should perhaps be taken with a grain of salt.\n\nDespite all the industry\u2019s enthusiasm for Tetris-like games, Pajitnov and Pokhilko\u2019s plans for an Eliza killer came to naught. Publishers were always willing to use Pajitnov\u2019s name to try to sell one more falling-something game, but didn\u2019t think it had much value attached to high-concept fare like Biographer.\n\nIn 1991, Pajitnov finally answered in the affirmative the question Rogers had posed to him on that evening in Hawaii; he and his family immigrated to San Francisco. Cutting ties with Dialog back in a Soviet Union that was soon to be known simply as Russia again, he formed a design partnership with Pokhilko, who soon joined him in the United States. Over the next several years, the two created a variety of simple puzzle games, some more and some less Tetris-like, for various publishers, along with at least one truly outr\u00e9 concept, the meditative, non-competitive \u201caquarium simulator\u201d El-Fish.\n\nMeanwhile the times were continuing to change back in Russia, as piece after piece of the state-owned economy was privatized. Among the entities that were spun off as independent businesses was ELORG. Nikoli Belikov, savvy as ever, ended his career in the bureaucracy by becoming the owner and chief executive of the new ELORG LLC.\n\nHenk Rogers had all but promised Pajitnov shortly after they had met in Moscow that, although he might not be able to secure a\u00a0Tetris royalty for him right away, he would take care of him in the long run.\u00a0He had indeed looked out for him ever since \u2014 and now he was about to deliver the ultimate prize. He came to Belikov with a proposal. Belikov still didn\u2019t know much about the videogame business, he said, but he did. In return for a 50 percent stake in Tetris, he would take over the management of what was by now not so much a videogame as a global brand. Belikov agreed, and Rogers and Pajitnov together formed The Tetris Company in 1996 to manage the Western stake. And so at last Alexey Pajitnov started getting paid \u2014 and paid very well at that \u2014 for his signature creation.\n\nFor Rogers, protecting Tetris represented an almost unique challenge. More than virtually any other videogame, the genius of Tetris is in the concept; the implementation is trivial in comparison, manageable by any reasonably competent programmer within a few weeks. And, indeed, the public-domain and shareware software communities in the West had been flooded with clones and variants almost from the moment the game had first appeared on Western computers in 1988, just as had been the land behind the Iron Curtain in the years prior to that. No more, said Rogers. He hired a team of lawyers to go after anyone and everyone who made a game of falling somethings without the authorization of The Tetris Company, attacking with equal prejudice those who tried to sell their versions and those \u2014 often beginning game programmers who were merely proud to show off their first creations \u2014 who shared them for free. His efforts created no small uproar on the Internet of the late 1990s, leaving him to take plenty of heat as, as he once put it himself, \u201cthe jerk behind The Tetris Company.\u201d\n\nTo this day, the bulk of The Tetris Company\u2019s time and energy is devoted to the relentless policing of their intellectual property. As one would expect, Rogers and company have tended to draw the broadest possible line around what constitutes an infringing Tetris clone. The location of the actual line between legality and illegality, however, remains curiously unresolved. The Tetris Company has always had a lot of money and a lot of lawyers to hand, and no one has ever dared engage them in a legal battle to the death over the issue.\n\nIn addition to managing The Tetris Company, Henk Rogers continued to run Bullet-Proof Software throughout the decade of the 1990s. The first half of that period was marked by a number of successful non-Tetris titles, such as the aforementioned Pipe Dream, but over time Bullet-Proof increasingly dedicated themselves to churning out permutation after permutation on a game that many would argue had been born perfect: Tetris 2, Tetris Blast, V-Tetris, Tetris S, Tetris 4D. By decade\u2019s end, they were running out of names. Screw it, they said in 1998, we\u2019ll just call the next Tetris\u2026 The Next Tetris.\n\nBullet-Proof closed up shop shortly after that game, but Rogers formed Blue Lava Wireless in 2002 to make games for the first wave of feature phones. Their most successful titles by far were\u2026 you guessed it, mobile versions of Tetris. Indeed, the convergence of Tetris with mobile phones drove a second boom that proved just as profitable as the first, Game Boy-driven wave of mobile success.\n\nHaving long since sold Blue Lava, Rogers lives the good life today in his first geographical love of Hawaii, running the Blue Planet Foundation, which has the laudable goal of ending the use of carbon-based fuels in Hawaii and eventually all over the world; he also oversees a commercial spinoff of the foundation\u2019s research called Blue Planet Energy. He\u2019s still married to the girl who tempted him to move to Japan all those years ago. And yes, he\u2019s a very, very rich man, still making millions every year from Tetris.\n\nAlexey Pajitnov has continued to kick around the games industry, plying his stock-in-trade as a designer of simple but (hopefully) addictive puzzle games and enjoying his modest celebrity as the man who made Tetris. He spent several years at Microsoft, where he was responsible for titles like\u00a0The Microsoft Puzzle Collection and Pandora\u2019s Box; some of the puzzles found therein were new, but others were developed contemporaneously with the original Tetris during all those long days and nights at the Moscow Computer Center. He\u2019s slowed down a bit since 2000, but keeps his hand in with an occasional mobile game, which his reputation is always sufficient to see published. In light of his ongoing design work, it was perhaps a bit unkind of a 2012 IGN article to call him one of the games industry\u2019s \u201cone-hit wonders.\u201d Still, Tetris was so massively important that it was all but an inevitability that it would overshadow every other aspect of his career. Pajitnov, for his part, seems to have made his peace with that, humoring the wide-eyed reporters who continue to show up to interview him at his current home near Seattle. It may have taken Tetris a long time to pay off for him, but he can have no complaints about the rewards it brings him today: he is, like Henk Rogers, very, very rich. The two men remain close friends as well as business partners in The Tetris Company; the warm relationship forged over vodka on that cold Moscow night back in February of 1989 continues to endure.\n\nNikoli Belikov, the most unlikely person ever to become a millionaire thanks to videogames, finally cashed in his Tetris chips in 2005, selling his stake in the game to The Tetris Company for $15 million. Doing business in his country remained as complicated as ever at that time. The only difference was that Belikov now had to fear the Russian Mafia finding out about his windfall rather than the KGB and the other entrenched forces of the old communist system. In an episode that had much the same spy-movie flavor as the original Tetris negotiations, Belikov and Rogers signed the deal and exchanged funds in Panama. The former then went back home to Moscow to enjoy a retirement worthy of an oligarch.\n\nThe other people Alexey Pajitnov left behind in Russia weren\u2019t quite so fortunate. Dialog, the company he had helped found there before emigrating, collapsed soon after the departure of their star designer. And with the end of Dialog ended the game-design career of Dmitry Pavlovsky, who went back to other computer-science pursuits.\n\nVadim Gerasimov, the third member of the little game-making collective that had spawned Tetris, had parted ways with Pajitnov even before the brief-lived Dialog experiment. The most idealistic hacker in the Russian camp, the circus that sprang up around the Tetris rights had never set well with him. He claims that at some point after ELORG got involved in the negotiations Pajitnov came to him with a paper to sign. It stated, according to him, \u201cthat I agree to only claim porting Tetris to the PC, agree to give Pajitnov the right to handle all business arrangements, and refuse any rewards related to Tetris. I did not entirely agree with the content, but I trusted Alexey and signed the paper anyway.\u201d Gerasimov had been given credit right after Pajitnov himself as the \u201coriginal programmer\u201d of\u00a0Tetris in the Mirrorsoft and Spectrum Holobyte versions, but by the time the Nintendo versions appeared his name had been scrubbed from the game. To my knowledge, he never made any money at all from writing the first version of Tetris to reach beyond the Iron Curtain. While I hesitate to condemn Pajitnov or anyone else too roundly for that state of affairs \u2014 after all, even Pajitnov himself wasn\u2019t paid for many years for his game \u2014 it does strike me as unfortunate that Gerasimov was allowed, whether consciously or accidentally, so completely to slip through the cracks. Had he not lent Pajitnov his talents, it\u2019s highly unlikely that the game would ever have become more than a curiosity enjoyed by a handful of people in and around the Moscow Computer Center.\n\nGerasimov does evince a tinge of bitterness when he speaks about the subject, but, to his credit, he hasn\u2019t let it consume his life. Instead he\u2019s made a fine career for himself, immigrating to the United States, earning a doctorate from MIT, and finally winding up in Australia, where he works for Google today as a software engineer. He claims not to agree with The Tetris Company\u2019s policy of so zealously protecting the property \u2014 although his position should perhaps be considered with a certain skepticism in light of the fact that he has never been in a position to benefit from that protection.\n\nIt was Pajitnov\u2019s good friend and frequent design partner Vladimir Pokhilko who came to by far the worst end among the Russians who were there to witness Tetris\u2018s birth. Having cut his business ties with Pajitnov after the latter took a staff job with Microsoft in 1996, he tried to make a go of it in Silicon Valley, but bet on all the wrong horses. Beset by financial problems and rumored entanglements with the Russian Mafia, in 1998 he murdered his wife and son with a hammer and a hunting knife, then cut his own throat. His good-natured comment to his buddy at the Moscow Computer Center in 1984 \u2014 \u201cI can\u2019t live with your Tetris anymore!\u201d \u2014 suddenly took on a different dimension in the aftermath. Pajitnov, so garrulous on most subjects, clams up when the topic turns to Pokhilko, the old friend who was obviously hiding a profound darkness behind his cheerful smile. \u201cWe [were] always friends, colleagues, and partners with good and warm relations,\u201d he says, and leaves it at that \u2014 as shall we.\n\nPokhilko wasn\u2019t the only character in the Tetris story to come to a bad end. Sometime during the early morning of November 5, 1991, Robert Maxwell jumped or fell off the deck of his luxury yacht into the sea near the Canary Islands. His body was discovered by a passing fishing boat the next day.\n\nWhen executors and regulators began to look closely at Maxwell\u2019s personal finances and those of his vaunted publishing empire in the aftermath of his death, what they found appalled them. He had racked up more than $3 billion in debts, and had been stealing from his employees\u2019 pension funds and making countless other illegal transactions in order to shore up both his business interests and his own lavish lifestyle. Pathologists hadn\u2019t been able to agree on a definite cause of death after his battered body was recovered. In light of this, and in light of his political and financial entanglements all over the world, not to mention the accusations of espionage that have occasionally dogged him, conspiracy theories abound about his fate. One suspects, however, that the more prosaic explanations are the more likely: either he deliberately threw himself into the water to escape the financial reckoning he knew was coming, or, being grossly overweight, he had a heart attack while taking in the view and simply fell into the water. There is, however, one other oddity about the whole thing to reckon with: his body was completely naked when it was recovered. It\u2019s highly doubtful that the mystery of Robert Maxwell\u2019s death will ever be solved beyond a shadow of doubt.\n\nIn the wake of the Maxwell empire\u2019s collapse, Kevin Maxwell filed for the biggest personal bankruptcy in British history, writing off more than $1 billion in debts. He was taken to trial for conspiracy to commit fraud for his involvement with his father\u2019s house of cards, but was acquitted. His business record since has been checkered, encompassing another huge bankruptcy and repeated accusations of malfeasance of various stripes.\n\nLike most of Robert Maxwell\u2019s properties, Mirrorsoft was sold off in the scandal that followed his death. The operation wound up in the hands of Acclaim Entertainment, but the name disappeared forever.\n\nOver in the United States, Spectrum Holobyte managed to live considerably longer. Phil Adam and Gilman Louie, the partners who had long run the publisher, pulled together enough venture capital in the midst of the Maxwell empire\u2019s collapse to buy their complete independence. They then went on another buying spree as the merry 1990s got going in earnest, picking up both the computer-game publisher MicroProse and the American division of Henk Rogers\u2019s Bullet-Proof Software in 1993. Combined with the good relations they enjoyed with Rogers and Pajitnov, the latter purchase seemingly left them well-positioned to continue to exploit Tetris for years to come. But they had extended themselves too far too quickly, picking up a mountain of debt in the process. Caught out in between the first and the second great Tetris booms, they were never quite able to turn the corner into reliable profitability. Hasbro Interactive bought the troubled company in 1998, and the name of Spectrum Holobyte also vanished into history.\n\nRobert Stein lost his rights to Tetris on personal computers in 1990, when ELORG terminated his license due to his ongoing failure to pay them in a timely manner. Stein did cite an excuse for his tardiness this time, claiming that Mirrorsoft had simply stopped paying him for their sub-license altogether amid the brouhaha of 1989. Since Spectrum Holobyte was paying their royalties to Mirrorsoft, who were supposed to then pass them on to Stein, such a refusal would have meant that Stein himself would have received nothing at all to pass on to the Russians. Regardless of the full truth of the matter, Stein was forced out of the picture, and Spectrum Holobyte negotiated their own license directly with ELORG in order to continue making their version of Tetris.\n\nStein lost his last remaining Tetris deal, for the arcade rights, in 1992. Once again, ELORG cited non-payment as their reason for terminating the contract, and once again Stein claimed that his sub-licensee \u2014 this time Atari Games \u2014 wasn\u2019t paying him.\n\nYears before this event, Stein\u2019s little company Andromeda Software, thoroughly unequipped to compete in the evolving international videogame market, had ceased to exist as anything other than a paper entity. With the termination of his last ELORG deal ended his time in games. I don\u2019t know what he\u2019s been doing in the years since, but he is alive and apparently well.\n\nTo this day, however, Stein remains deeply embittered against virtually every other character in the story of Tetris. His own narrative is at odds with that of the other principals on a number of key points. He confesses only to naivet\u00e9 and perhaps the occasional bout of carelessness, never to deliberate wrongdoing. In his telling, the story of February and March of 1989 is that of a premeditated conspiracy, orchestrated by Henk Rogers, to steal Tetris away from him. While he admits to having earned about $150,000 to $200,000 from Tetris \u2014 no mean total in the context of most videogames of its era \u2014 he\u2019s clearly haunted by all the tens of millions earned by Rogers, Pajitnov, and Belikov. \u201cTetris made enemies out of friends and corrupted people left, right, and center,\u201d he says.\n\nThe war between the two Ataris and Nintendo raged on long after the issue of the Tetris rights was decided in November of 1989.\n\nIn May of 1989, Atari Games had launched still another lawsuit against Nintendo, this time alleging them to have violated their patent on an \u201capparatus for scrolling a video display\u201d from 1979. Meanwhile Nintendo continued to put the squeeze on Atari at retail, threatening to cut off stores who dared to stock the Tengen games. Atari\u2019s Dan Van Elderen claims that eventually all fifteen of the largest retail chains in the country dropped Tengen in the face of such pressure.\n\nAtari found a friendly ear for their tale of woe in United States Congressman Dennis Eckart. The Democrat from Ohio was the chairman of a subcommittee focused on antitrust enforcement, and he was already inquiring into Nintendo\u2019s business practices when he was contacted by Van Elderen. Van Elderen and other Atari executives became star witnesses in building the case for an official government investigation into Nintendo, while Eckart never even contacted anyone from the opposing camp to ask for their side of the story. On December 7, 1989, the 48th anniversary of the attack on Pearl Harbor \u2014 the timing struck very few as coincidental \u2014 Eckart held a press conference to announce his recommendation that the Justice Department launch a probe of Nintendo\u2019s role in the videogame market. As Howard Lincoln would later note, the press conference couldn\u2019t have favored Atari\u2019s position more had the latter written the script \u2014 which, in light of the cozy relationship that had sprung up between Eckart and Atari, there was grounds to suspect they had. The Justice Department soon handed the case to the Federal Trade Commission.\n\nWhile the FTC investigated, Eckart continued to speak out against the Menace from Japan, blending long-established generational hysteria against videogames in general with Japophobia. Like many in the software industry, his greatest fear was that Nintendo would turn the NES into a real home computer, as they were trying to do to the Famicom in Japan, and use it to take over the entire market for consumer software. Think of the children, he thundered: \u201cHow did they [Nintendo] get in American homes? They enticed their way in through our children\u2019s hearts. If you turn a toy into a computer, what\u2019s the next step?\u201d\n\nUnder mounting pressure from several sides, Nintendo eased their licensing conditions somewhat on October 22, 1990, allowing some licensees to begin manufacturing their own game cartridges. They also dropped the provisions from their standard licensing agreements that restricted their licensees from releasing their games on rival platforms. Many publishers would admit privately that the removal of this specific language from the contract changed little in actuality \u2014 \u201cI\u2019m not going to make games for competing systems because we know that Nintendo would get even, one way or another,\u201d said one \u2014 but it did do much to nullify the primary charge in Jack Tramiel\u2019s Atari Corporation\u2019s case against Nintendo: that not allowing licensees to release a game on a rival platform for two years after its appearance on the NES constituted an abuse of monopoly. Some suspected that the changes at Nintendo had been the result of a deal with the FTC which allowed them to avoid some charges of engaging in anti-competitive practices.\n\nAlthough Nintendo was clearly moderating some of their stances in response to recent developments, Atari Games still couldn\u2019t manage to get a win in Judge Fern Smith\u2019s courtroom. On the contrary: on March 27, 1991, she handed them another devastating defeat. A forensic study of the code used in the lockout-defeat mechanism employed by the Tengen cartridges having shown it to be virtually identical to Nintendo\u2019s own, Judge Smith excoriated Atari for their actions in words that left no doubt about her opinion of their business ethics. She certainly made no effort to sugarcoat her verbiage: \u201cAtari lied to the Copyright Office in order to obtain the copyrighted program,\u201d she bluntly wrote. After having killed the Tengen Tetris fifteen months before, she now ordered all unauthorized Tengen games for the NES to be pulled from the market.\n\nOn April 10, 1991, the FTC announced that they had charged Nintendo with price-fixing in light of the latter\u2019s policy of demanding that retailers not discount their products. But, anxious to avoid another ugly public legal battle, Nintendo had agreed to a settlement which required them to discontinue the policy in question, to pay $4.75 million to cover the government\u2019s administrative costs in conducting the investigation, and to send consumers who the government claimed had been negatively affected a $5 coupon good for future Nintendo purchases. And with that, the FTC\u2019s Nintendo probe was effectively finished. Everyone agreed that Nintendo had gotten off rather shockingly easy in being allowed to turn what should have been the negative press of a major judgment against them into what amounted to a new sales promotion. It almost seemed like someone at the FTC had a soft spot for them. Both Ataris blasted the easy treatment of Nintendo in the press, indelicately implying that something underhanded must be going on between Nintendo and the FTC.\n\nBut brighter news for the Atari camp did come the very next day. Atari Games had bitterly contested Judge Smith\u2019s injunction stating that they couldn\u2019t sell their Tengen games on the NES unless and until their appeal of her most recent ruling was concluded in their favor. They claimed the injunction could very well drive them out of business before that day arrived, making their ongoing appeal moot. On April 11, the appeals court agreed to give them back their right to sell the Tengen games while the legal proceedings ground on.\n\nWhatever the situation in court, the two Ataris could feel fairly confident that they were at the very least holding their own in the public-relations war. In January of 1992, Michael Crichton summed up the mood of many inside and outside the American government with his novel Rising Sun. A thinly disguised polemic against Japan \u2014 Crichton himself described the book as a \u201cwake-up call\u201d to his country about the Japanese threat \u2014 it centers on a fictional corporation called Nakamoto whose mysterious leader sits far away in Japan at the center of a web of collusion and corruption. It was hard not to see the parallels with Nintendo\u2019s greatly-feared-but-seldom-seen president Hiroshi Yamauchi. Price-fixing and other forms of collusion are \u201cnormal procedure in Japan,\u201d says one of Crichton\u2019s characters. \u201cCollusive agreements are the way things are done.\u201d\n\nJust months after the publication of the novel, Yamauchi confirmed all of its worst insinuations in the eyes of some when he bought the Seattle Mariners baseball team. The Japanese, it seemed, were taking over even the Great American Pastime. What was next, Mom and apple pie? Major League Baseball approved the sale only on the condition that the day-to-day management of the team remain in American hands.\n\nHoping to capitalize on the political sentiment that so often painted Nintendo as a dangerous foreign invader, Tramiel\u2019s Atari Corporation elected to take their $250 million lawsuit against Nintendo to a jury trial. But whatever abuses Nintendo may have committed, they had been smart enough not to give Atari any smoking guns in the form of written documentation of their more questionable policies. When Minoru Arakawa took the stand, he faced a barrage of aggressive accusations from Atari\u2019s lawyers.\n\nOn May 1, 1992, the verdict came back. The jury did acknowledge that Nintendo had enjoyed a de facto monopoly over the American console market at the time Atari had filed their suit, but just having a monopoly is not illegal in itself. The jury found that Atari hadn\u2019t managed to prove that Nintendo had abused their monopoly power. Atari Corporation would elect not to appeal the verdict.\n\nOn September 10, 1992, Atari Games lost their appeal to Judge Smith\u2019s ruling against them of the previous year, and her overturned injunction against them went into permanent effect. All Tengen games on Nintendo\u2019s platforms were to be pulled from store shelves and destroyed, effective immediately. Tengen and Nintendo had been parted forever.\n\nWith these last two rulings, the war entered the mopping-up phase, the final result all but a foregone conclusion. In 1990, the delicate stock-balancing act that had allowed Hideyuki Nakajima to run Atari Games as an independent entity had collapsed, and Time Warner had assumed control. The latter was skeptical from the beginning of this war that felt like it had at least as much to do with pride and legacy as it did with sound business strategy. Following these latest setbacks, they pressed Nakajima hard to cut his losses and settle the remaining legal issues. Atari Games and Nintendo announced a closed settlement agreement on March 24, 1994, that put the last of the litigation to bed, presumably at the cost of some number of millions from Atari.\n\nShortly thereafter, Atari Games ceased to exist under that name. On April 11, 1994, Time Warner went through a restructuring which saw Atari and Tengen subsumed into the preexisting subsidiary Time Warner Interactive. With the arcade market slowly dying and Nintendo certainly not likely to let them back onto their platforms anytime soon, the storied name of Atari had become a liability rather than an asset.\n\nAtari Corporation came to a similarly dispiriting end. After years of creeping irrelevancy brought on by the slow decline of their ST line of computers and the more dramatic failure of the Atari Jaguar, a quixotic last-ditch effort to launch a game console to compete directly with Nintendo, the remnants of the company were scooped up by JTS Storage, a maker of hard disks, on July 30, 1996. Thanks to the financial contortions that were used to bring off the deal, the transaction was counter-intuitively recorded as an acquisition of JTS by Atari, but there was no doubt on the scene about who was really acquiring whom. Like Time Warner, JTS saw little remaining value to the Atari name; they had acquired Atari Corporation for nothing more nor less than a pile of cold hard cash the company had recently collected after winning a patent-infringement judgment against Sega, not in the hope of making Atari mean something again to a new generation of gamers for whom the name\u2019s glory days were ancient history. But the money didn\u2019t do them much good; JTS went bankrupt in 1999.\n\nIn a previous article, I called the war between the two Ataris and Nintendo the past of videogames versus their future. As we can now see, that description is almost literally true. The two Ataris could perhaps console themselves that they had forced some changes in Nintendo\u2019s behavior, but they had paid a Pyrrhic price for those modest tactical victories. After the war was over, both Ataris died while Nintendo thrived. Winning the war so utterly became one of the proudest achievements of Howard Lincoln, that unapologetically vindictive master strategist. \u201cLincoln\u2019s motto was \u2018fuck with us and we will destroy you,'\u201d said one of Nintendo\u2019s lawyers. \u201cOtherwise he\u2019s a really nice guy.\u201d\n\nYet the full import of the war extended far beyond its importance to the individual combatants: it marked a watershed moment for the way that software is sold. Buried in the text of Judge Smith\u2019s 1991 ruling against Atari Games was the statement that legitimized the future not just of console-based videogames but of much of the rest of the consumer-software market. Irrespective of the shady methods Atari had employed to violate Nintendo\u2019s patent in the case at hand, Judge Smith affirmed that Nintendo did have the abstract right to \u201cexclude others and reserve to itself, if it chooses\u201d complete control of the Nintendo cartridge market. This statement essentially reversed an established precedent, dating back to an antitrust case that was decided against IBM in 1969, that a hardware manufacturer could not decide what software was allowed to run on their machines. With the legal cover Judge Smith provided, what had once been shocking enough to set most of the American software industry up in arms soon became routine. Every successful console that would follow the NES, from Nintendo or anyone else, would use the walled-garden model. Even more significantly, virtually every significant new software market to arise in the future, such as the mobile-phone and tablet markets that thrive today, would also be a walled garden controlled by a single corporation. Today, the un-walled marketplace for personal-computer software has become the exception rather than the rule, a shambling ghost from the past which no corporation has yet been able to corral. And long may it shamble on, for it continues to provide a haven in interactive media for the experimental, the controversial, the iconoclastic, and the esoteric \u2014 all the things the walled gardens reject.\n\nThe anti-Japanese, anti-Nintendo sentiment in the country, which had threatened to reach xenophobic levels in some circles, gradually faded as the decade wore on and Nintendo lost some of their standing as the be-all, end-all in videogames. Their much-feared strategy of using the NES as a Trojan Horse to take over all of consumer computing never really got off the ground. The enhancements that turned the Famicom into a full-fledged computer had never done tremendously well in Japan, and the Nintendo Network there turned into one of the company\u2019s rare outright failures, never getting beyond the tens of thousands of subscribers. A survey found that the biggest source of consumer resistance to the idea was, ironically, Nintendo\u2019s established reputation in videogames. People just weren\u2019t excited about using what they thought of as their children\u2019s toys to manage their stock portfolios. In light of these setbacks in Japan, Nintendo never introduced either the \u201ccomputery\u201d hardware enhancements they had tried on the Famicom or the Nintendo Network to North America. They instead elected to content themselves with their lot as the biggest company in videogames, much to the relief of the American software industry.\n\nBut even in the field of videogames, Nintendo wouldn\u2019t stand alone for much longer. Sega had introduced their Genesis console in North America already at the end of 1989. Following a slow start, it eventually turned into a viable competitor for the aging NES. At mid-decade, Sony arrived with the PlayStation as a third major player in the game-console space. While both Sega and Sony adopted Nintendo\u2019s walled-garden approach to software, one could no longer claim that videogaming writ large in North America lived by the whims of a single company. Yes, one could still be unhappy that all three popular console-sellers in the United States were Japanese \u2014 another successful born-in-America console wouldn\u2019t arrive until the Microsoft Xbox in 2001 \u2014 but even that concern faded somewhat with the tech boom of the mid- and late-1990s. In this market, there was plenty of room for everyone, even the shifty-eyed foreigners.\n\nMinoru Arakawa resigned as president of Nintendo of America in January of 2002. When not enjoying semi-retirement, he has since worked with Henk Rogers and Alexey Pajitnov on various projects related to Tetris.\n\nHoward Lincoln was appointed CEO of the Seattle Mariners in 1999, signaling a scaling back in his involvement with Nintendo proper. He continued to run the team until 2016. His tenure produced no triumphs to compete with his great victory over the two Ataris; the team made the playoffs the first two years with him at the helm, but never again after that.\n\nIn 2003, the North American arm of the French publisher Infogrames re-christened themselves Atari. Any hopes they might have had to revive the name\u2019s glory days were, however, sadly disappointed. After years of lurching from crisis to crisis, the new Atari filed for Chapter 11 bankruptcy in 2013. Today a skeleton staff makes casino games under the name.\n\nAs for Nintendo\u2026 well, Nintendo remains Nintendo, of course. They\u2019ve long since surpassed their old rival Atari to become the most iconic name in videogames. Once, parents would say that their children liked to \u201cplay Atari,\u201d regardless of what name happened actually to be printed on the front of their game console. Now, they say their children like to \u201cplay Nintendo\u201d \u2014 and, thanks largely to what\u00a0Tetris first began, they\u2019ll often say that they \u201cplay Nintendo\u201d themselves. Nintendo has had ups and downs over the years, but have on the whole remained ridiculously successful thanks to the same old blending of strategic smarts with a deep well of ruthlessness ready to be employed when they judge the situation to call for it. Some variant or another of Tetris \u2014 usually more than one \u2014 has been a staple of every Nintendo machine since the NES and Game Boy.\n\nAnd so we\u2019re left with only one fate left to describe: that of the place where we began this journey, the Mirror World of the Soviet Union.\n\nThe dawn of the 1990s was an extraordinary time in the often fraught history of East/West relations. The opening of the Soviet Union brought with it the expectation that the world was witnessing the dawn of a new economic superpower even as a military superpower fell. Freed from the yoke of communism, Russia seemingly had everything going for it. It was a sprawling land bursting with natural resources, with an educated population eager to shed their isolation and become a part of the free world\u2019s economic and political order. This was the period when Francis Fukuyama was writing The End of History, claiming that with the end of the Cold War free societies and free markets had won history\u2019s argument, leaving humanity with nothing left to do but enjoy their fruits.\n\nFew industries were more excited about jumping through the mirror and doing business in the lands beyond than the computer industry. As relations improved between West and East, the restrictions implemented by the Coordination Committee on Export Controls in the West were gradually eased. Already by 1988, it had been permitted to sell 16-bit microprocessors like the 80286 to the Soviet Union; by 1990, 32-bit processors like the 80386 were also allowed; by 1991, the restrictions as a whole were no more. Conferences and seminars sprung up, places for Western business executives to meet with former Eastern government bureaucrats newly thrust into the same role by their countries\u2019 privatizing economies.\n\nOf course, those Westerners peering eagerly through the mirror still had their work cut out for them in lots of ways. Most of the Eastern European economies were in complete disarray, with devalued currencies and thus little hard cash for buying Western products.\n\nBut where there\u2019s a desire to do business, there\u2019s usually a way. Some enterprising Western exporters resorted to complicated three-party deals. The would-be computer exporter would give their machines to an agent, who would send them on to Eastern Europe in exchange for raw goods. The agent would then sell the goods back in the West, and give the computer exporter a chunk of the profits. By 1992, an 80286-based PC in Russia cost about $1100. This was certainly better than the $17,000 one could have expected to pay for a shoddy Apple II clone nine years before, even if that kind of money would buy you a much more powerful 80386-based machine in the United States.\n\nIssues of Byte magazine from the early 1990s buzz with excitement about the opportunities awaiting citizens of the Mirror World, not to mention those in the West who planned to guide them down the tricky paths of capitalism. \u201cThese are people who have felt useless \u2014 useless \u2014 all their lives!\u201d said American business pundit Esther Dyson of the masses getting their first taste of freedom. \u201cDo you know what it is like to feel useless all your life? Computers are turning many of these people into entrepreneurs. They are creating the entrepreneurs these countries need.\u201d\n\nAnd yet, sadly, the picture of a Russian entrepreneur in the popular imagination of the West of today is that of a mobster. Under the benighted stewardship of Boris Yeltsin, the high hopes for the Russia of the early 1990s gave way to the economic chaos of the mid- and late-decade years, paving the way for one Vladimir Putin. The new Russia proved unable to overcome the culture of corruption that had become so endemic during the old Soviet Union\u2019s Brezhnev era.\n\nI\u2019m hardly qualified to provide a detailed analysis of why it has been so hard for Russia to escape its tragic past, any more than you are likely up for reading such a thing at the end of this already lengthy article. Since moving to Europe in 2009 and continuing to be subjected to my fellow Americans\u2019 blinkered notions of what is \u201cwrong\u201d here and how it should be fixed, I\u2019ve reluctantly concluded that the only way to really know a place may be to live there. So, in lieu of flaunting my ignorance on this subject I\u2019ll just provide a few final anecdotes from my trip across Russia back in 2002.\n\nPerhaps somewhere in the intersection of these anecdotes can be found some clues as to what went wrong with the dreams for a healthy, stable, and free Russia.\n\nToday Putin revels in his role of the comic-book evil mastermind, gobbling up territory here, hacking elections there, scheming always to undermine the existing world order and sometimes seeming to succeed at it to a disconcerting degree. Like most \u201cstrong-man\u201d leaders, he tells himself and his people that he does these things in the name of nationalism and ethnic pride. Yet the would-be strong man fails to understand that by embracing the role of the geopolitical pariah, by running his country as a criminal enterprise with himself at the top of the oligarchial food chain, he actually turns Russia into a weak nation when it could be such a strong one. The largest country in the world has a gross domestic product less than that of South Korea, and 7 percent that of the United States, the nation it so desperately wishes to challenge again on the world stage. Now that the Iron Curtain no longer blocks their way, far too many of the best and the brightest in Russia flee to the West, leaving behind a generation of often hopeless men to literally drink themselves to death; the average life expectancy for a man in Russia is 64 years.\u00a0The country remains what it has been for centuries: the greatest example of wasted potential on earth.\n\nThe storied Moscow Computer Center still exists inside the changed Russia, under the official name of the Dorodnicyn Computing Center of the Russian Academy of Sciences. But even in 2004, when the BBC filmed there for a documentary about Tetris, its luster was yet more faded than it had been during Alexey Pajitnov\u2019s time there. Yuri Yevtushenko, the director of the place at the time, painted a rather grim picture: \u201cOur institute is getting older. The average age of the staff is fifty, and I\u2019m afraid that in ten years if this continues without good young support we will cease to exist.\u201d Working there paid a wage of $200 per month \u2014 hardly much enticement for the next generation of talented young Russian hackers. His analysis of the Computer Center\u2019s future prospects could stand in for those of his country: \u201cIn Russia, the most widespread strategy is the \u2018perhaps\u2019 strategy. It has often saved us. During wars, at the beginning everything looks hopeless. Any other country would probably have been destroyed and died, but Russia somehow finds a way to pull through and survive. I hope it will be the same here too.\u201d\n\nYes, hope must live on. The Putin era too shall pass, and Russia will perhaps in time get another chance to realize its potential.\n\nThe way a narrative history like this one reads has always been a function of where it begins and ends \u2014 what the historian Hayden White calls \u201cemplotment.\u201d Writing history at the wrong time can be intellectually dangerous, as Francis Fukuyama, who has become a walking punch line in the wake of all the history that has transpired since his The End of History, can doubtless well attest. The thing about history, for good and, yes, sometimes for ill, is that it just keeps on happening. Maybe, then, I\u2019ll someday be able to write a less melancholic ending for this tale of the Mirror World. The people of Russia certainly deserve one."},
{"url": "http://slatestarcodex.com/2017/08/07/contra-grant-on-exaggerated-differences/", "link_title": "Contra Grant On Exaggerated Differences", "text": "An article by Adam Grant called Differences Between Men And Women Are Vastly Exaggerated is going viral, thanks in part to a share by Facebook exec Sheryl Sandberg. It\u2019s a response to an email by a Google employee saying that he thought Google\u2019s low female representation wasn\u2019t a result of sexism, but a result of men and women having different interests long before either gender thinks about joining Google. Grant says that gender differences are small and irrelevant to the current issue. I disagree.\n\nThe meta-analysis Grant cites is Hyde\u2019s, available here. I\u2019ve looked into it before, and I don\u2019t think it shows what he wants it to show.\n\nSuppose I wanted to convince you that men and women had physically identical bodies. I run studies on things like number of arms, number of kidneys, size of the pancreas, caliber of the aorta, whether the brain is in the head or the chest, et cetera. 90% of these come back identical \u2013 in fact, the only ones that don\u2019t are a few outliers like \u201cbreast size\u201d or \u201cnumber of penises\u201d. I conclude that men and women are mostly physically similar. I can even make a statistic like \u201cmen and women are physically the same in 78% of traits\u201d.\n\nThen I go back to the person who says women have larger breasts and men are more likely to have penises, and I say \u201cHa, actually studies prove men and women are mostly physically identical! I sure showed you, you sexist!\u201d\n\nI worry that Hyde\u2019s analysis plays the same trick. She does a wonderful job finding that men and women have minimal differences in eg \u201clikelihood of smiling when not being observed\u201d, \u201cinterpersonal leadership style\u201d, et cetera. But if you ask the man on the street \u201cAre men and women different?\u201d, he\u2019s likely to say something like \u201cYeah, men are more aggressive and women are more sensitive\u201d. And in fact, Hyde found that men were indeed definitely more aggressive, and women indeed definitely more sensitive. But throw in a hundred other effects nobody cares about like \u201clikelihood of smiling when not observed\u201d, and you can report that \u201c78% of gender differences are small or zero\u201d.\n\nHyde found moderate or large gender differences in (and here I\u2019m paraphrasing very scientific-sounding constructs into more understandable terms) aggressiveness, horniness, language abilities, mechanical abilities, visuospatial skills, mechanical ability, aggressiveness, tendermindness, assertiveness, comfort with body, various physical abilities, and computer skills.\n\nPerhaps some peeople might think that finding moderate-to-large-differences in mechanical abilities, computer skills, etc supports the idea that gender differences might play a role in gender balance in the tech industry. But because Hyde\u2019s meta-analysis drowns all of this out with stuff about smiling-when-not-observed, Grant is able to make it sound like this proves his point.\n\nIt\u2019s actually worse than this, because Grant misreports the study findings in various ways. For example, he states that the sex differences in physical aggression and physical strength are \u201clarge\u201d. The study very specifically says the opposite of this. Its three different numbers for physical aggression (from three different studies) are 0.4, 0.59, and 0.6, and it sets a cutoff for \u201clarge\u201d effects at 0.75.\n\nOn the other hand, Grant fails to report an effect that actually is large: mechanical reasoning ability (in the paper as Feingold 1998 DAT mechanical reasoning). There is a large gender difference on this, d = 0.76.\n\nAnd although Hyde doesn\u2019t look into it in her meta-analysis, multiple other meta-analyses, like this one, find a large effect size (d = 1.18) for thing-oriented vs. people-oriented interest, the very claim that the argument that Grant is trying to argue against centers around.\n\nSo Grant tries to argue against large thing-oriented vs. people-oriented differences by citing a meta-analysis that doesn\u2019t look into them at all. He then misreports the findings of that meta-analysis, exaggerating effects that fit his thesis and failing to report the ones that don\u2019t. Finally, he cites a \u201csummary statistic\u201d that averages the variation we\u2019re looking for out by combining it with a bunch of noise, and claims the noise proves his point even though the variation is as big as ever.\n\nGrant claims that there are no sex differences in mathematical ability, then goes on to claim that the sex differences in mathematical ability are culturally determined. I\u2019m not really sure what to do with this. Anyway:\n\nSince Grant agrees with me that there is no gender difference in ability, then we should probably redirect ourselves back to the original question: why is there a gender difference in tech-industry-representation? Here his theory that this reflects \u201ccountries that lack gender equity in school enrollment\u201d and \u201cstereotypes associating science with males\u201d fails.\n\nGalpin investigated the percent of women in computer classes all around the world. Her number of 26% for the US is slightly higher than I usually hear, probably because it\u2019s older (the percent women in computing has actually gone down over time!). The least sexist countries I can think of \u2013 Sweden, New Zealand, Canada, etc \u2013 all have somewhere around the same number (30%, 20%, and 24%, respectively). The most sexist countries do extremely well on this metric! The highest numbers on the chart are all from non-Western, non-First-World countries that do middling-to-poor on the Gender Development Index: Thailand with 55%, Guyana with 54%, Malaysia with 51%, Iran with 41%, Zimbabwe with 41%, and Mexico with 39%. Needless to say, Zimbabwe is not exactly famous for its deep commitment to gender equality.\n\nWhy is this? It\u2019s a very common and well-replicated finding that the more progressive and gender-equal a country, the larger gender differences in personality of the sort Hyde found become. I agree this is a very strange finding, but it\u2019s definitely true. See eg Journal of Personality and Social Psychology, Sex Differences In Big Five Personality Traits Across 55 Cultures:\n\nIn case you\u2019re wondering, the countries with the highest gender differences in personality are France, Netherlands, and the Czech Republic. The countries with the lowest sex differences are Indonesia, Fiji, and the Congo.\n\nI conclude that whatever gender-equality-stereotype-related differences Grant has found in the nonexistent math ability difference between men and women, they are more than swamped by the large opposite effects in gender differences in personality. This meshes with what I\u2019ve been saying all along: outside a few exceptions that don\u2019t matter for the current discussion, it\u2019s not about ability, it\u2019s about interest.\n\nI highly recommend Freddie deBoer\u2019s Why Selection Bias Is The Most Powerful Force In Education. If an educational program shows amazing results, and there\u2019s any possible way it\u2019s selection bias, it\u2019s selection bias.\n\nI looked into Harvey Mudd\u2019s STEM admission numbers, and, sure enough, they admits women at 2.5x the rate as men. So, yeah, it\u2019s selection bias.\n\nI don\u2019t blame them. All they have to do is cultivate a reputation as a place to go if you\u2019re a woman interested in computer science, attract lots of female CS applicants, then make sure to admit all the CS-interested female applicants they get. In exchange, they get constant glowing praise from every newspaper in the country (1, 2,\n\n 3, 4, 5, 6, 7, 8, 9, 10, etc, etc, etc).\n\nHow would we know this was selection bias if we couldn\u2019t just look at the numbers? The graph that Grant himself cites just above this statement shows that, over the same ten year period, percent women CS graduates has declined nationwide. This has corresponded with such a massive push to get more women in tech that\u2026well, that a college which succeeds will get constant glowing praise from every newspaper in the country even when they admit they\u2019re using selection bias. Do you think no one else has tried? Every college diversity office in the country is working overtime to try to get more women into tech, there are women in tech scholarships, women in tech conferences, women in tech prizes \u2013 and, over the period that\u2019s happened, Grant\u2019s own graph shows the percent of women in tech going down.\n\n(I don\u2019t understand why it\u2019s going down, but my guess is a combination of constant messaging that there are no women in tech making women think it isn\u2019t for them, plus the effect from society getting more gender-equitable that we described in Part II \u2013 ie we\u2019re now less like Zimbabwe, and so we can\u2019t expect our gender ratios to be as good as theirs are).\n\nLook. If I recruit only gingers, and I admit only gingers, I can get a 100% ginger CS program. That doesn\u2019t mean I\u2019ve proven that gingers are really more interested in CS than everyone else, and it was just discrimination holding them back. It means I\u2019ve done what every single private school and college does anyway, all the time \u2013 finagle with admissions to make myself look good.\n\nBefore we discuss this, a quick step back.\n\nIn the year 1850, women were locked out of almost every major field, with a few exceptions like nursing and teaching. The average man of the day would have been equally confident that women were unfit for law, unfit for medicine, unfit for mathematics, unfit for linguistics, unfit for engineering, unfit for journalism, unfit for psychology, and unfit for biology. He would have had various sexist justifications \u2013 women shouldn\u2019t be in law because it\u2019s too competitive and high-pressure; women shouldn\u2019t be in medicine because they\u2019re fragile and will faint at the sight of blood; et cetera.\n\nAs the feminist movement gradually took hold, women conquered one of these fields after another. 51% of law students are now female. So are 49.8% of medical students, 45% of math majors, 60% of linguistics majors, 60% of journalism majors, 75% of psychology majors, and 60% of biology postdocs. Yet for some reason, engineering remains only about 20% female.\n\nAnd everyone says \u201cAha! I bet it\u2019s because of negative stereotypes!\u201d\n\nThis makes no sense. There were negative stereotypes about everything! Somebody has to explain why the equal and greater negative stereotypes against women in law, medicine, etc were completely powerless, yet for some reason the negative stereotypes in engineering were the ones that took hold and prevented women from succeeding there.\n\nAnd if your answer is just going to be that apparently the negative stereotypes in engineering were stronger than the negative stereotypes about everything else, why would that be? Put yourself in the shoes of our Victorian sexist, trying to maintain his male privilege. He thinks to himself \u201cWell, I suppose I could tolerate women doctors saving my life. And if I had to, I would accept women going into math and learning the secrets of the Universe itself. I\u2019m even sort of okay with women going into journalism and crafting the narratives that shape our world. But women building bridges? NO MERE FEMALE COULD EVER DO SUCH A THING!\u201d Really? This is the best explanation the world can come up with? Doesn\u2019t anyone have at least a little bit of curiousity about this?\n\nWhenever I ask this question, I get something like \u201cengineering and computer science are two of the highest-paying, highest-status jobs, so of course men would try to keep women out of them, in order to maintain their supremacy\u201d. But I notice that doctors and lawyers are also pretty high-paying, high-status jobs, and that nothing of the sort happened there. And that when people aren\u2019t using engineering/programming\u2019s high status to justify their beliefs about gender stereotypes in it, they\u2019re ruthlessly making fun of engineers and programmers, whether it\u2019s watching Big Bang Theory or reading Dilbert or just going on about \u201cpocket protectors\u201d.\n\nMeanwhile, men make up only 10% of nurses, only 20% of new veterinarians, only 25% of new psychologists, about 25% of new paediatricians, about 26% of forensic scientists, about 28% of medical managers, and 42% of new biologists.\n\nNote that many of these imbalances are even more lopsided than the imbalance favoring men in technology, and that many of these jobs earn much more than the average programmer. For example, the average computer programmer only makes about $80,000; the average veterinarian makes about $88,000, and the average pediatrician makes a whopping $170,000.\n\nAs long as you\u2019re comparing some poor woman janitor to a male programmer making $80,000, you can talk about how it\u2019s clearly sexism against women getting the good jobs. But once you take off the blinders and try to look at an even slightly bigger picture, you start wondering why veterinarians, who make even more money than that, are even more lopsidedly female than programmers are male. And then you start thinking that maybe you need some framework more sophisticated than the simple sexism theory in order to predict who\u2019s doing all of these different jobs. And once you have that framework, maybe the sexism theory isn\u2019t necessary any longer, and you can throw it out, and use the same theory to predict why women dominate veterinary medicine and psychology, why men dominate engineering and computer science, and why none of this has any relation at all to what fields that some sexist in the 1850s wanted to keep women out of.\n\nSo let\u2019s look deeper into what prevents women from entering these STEM fields.\n\nDoes it happen at the college level? About 20% of high school students taking AP Computer Science are women. The ratio of women graduating from college with computer science degrees is exactly what you would expect from the ratio of women who showed interest in it in high school (the numbers are even lower in Britain, where 8% of high school computer students are girls. So differences exist before the college level, and nothing that happens at the college level \u2013 no discriminatory professors, no sexist classmates \u2013 change the numbers at all.\n\nDoes it happen at the high school level? There\u2019s not a lot of obvious room for discrimination \u2013 AP classes are voluntary; students who want to go into them do, and students who don\u2019t want to go into them don\u2019t. There are no prerequisites except basic mathematical competency or other open-access courses. It seems like of the people who voluntarily choose to take AP classes that nobody can stop them from going into, 80% are men and 20% are women, which exactly matches the ratio of each gender that eventually get tech company jobs.\n\nRather than go through every step individually, I\u2019ll skip to the punch and point out that the same pattern repeats in middle school, elementary school, and about as young as anybody has ever bothered checking. So something produces these differences very early on? What might that be?\n\nMight young women be avoiding computers because they\u2019ve absorbed stereotypes telling them that they\u2019re not smart enough, or that they\u2019re \u201conly for boys\u201d? No. As per Shashaani 1997, \u201c[undergraduate] females strongly agreed with the statement \u2018females have as much ability as males when learning to use computers\u2019, and strongly disagreed with the statement \u2018studying about computers is more important for men than for women\u2019. On a scale of 1-5, where 5 represents complete certainty in gender equality in computer skills, and 1 completely certainty in inequality, the average woman chooses 4.2; the average male 4.03. This seems to have been true since the very beginning of the age of personal computers: Smith 1986 finds that \u201cthere were no significant differences between males and females in their attitudes of efficacy or sense of confidence in ability to use the computer, contrary to expectation\u2026females [showed] stronger beliefs in equity of ability and competencies in use of the computer.\u201d This is a very consistent result and you can find other studies corroborating it in the bibliographies of both papers.\n\nMight girls be worried not by stereotypes about computers themselves, but by stereotypes that girls are bad at math and so can\u2019t succeed in the math-heavy world of computer science? No. About 45% of college math majors are women, compared to (again) only 20% of computer science majors. Undergraduate mathematics itself more-or-less shows gender parity. This can\u2019t be an explanation for the computer results.\n\nMight sexist parents be buying computers for their sons but not their daughters, giving boys a leg up in learning computer skills? In the 80s and 90s, everybody was certain that this was the cause of the gap. Newspapers would tell lurid (and entirely hypothetical) stories of girls sitting down to use a computer when suddenly a boy would show up, push her away, and demand it all to himself. But move forward a few decades and now young girls are more likely to own computers than young boys \u2013 with zero little change in the high school computer interest numbers. So that isn\u2019t it either.\n\nSo if it happens before middle school, and it\u2019s not stereotypes, what might it be?\n\nOne subgroup of women does not display these gender differences at any age. These are women with congenital adrenal hyperplasia, a condition that gives them a more typically-male hormone balance. For a good review, see Gendered Occupational Interests: Prenatal Androgen Effects on Psychological Orientation to Things Versus People. They find that:\n\nIn their own study, they compare 125 such women and find a Things-People effect size of -0.75 \u2013 that is, the difference between CAH women and unaffected women is more than half the difference between men and unaffected women. They write:\n\nWhat is this \u201cobject vs. people\u201d distinction?\n\nIt\u2019s pretty relevant. Meta-analyses have shown a very large (d = 1.18) difference in healthy men and women (ie without CAH) in this domain. It\u2019s traditionally summarized as \u201cmen are more interested in things and women are more interested in people\u201d. I would flesh out \u201cthings\u201d to include both physical objects like machines as well as complex abstract systems; I\u2019d also add in another finding from those same studies that men are more risk-taking and like danger. And I would flesh out \u201cpeople\u201d to include communities, talking, helping, children, and animals.\n\nSo this theory predicts that men will be more likely to choose jobs with objects, machines, systems, and danger; women will be more likely to choose jobs with people, talking, helping, children, and animals.\n\nSomebody armed with this theory could pretty well pretty well predict that women would do well in medicine and law, since both of them involve people, talking, and helping. They would predict that women would dominate veterinary medicine (animals, helping), psychology (people, talking, helping, sometimes children), and education (people, children, helping). Of all the hard sciences, they might expect women to prefer biology (animals). And they might expect men to do best in engineering (objects, machines, abstract systems, sometimes danger) and computer science (machines, abstract systems).\n\nI mentioned that about 50% of medical students were female, but this masks a lot of variation. There are wide differences in doctor gender by medical specialty. For example:\n\nA privilege-based theory fails \u2013 there\u2019s not much of a tendency for women to be restricted to less prestigious and lower-paying fields \u2013 Ob/Gyn (mostly female) is extremely lucrative, and internal medicine (mostly male) is pretty low-paying for a medical job.\n\nBut the people/thing theory above does extremely well! Ob/Gyn is babies, Pediatrics is babies/children, Psychiatry is people/talking (and of course women are disproportionately child psychiatrists), and family medicine is people/talking/babies/children.\n\nMeanwhile, Radiology is machines and no patient contact, Anaesthesiology is also machines and no patient contact, Emergency Medicine is danger, and Surgery is machines, danger, and no patient contact.\n\nThe only one that doesn\u2019t quite fit the theory is Internal Medicine, but that\u2019s not far from 50-50 anyway.\n\nI\u2019m not familiar with any gender breakdown of legal specialties, but I will bet you that family law, child-related law, and various prosocial helping-communities law are disproportionately female, and patent law, technology law, and law working with scary dangerous criminals are disproportionately male.\n\nThis theory gives everyone what they want. It explains the data about women in tech. It explains the time course around women in tech. It explains other jobs like veterinary medicine where women dominate. It explains which medical subspecialties women will be dominant or underrepresented in. It doesn\u2019t claim that women are \u201cworse than men\u201d or \u201cbiologically inferior\u201d at anything. It doesn\u2019t say that no woman will ever be interested in things, or no man ever interested in people. It doesn\u2019t say even that women in tech don\u2019t face a lot of extra harassment (any domain with more men than women will see more potential perpetrators concentrating their harassment concentrated on fewer potential victims, which will result in each woman being more harassed).\n\nIt just says that sometimes, in a population-based way that doesn\u2019t necessarily apply to any given woman or any given man, women and men will have some different interests. Which should be pretty obvious to anyone who\u2019s spent more than a few minutes with men or women.\n\nWhy am I writing this?\n\nGrant\u2019s piece was in response to a person at Google sending out a memo claiming some of this stuff. Here is a pretty typical response that a Googler sent to that memo \u2013 I\u2019ve blocked the name so this person doesn\u2019t get harassed over it, but if you doubt this is real I can direct you to the original:\n\nA lot of people without connections to the tech industry don\u2019t realize how bad it\u2019s gotten. This is how bad. It would be pointless trying to do anything about this person in particular. This is the climate.\n\nSilicon Valley was supposed to be better than this. It was supposed to be the life of the mind, where people who were interested in the mysteries of computation and cognition could get together and make the world better for everybody. Now it\u2019s degenerated into this giant hatefest of everybody writing long screeds calling everyone else Nazis and demanding violence against them. Where if someone disagrees with the consensus, it\u2019s just taken as a matter of course that we need to hunt them down, deny them of the cloak of anonymity, fire them, and blacklist them so they can never get a job again. Where the idea that we shouldn\u2019t be a surveillance society where we carefully watch our coworkers for signs of sexism so we can report them to the authorities is exactly the sort of thing you get reported to the authorities if people see you saying.\n\nOn the Twitter debate on this, someone mentioned that people felt afraid to share their thoughts anymore. An official, blue-checkmarked Woman In Tech activist responded with (note the 500+ likes):\n\nThis is the world we\u2019ve built. Where making people live in fear is a feature, not a bug.\n\nAnd trust me: it\u2019s going to get worse. If you only read one link, let it be this one about the young adult publishing industry A sample quote:\n\nParts of tech are already this bad. For the rest of you: it\u2019s what you have to look forward to.\n\nIt doesn\u2019t have to be this way. Nobody has any real policy disagreements. Everyone can just agree that men and women are equal, that they both have the same rights, that nobody should face harassment or discrimination. We can relax the Permanent State Of Emergency around too few women in tech, and admit that women have the right to go into whatever field they want, and that if they want to go off and be 80% of veterinarians and 74% of forensic scientists, those careers seem good too. We can appreciate the contributions of existing women in tech, make sure the door is open for any new ones who want to join, and start treating each other as human beings again. Your co-worker could just be your co-worker, not a potential Nazi to be assaulted or a potential Stalinist who\u2019s going to rat on you. Your project manager could just be your project manager, not the person tasked with monitoring you for signs of evil to be rooted out. Your female co-worker could just be your female co-worker, not a Strong Grrl Coder Who Has Overcome Adversity And Is A Symbol Of Everything Good In The World. Your male co-worker could just be your male co-worker, not a Tool Of The Patriarchy Who Is Keeping Everyone More Talented Down. I promise there are industries like this. Medicine is like this! Loads of things are like this! This could be you.\n\nAdam Grant seems like a good person. He is superficially doing everything right. He\u2019s not demanding people feel afraid, or saying that everyone who disagrees with him is a fascist. He\u2019s just trying to argue the science.\n\nBut I think he\u2019s very wrong about the science. I think Hyde\u2019s article is a gimmick which buries very real differences under a heap of meaningless similarities. I think that it\u2019s inappropriate to cite it to respond to claims of specific differences that it didn\u2019t investigate. I think that claims of a gender-equitable-society-effect in a different domain are inappropriate given the clear opposite effect in the domain being talked about it. I think it\u2019s wrong to privilege likely-selection-biased evidence from a single college over all the evidence from the country as a whole. I think it\u2019s wrong to suppose unique stereotypes in tech and engineering domains with no theory of how they got there, when there are non-stereotype-based theories that better explain the evidence. And I think it\u2019s wrong to ignore all the studies about congenital adrenal hyperplasia.\n\nAnd I think that, in being wrong about the science, he\u2019s (probably unintentionally) giving aid and comfort to the people who have admitted that turning tech into a climate of constant fear and violence is the end goal.\n\nGrant is one of the few people doing the virtuous thing and trying to debate this without calling for other people\u2019s deaths. I\u2019m trying to do the virtuous thing and respond to him. But I worry that lots of people on Grant\u2019s side aren\u2019t as virtuous as he is, and I don\u2019t know how to protect anybody from that except by begging people to please look at the science and try to get it right."},
{"url": "https://www.nytimes.com/2017/08/06/health/prescription-drugs-brand-name-generic.html", "link_title": "Some insurers insist that patients forgo generics and buy brand-name drugs", "text": "Consumers have grown accustomed to being told by insurers \u2014 and middlemen known as pharmacy benefit managers \u2014 that they must give up their brand-name drugs in favor of cheaper generics. But some are finding the opposite is true, as pharmaceutical companies squeeze the last profits from products that are facing cheaper generic competition.\n\nOut of public view, corporations are cutting deals that give consumers little choice but to buy brand-name drugs \u2014 and sometimes pay more at the pharmacy counter than they would for generics.\n\nThe practice is not easy to track, and has been going on sporadically for years. But several clues suggest it is becoming more common.\n\nIn recent months, some insurers and benefit managers have insisted that patients forgo generics and buy brand-name drugs such as the cholesterol treatment Zetia, the stroke-prevention drug Aggrenox and the pain-relieving gel Voltaren, along with about a dozen others, according to memos and prescription drug claims that pharmacies shared with ProPublica and The New York Times. At the same time, consumers are sounding off on social media.\n\nNow it appears the practice is spreading to biosimilars, the competitors for expensive, complex biologic drugs that are beginning to arrive on the market.\n\nConsumers have become increasingly angry over what they pay for drugs, and that outrage has caught the attention of lawmakers from both parties. Democrats have identified lowering drug prices as a pillar of their economic agenda, and President Trump has raised the issue repeatedly. But for now, solutions have proved elusive.\n\nThe continued success of the brand-name drug Adderall XR, long after generic competitors arrived on the market, is a case in point.\n\nDr. Lawrence Diller, a behavioral pediatrician in Walnut Creek, Calif., said he began noticing \u201cvery odd things\u201d going on with Adderall XR and other attention-deficit drugs about two years ago. He began receiving faxes from pharmacies telling him that he had to specify that patients required brand-name versions of the drugs.\n\nHe had been practicing for 40 years, but until then had never had a pharmacy tell him that he had to prescribe a brand-name drug instead of a generic.\n\n\u201cIt\u2019s Alice-in-Wonderland time in the drug world,\u201d he said.\n\nSome insurers require members to have prescriptions filled with brand-name drugs and do not charge them more than for generics. But 29 percent of Americans with health insurance paid for by their employer have a high-deductible insurance plan. They acutely feel the cost difference between branded and generic drugs because they often have to pick up the full sticker price of medications until they have paid out thousands of dollars.\n\nNaomi Freundlich, a Brooklyn writer, had been buying the generic version of Adderall XR for two years to treat her son\u2019s attention-deficit hyperactivity disorder. Her family had a $3,000 annual deductible, and the relatively lower price helped keep medical costs down.\n\nThen, in 2014, her pharmacist told her that her insurance plan would cover only the brand-name drug, which cost her family some $50 more a month than the generic. If she paid for the generic herself, it would not have counted toward her deductible. Ms. Freundlich complained to her insurer, UnitedHealthcare, but could not get a clear answer.\n\n\u201cIt\u2019s hard to explain because it doesn\u2019t really make sense,\u201d she said.\n\nUnitedHealthcare has continued to favor Adderall XR and certain other brand-name drugs over generics, according to claims provided by independent pharmacists and reviewed by ProPublica and The Times. The insurer also recently told health providers that it preferred Remicade, the expensive rheumatoid arthritis drug made by Johnson & Johnson, over biosimilars that have a lower list price and are just beginning to come on the market.\n\nA spokesman for UnitedHealthcare, Matthew N. Wiggin, said the insurer does at times prefer brand-name drugs. \u201cBy providing access to these drugs at a lower cost, we are able to improve affordability for our customers and members,\u201d he said in an email.\n\nAsked whether consumers sometimes ended up paying more because of these choices, he said pharmacies and doctors could seek an exemption from the insurer if they wanted the generic instead. Several patients said they had not been told of that option.\n\nShire, the maker of Adderall XR, and some other brand-name drug manufacturers are no longer content to allow sales of their products to plummet when generic competitors arrive on the market. Instead, they are negotiating deals with insurers and pharmacy benefit managers to give priority to their versions. Consumers are given no details about these deals.\n\nA Shire spokeswoman said the company had been able to hold on to market share for Adderall XR by offering insurers and government programs prices that are competitive with those of generic manufacturers.\n\nAdderall XR, the long-acting version of Shire\u2019s popular treatment Adderall, had for years been the company\u2019s top-selling product, bringing in $1.1 billion in sales in 2008, about one-third of its revenue that year.\n\nBut mindful that its blockbuster could soon face generic competition, Shire acted aggressively to protect its franchise.\n\nFirst, in the mid-2000s, Shire sued generic drug companies to block them from bringing cheaper copies to the market, alleging patent infringement. Then, it made deals with two makers of generic drugs to sell authorized copies of its drug, a tactic in which the branded manufacturer supplies its product in exchange for a share of royalties. Those agreements soured after the two companies, Teva Pharmaceuticals and Impax Laboratories, accused Shire of not playing fair by failing to supply them with enough pills to compete in the marketplace. More lawsuits ensued, followed by settlements.\n\nThen, a few years ago, Shire tried a new tactic: giving ever-larger discounts to pharmacy benefit managers and insurers for preferential treatment over the generics. That did not mean lowering the list price of the drug, but rather negotiating rebates that were paid not to the patients but to insurers and middlemen such as CVS Caremark.\n\nBenefit managers and insurers have been passionate advocates of generic drugs, arguing that the cheaper products save patients and their employers billions of dollars. Indeed, generic drugs have come to dominate the market, and today account for nearly 90 percent of all prescriptions filled in the United States.\n\nShire has managed to hold on to a much larger share of the market through its deals than most companies do when their drugs come off patent and face generic competition.\n\nAdderall XR, the brand-name version of extended-release mixed amphetamine salts, accounted for 29 percent of the 13.1 million prescriptions for the drug in 2016, according to QuintilesIMS, a health information company that purchases the data from pharmacies and sells it to clients that include drug companies. The average market share of brand-name products dwindles to less than 6 percent two years after the first generic competitor arrives, according to QuintilesIMS.\n\nThe list price of Adderall XR has remained $7.12 per pill since mid-2012. But according to data from SSR Health, a research firm that tracks drug prices, the portion that Shire keeps has steadily declined.\n\nIn the first quarter of 2017, SSR estimated that Shire kept only $1.73, down from $2.93 per pill in the first quarter of 2013. Shire does not break out how much it pays to each middleman in the system, from distributors to pharmacy benefit managers.\n\nBut Ryan Baum, an analyst at SSR Health, said it was clear that Shire\u2019s declining share of the list price reflected \u201cjust a really aggressive instance of trying to hang on.\u201d\n\nIn contrast, the generics cost as low as $3.89 per pill, but that does not include unspecified concessions that generic makers offer to pharmacies and distributors, according to Truven Health Analytics, another research firm that tracks the prices wholesalers pay for drugs.\n\nA spokeswoman for Shire, Gwendolyn Fisher, said that while Shire did not make decisions about how much patients paid in out-of-pocket costs, \u201cShire is helping to deliver cost savings to the system and greater patient access to an important medicine.\u201d\n\nShire said last week that it was considering spinning off the portion of its business that sells attention-deficit drugs in order to focus on developing rare-disease treatments.\n\nGeneric drug makers say they have seen an increase in efforts by manufacturers of brand-name drugs to fight to retain sales after they lose patent protection.\n\n\u201cYou definitely see a much more aggressive posture than you used to see,\u201d said Christine Baeder, senior vice president for customer and marketing operations at Teva, the world\u2019s largest generic drug manufacturer.\n\nIn December, CVS Caremark, one of the largest benefit managers, sent a memo to pharmacies informing them that some of its Medicare prescription drug plans would cover only brand-name versions of 12 drugs. Some of the drugs, such as the antipsychotic medication Invega, have had generic competitors for over a year.\n\nAlso on the list was Copaxone, a brand-name drug sold by Teva that treats multiple sclerosis and that recently lost patent protection on its daily injection. Though Teva primarily makes generic drugs, in a twist it has taken a page from brand-name manufacturers to preserve sales of one of its key products.\n\nIn a statement, Teva said many patients had moved to its three-times-weekly version of Copaxone, for which there is no generic, but said it wanted to ensure that patients who \u201cwish to remain on therapy continue to have access.\u201d\n\nConsumers taking other medications said they had experienced the same phenomenon. Lisa Hopkins, a disabled food and nutrition supervisor in Pennsylvania, went to fill a prescription for the anti-inflammatory Voltaren gel this year.\n\nMs. Hopkins, 52, said her pharmacist had told her that her drug plan, CVS\u2019s SilverScript, denied her claim because it was for a generic.\n\n\u201cI said to the lady at the insurance company, \u2018That\u2019s really, really odd to me,\u2019\u201d Ms. Hopkins said. \u201cShe said: \u2018Yes. It\u2019s happening more and more that the name brand is covered but the generic isn\u2019t.\u2019\u201d\n\nMs. Hopkins has osteoporosis and bulging spinal disks and has been on disability for almost a decade. She is covered through Medicare and receives extra help from the government for her medications, lowering her out-of-pocket costs. That means that when her drugs cost a lot, taxpayers pay the bill. By law, Medicare cannot negotiate directly with drug manufacturers and instead gets a share of any rebates collected by insurers and benefit managers, like CVS Caremark, which operate Medicare\u2019s drug plans.\n\nIn an email, a spokeswoman for CVS Caremark, Christine Cramer, said consumers never pay more in the rare instances in which the company favors a brand-name drug over a generic. \u201cThis generally occurs when there is limited or no competition among generics,\u201d she said.\n\nPharmacists say they are noticing the trend, too, and it takes time to understand the denied claim and pursue a remedy, including sometimes calling the doctor. While favorable treatment for a brand-name drug doesn\u2019t happen all the time, it is startling when it does, said Robert Frankil, president of Sellersville Pharmacy Inc. in Pennsylvania, which owns two pharmacies.\n\n\u201cThere\u2019s only one reason why they\u2019re requiring you to use a more expensive product,\u201d Mr. Frankil said. \u201cBecause somewhere down the road, somebody is earning more money.\u201d"},
{"url": "https://send.firefox.com/", "link_title": "Firefox Send \u2013 Private, Encrypted File Sharing", "text": "Please enable JavaScript and try again."},
{"url": "https://www.recode.net/2017/8/7/16110696/firing-google-ceo-employee-penned-controversial-memo-on-women-has-violated-its-code-of-conduct?utm_campaign=karaswisher&utm_content=chorus&utm_medium=social&utm_source=twitter", "link_title": "Firing expected of Google employee who penned controversial memo on women", "text": "In a memo to employees, Google CEO Sundar Pichai said the employee who penned a controversial memo that claimed that women had biological issues that prevented them from being as successful as men in tech had violated its Code of Conduct, and that the post had crossed \u201cthe line by advancing harmful gender stereotypes in our workplace.\u201d\n\nHe added: \u201cTo suggest a group of our colleagues have traits that make them less biologically suited to that work is offensive and not OK.\u201d\n\nPichai\u2019s wording appears to indicate that the employee is likely be fired, which some inside and outside the company have been calling for. A Google spokesperson said the company would not confirm any firing of an individual employee, but others have been let go for violating its Code of Conduct in the past.\n\nOnce it does happen \u2014 and it should not be long \u2014 the move is sure to attract a firestorm of criticism on both sides, putting the search giant in the crosshairs of a wider debate about gender issues taking place in Silicon Valley and across the country.\n\nThe employee memo \u2014 which was up for days without action by Google \u2014 went viral within the search giant\u2019s internal discussion boards this weekend, with some decrying it and others defending it. Sources said the company\u2019s top execs have been struggling with how to deal with it and the fallout, trying to decide if its troubling content crossed a line.\n\nApparently it did. In a memo to employees titled \u201cOur words matter,\u201d Google CEO Sundar Pichai said that the employee \u2014 who has been named on Twitter, although his identity could not be verified \u2014 had violated its code of conduct. (I am not publishing his name, because he \u2014 and others who disagree with him \u2014 have been threatened with violence online.)\n\nHad the employee not belittled women\u2019s skills, I assume, he would not be let go, but he made claims that many consider problematic, although others maintain that his myriad of claims are worthy.\n\nOne thing is clear, the memo has become radioactive at Google.\n\nMultiple sources said the memo has caused a massive debate to go on internally, which has devolved in ways not unlike those taking place across the country. \u201cIt has been really toxic,\u201d said one person at Google. \u201cIt\u2019s a microcosm of America.\u201d\n\nStill, this is a corporation with rules and managers who rule on those rules. So, what is also true is that most free speech is allowed when it comes to the government and within society, but not necessarily within companies. In, fact, it is common for people to lose their jobs for making sexist and racist remarks.\n\nThat said, Pichai also noted that the memo did raise some important issues, such as the need for more willingness at Google to include more points of view at the company, including more conservative ones.\n\nIt\u2019s really a no-win situation for him or anyone, as these issues engender really profound and often ugly disagreement to take place.\n\n\u201cFirst, let me say that we strongly support the right of Googlers to express themselves, and much of what was in that memo is fair to debate, regardless of whether a vast majority of Googlers disagree with it. However, portions of the memo violate our Code of Conduct and cross the line by advancing harmful gender stereotypes in our workplace. Our job is to build great products for users that make a difference in their lives. To suggest a group of our colleagues have traits that make them less biologically suited to that work is offensive and not OK. It is contrary to our basic values and our Code of Conduct, which expects \u2018each Googler to do their utmost to create a workplace culture that is free of harassment, intimidation, bias and unlawful discrimination.\u2019\u201d\n\nOn Sunday, Google\u2019s head of diversity, Danielle Brown, said in a memo \u2014 her first to the company \u2014 that she would not link to the employee\u2019s memo because \u201cit's not a viewpoint that I or this company endorses, promotes or encourages.\u201d\n\nGoogle does not have an easy line to walk, especially since the employee penned a piece he sent across the company that posited, among other things, that women were biologically not suited to do tech.\n\nTitled \u201cGoogle\u2019s Ideological Echo Chamber,\u201d it begins promisingly enough (and is, for the most part, well-written):\n\n\u201cI value diversity and inclusion, am not denying that sexism exists, and don\u2019t endorse using stereotypes. When addressing the gap in representation in the population, we need to look at population level differences in distributions. If we can't have an honest discussion about this, then we can never truly solve the problem.\u201d\n\nBut then, in what is pretty much the main premise, he went on in detail: \u201cI\u2019m simply stating that the distribution of preferences and abilities of men and women differ in part due to biological causes and that these differences may explain why we don\u2019t see equal representation of women in tech and leadership.\u201d\n\nWhat followed was a list of those differences, including a claim that women were more social and artistic and could not take the stress of high-pressure jobs. Hence, neuroticism, or higher anxiety and lower stress tolerance, which he claimed was backed up by studies.\n\nPerhaps most disingenuously, the author also claimed that he had no voice, even after penning a 3,000-word memo that he was able to send companywide and also was read by millions more.\n\nIn other words, he got heard.\n\n\u201cPsychological safety is built on mutual respect and acceptance, but unfortunately our culture of shaming and misrepresentation is disrespectful and unaccepting of anyone outside its echo chamber,\u201d he wrote.\n\nWell, maybe so, but it also looks like it also will lead to more serious consequences for the employee.\n\nIronically, Google is now hosting a conference on girls in tech\n\nIt is also in the midst of a lawsuit with the Labor Department, which has alleged that Google has a gender gap in pay. The company has denied this, and has declined to provide salary information to the government. But Google, like many tech companies has released its diversity statistics \u2014 men make up almost 70 percent of the staff and a full 80 percent of the technical employees.\n\nHere is the Pichai memo in total \u2014 if you want to also read between the lines:\n\nThis has been a very difficult few days. I wanted to provide an update on the memo that was circulated over this past week. First, let me say that we strongly support the right of Googlers to express themselves, and much of what was in that memo is fair to debate, regardless of whether a vast majority of Googlers disagree with it. However, portions of the memo violate our Code of Conduct and cross the line by advancing harmful gender stereotypes in our workplace. Our job is to build great products for users that make a difference in their lives. To suggest a group of our colleagues have traits that make them less biologically suited to that work is offensive and not OK. It is contrary to our basic values and our Code of Conduct, which expects \u201ceach Googler to do their utmost to create a workplace culture that is free of harassment, intimidation, bias and unlawful discrimination.\u201d The memo has clearly impacted our co-workers, some of whom are hurting and feel judged based on their gender. Our co-workers shouldn\u2019t have to worry that each time they open their mouths to speak in a meeting, they have to prove that they are not like the memo states, being \u201cagreeable\u201d rather than \u201cassertive,\u201d showing a \u201clower stress tolerance,\u201d or being \u201cneurotic.\u201d At the same time, there are co-workers who are questioning whether they can safely express their views in the workplace (especially those with a minority viewpoint). They too feel under threat, and that is also not OK. People must feel free to express dissent. So to be clear again, many points raised in the memo \u2014 such as the portions criticizing Google\u2019s trainings, questioning the role of ideology in the workplace, and debating whether programs for women and underserved groups are sufficiently open to all \u2014 are important topics. The author had a right to express their views on those topics \u2014 we encourage an environment in which people can do this and it remains our policy to not take action against anyone for prompting these discussions. The past few days have been very difficult for many at the company, and we need to find a way to debate issues on which we might disagree \u2014 while doing so in line with our Code of Conduct. I\u2019d encourage each of you to make an effort over the coming days to reach out to those who might have different perspectives from your own. I will be doing the same. I have been on work related travel in Africa and Europe the past couple of weeks and had just started my family vacation here this week. I have decided to return tomorrow as clearly there\u2019s a lot more to discuss as a group \u2014 including how we create a more inclusive environment for all. So please join me, along with members of the leadership team at a town hall on Thursday. Check your calendar soon for details."},
{"url": "https://www.inc.com/sonya-mann/google-manifesto-blacklists.html", "link_title": "Some Google managers maintain personal blacklists", "text": "On Friday night, Vice's Motherboard reported that a controversial internal memo written by a concerned Google employee was going viral within the company. The memo, titled \"PC Considered Harmful\" and since dubbed \"the Google manifesto\" on social media, argued two points: First, that Google has become an ideological echo chamber where anyone with centrist or right-of-center views fears to speak their mind. Second, that part of the tech industry's gender gap can be attributed to biological differences between men and women.\n\nThis news caused an immediate and lasting uproar, both within Google and on public discussion forums like Twitter. The dismay and outrage -- and then the inevitable counter-outrage in response to the initial outrage -- heated up further when Gizmodo released the full text of the open letter. Critics have primarily focused on author James Damore's implication that women are less prevalent in software engineering and leadership roles due to the unequal distribution of innate characteristics like spatial reasoning and neuroticism. Update: Damore has since been fired, Bloomberg reported.\n\nWithin Google, a few sympathetic employees were dismayed to see Damore so vehemently criticized by their colleagues. In a poll distributed on a mailing list dedicated to discussing the manifesto, opinion broke down differently than it did in non-anonymous internal Google Plus posts:\n\nThe contentious internal discussion revived a concern dating back to 2015: An unknown number of Google managers keep blacklists of employees whom they say they will not work with. The blacklists are based factors including personal experience of others' behavior and views expressed on politics, social justice issues and Google's diversity efforts. Inc. reviewed screenshots documenting several managers attesting to this practice, both in the past and currently. The screenshots were shared by a Google employee who requested anonymity due to having signed an NDA. In the screenshots, one employee declared his intent to quit if the manifesto's author were not fired, and another said he would refuse to work with the manifesto's author in any capacity.\n\nA Google spokesperson told Inc. that the practice of keeping blacklists is not condoned by upper management, and that Google employees who discriminate against members of protected classes will be terminated. But it's far from clear whether that applies here. Although political affiliation is a protected class according to California labor law, the views expressed by the manifesto author and others who oppose political correctness do not seem to merit some protection. (Indeed, Google's decision to fire Damore suggests the company concluded they don't, although its slowness in acting suggests it was not an easy call.)\n\nDamore's manifesto wasn't classic political speech, said Harmeet Kaur Dhillon, an experienced business and labor lawyer. Rather, it's what she labeled \"controversial speech.\" She added, \"These are not insane views that he's extolling here -- they're [just] out of the mainstream. He's entitled to hold views that are inconsistent with the mainstream.\"\n\nAs to whether Google is free to fire the manifesto author and those who voice agreement -- or even has an obligation to do so -- \"the question is whether he's acting on those views in a way that violates discrimination law.\" Dhillon noted that although California has stronger labor protections than most other states, \"The cases involving political speech are much more cut-and-dried,\" involving conventional political activities like voting for a candidate or running for office.\n\nEditor's note: This article has been updated to reflect the fact that Damore has been publicly identified as the manifesto's author and fired from Google."},
{"url": "https://www.fastcompany.com/40449815/london-is-using-optical-illusions-to-make-cars-slow-down", "link_title": "London is using optical illusions to make cars slow down", "text": "London has implemented an interesting idea to\u00a0curb\u00a0speeding: magic. The British capital has painted optical illusions on its\u00a0streets as part of a pilot program to get drivers to slow down, according to podcast 99% Invisible. The idea is both simple and clever: Paint the streets to look like they have speed bumps on them, but don\u2019t use finite city resources to actually build speed bumps into the road.\u00a0The 18-month pilot program was launched in September of last year, according to the BBC, and the city is still determining whether the black-and-white stencils are as effective as actual bumps to deter drivers from exceeding 20 mph (as if traffic in London ever goes faster than 20 mph)."},
{"url": "http://www.sltrib.com/news/politics/2017/08/06/sl-co-mayor-ben-mcadams-posed-as-a-homeless-person-for-3-days-and-2-nights-heres-what-he-saw/", "link_title": "Mayor Ben McAdams posed as a homeless person for 3 days and 2 nights", "text": "Rick Egan | The Salt Lake Tribune Homeless campers are forced to remove their belonging from the Rio Grande Area, as the S... Rick Egan | The Salt Lake Tribune Homeless campers are forced to remove their belonging from the Rio Grande Area, as the S... Rick Egan | The Salt Lake Tribune Homeless campers are forced to remove their belonging from the Rio Grande Area, as the S... Francisco Kjolseth | The Salt Lake Tribune A chain link fence has been erected in the Rio Grande area on the north end of 500... Steve Griffin | The Salt Lake Tribune Celeste waits her turn as the Utah Harm Reduction Coalition provides a needle excha... Al Hartmann | The Salt Lake Tribune Bundled up homeless people walk along 500 West near the Road Home shelter past a person... Al Hartmann | The Salt Lake Tribune Bundled up homeless people in blankets and winter clothing either hunker down in place ... Al Hartmann | The Salt Lake Tribune Salt Lake City police officer Sgt. Sam Wolf talks to homeless people camped near 200 S.... Al Hartmann | The Salt Lake Tribune Two homeless women sleep along 200 S. and 500 West in Salt Lake City Wednesday July 19.... Al Hartmann | The Salt Lake Tribune Garbage on the sidewalk near 200 S. and 500 W. Wedesday July 19. Homeless people sleep... Chris Detrick | The Salt Lake Tribune Melisha Spooner and her daughter Saphira, 3, walk to The Road Home in Salt Lake City ...\n\nRick Egan | The Salt Lake Tribune Homeless campers are forced to remove their belonging from the Rio Grande Area, as the Salt Lake County Health Department brings in heavy machinery to clean up the area, Thursday, July 6, 2017. Rick Egan | The Salt Lake Tribune Homeless campers are forced to remove their belonging from the Rio Grande Area, as the Salt Lake County Health Department brings in heavy machinery to clean up the area, Thursday, July 6, 2017. Rick Egan | The Salt Lake Tribune Homeless campers are forced to remove their belonging from the Rio Grande Area, as the Salt Lake County Health Department brings in heavy machinery to clean up the area, Thursday, July 6, 2017. Francisco Kjolseth | The Salt Lake Tribune A chain link fence has been erected in the Rio Grande area on the north end of 500 West near the homeless shelter to keep people from camping in the median on Friday, July 28, 2017. It is unclear if more fencing will be added. Steve Griffin | The Salt Lake Tribune Celeste waits her turn as the Utah Harm Reduction Coalition provides a needle exchange on 500 west between 200 south and 300 south in Salt Lake City Thursday July 27, 2017. The state's increased attention to the Rio Grande neighborhood comes as Utah's leading needle exchange provider is under fire for handing out more needles than it collects. Mindy Vincent, founder of the coalition, says the goal was never to break even, and that optics aside, needle exchange is proven to reduce the spread of disease among IV drug users. Al Hartmann | The Salt Lake Tribune Bundled up homeless people walk along 500 West near the Road Home shelter past a person on the sidewalk in a sleeping bag on a cold morning Wedneday Dec. 7. The Collective Impact steering committee met today to review a survey of homeless people in the Rio Grande area to determine if more overflow shelter space is needed. Al Hartmann | The Salt Lake Tribune Bundled up homeless people in blankets and winter clothing either hunker down in place or keep walking to stay warm along 500 West near the Road Home shelter on a cold morning Wedneday Dec. 7. The Collective Impact steering committee met today to review a survey of homeless people in the Rio Grande area to determine if more overflow shelter space is needed. Al Hartmann | The Salt Lake Tribune Salt Lake City police officer Sgt. Sam Wolf talks to homeless people camped near 200 S. and 500 W. Wednesday morning July 19. He politely wakes up dozens of homeless camped on the sidewalks, telling them that they have to break down their camps and offers advice for help and rescources. Camping on the street is a class B misdemeanor and can now be enforced. Al Hartmann | The Salt Lake Tribune Two homeless women sleep along 200 S. and 500 West in Salt Lake City Wednesday July 19. Al Hartmann | The Salt Lake Tribune Garbage on the sidewalk near 200 S. and 500 W. Wedesday July 19. Homeless people sleep and camp out there by the dozens. Al Hartmann | The Salt Lake Tribune Salt Lake City police officer Sgt. Sam Wolf wakes a woman camped in a makeshift shelter along 200 S. and 500 W. Wednesday morning July 19. He politely wakes up dozens of homeless camped on the sidewalks, telling them that they have to break down their camps and offers advice for help and rescources. Camping on the street is a class B misdemeanor and can now be enforced. Chris Detrick | The Salt Lake Tribune Melisha Spooner and her daughter Saphira, 3, walk to The Road Home in Salt Lake City Tuesday August 4, 2015."},
{"url": "https://www.washingtonpost.com/business/san-francisco-street-sells-for-90k-neighbors-arent-happy/2017/08/07/41be3292-7bc7-11e7-b2b1-aeba62854dfa_story.html?utm_term=.f8fe5da46fbf", "link_title": "San Francisco street sells for $90K. Neighbors aren\u2019t happy", "text": "SAN FRANCISCO \u2014 These days, the price of a San Francisco home can easily top a million dollars. But one savvy investor has bought up a whole street in the city\u2019s most exclusive neighborhood for a mere $90,000.\n\nTrouble is, some of the extremely wealthy residents of Presidio Terrace were not aware their street was up for sale and are not pleased it has been sold.\n\nPresidio Terrace is an oval shaped street sealed off by a gate from the tony Presidio Heights neighborhood. Lined with towering palm trees and multimillion dollar mansions, the street has been home, over the years, to famous residents including Sen. Dianne Feinstein and House Democratic leader Nancy Pelosi.\n\nThanks to a city auction stemming from an unpaid tax bill, Bay Area real estate investor Michael Cheng, and his wife Tina Lam, bought the street and now own the sidewalks, the street itself and other areas of \u201ccommon ground\u201d in the private development that, the San Francisco Chronicle report ed, has been managed by the homeowners association since at least 1905.\n\nCheng says reaction to the sale has been less than neighborly.\n\n\u201cI thought they would reach out to us and invite us in as new neighbors,\u201d Cheng told The Associated Press. \u201cThis has certainly blown up a lot more than we expected.\u201d\n\nIt turns out the homeowners association for Presidio Terrace failed to pay a $14-a-year property tax, something that owners of all 181 private streets in San Francisco must do, the Chronicle reported.\n\nSo the city\u2019s tax office put the property up for sale at the cost of $994 in an online auction to regain unpaid back taxes, penalties and interest. The couple eventually won the street with a $90,100 bid in an April 2015 auction.\n\nScott Emblidge, the attorney for the Presidio Homeowners Association, said in a letter to the city that the owners failed to pay because the tax bill was mistakenly being sent to the address of an accountant who hadn\u2019t worked for the homeowners association since the 1980s, the Chronicle reported.\n\nEmblidge said the residents didn\u2019t know their street was put on the auction block, let alone sold, until May when a title search company hired by Cheng and Lam reached out to ask if any residents had interest in buying back the property.\n\nThat was one of several options Cheng and Lam have considered for making the investment pay off.\n\nAnother option is to charge residents to park on their street \u2014 and rent out the 120 parking spaces that line the grand circular road.\n\n\u201cAs legal owners of this property, we have a lot of options,\u201d Cheng said, adding that nothing has been decided.\n\nThe matter could be headed for court.\n\nLast month, the homeowners petitioned the Board of Supervisors for a hearing to rescind the tax sale. The board has scheduled a hearing for October. The homeowners association has also sued the couple and the city, seeking to block Cheng and Lam from selling the street to anyone while the city appeal is pending.\n\nCopyright 2017 The Associated Press. All rights reserved. This material may not be published, broadcast, rewritten or redistributed."},
{"url": "https://blog.data.gov.sg/open-sourcing-schoolpicker-sg-71ea91cc9db9", "link_title": "Open sourcing Schoolpicker.sg", "text": "Back in May, we released our SchoolPicker.sg app, aimed at helping parents and students discover and shortlist schools.\n\nOur initial motivation is to create a better user experience by merging school-related information with geospatial data. As the product took shape, it become obvious to us that what we are building can be extended to many other applications, where:\n\nSeeing the success of our SchoolPicker.sg app in reaching out to target users, the Data.gov.sg team decided our right next step will be to open source our code for the community to build similar apps riding on open data.\n\nThe repository is now available on the Data.gov.sg Github:"},
{"url": "http://www.campusreform.org/?ID=9551", "link_title": "Professor lets students choose own grades for 'stress reduction'", "text": "A University of Georgia professor has adopted a \u201cstress reduction policy\u201d that will allow students to select their own grades if they \u201cfeel unduly stressed\u201d by the ones they earned.\n\nAccording to online course syllabi for two of Dr. Richard Watson\u2019s fall business courses, he has introduced the policy because \u201cemotional reactions to stressful situations can have profound consequences for all involved.\u201d\n\nAs such, if students feel \u201cunduly stressed by a grade for any assessable material or the overall course,\u201d they can \u201cemail the instructor indicating what grade [they] think is appropriate, and it will be so changed\u201d with \u201cno explanation\u201d being required.\n\n\u201cIf in a group meeting, you feel stressed by your group\u2019s dynamics, you should leave the meeting immediately and need offer no explanation to the group members,\u201d the policy adds, saying such students can \u201cdiscontinue all further group work\u201d with their remaining grade being \u201cbased totally on non-group work.\u201d\n\nSimilarly, when it comes to \u201ctests and exams\u201d for Watson\u2019s \u201cData Management\u201d and \u201cEnergy Informatics\u201d courses, all will be \u201copen book and open notes\u201d and \u201cdesigned to assess low level mastery of the course material.\u201d\n\nFinally, for in-class presentations, Watson will allow \u201conly positive comments\u201d to be made, while \u201ccomments designed to improve future presentations will be communicated by email.\u201d\n\nWatson, notably, does concede that \u201cwhile this policy might hinder the development of group skills and mastery of the class material,\u201d those outcomes are ultimately a student\u2019s \u201cresponsibility,\u201d though he promises to \u201cprovide every opportunity for [students] to gain high level mastery.\u201d\n\nCampus Reform reached out to Watson\u2014a \u201cRegents Professor\u201d at the university, a\u00a0title \u201cbestowed by the Board of Regents on truly distinguished faculty\u201d\u2014but did not receive a response in time for publication.\n\nFollow the author of this article on Twitter: @AGockowski"},
{"url": "http://regenerateland.info/2015/12/03/letter-to-a-vegetarian-nation-or-we-should-eat-meat/", "link_title": "Letter to a vegetarian nation (or why we should eat meat)", "text": "Please note that this article is now outdated. I have updated this article, added over 120 peer-reviewed references, expanded it, added several chapters, and answered many more questions in the soon-to-be-released book with the same title: \u201cLetter To A Vegetarian Nation\u201c. If you are interested in this topic I highly recommend you read the full book. As an apology for inconveniencing you, I am going to give you a 20% discount on the book, just follow these instructions:\n\nYou can also read a preview of the book below:\n\nAnd now, without further ado, the article you were looking for\u2026.\n\nI have put much of the supporting information for these statements in separate articles to keep this article at a manageable length. I encourage you to read those supporting articles as you come to them. Especially if you want an in-depth understanding of the reasoning behind each of these statements.\n\nThese are the values I hold, and the values that I believe most people (especially Vegans and Vegetarians) hold. I am not going to argue for or against these basic values in this article because they have all been discussed in great detail elsewhere.\n\nHopefully you agree with most, or all of those statements. If not, you should probably read this book.\n\nI have tremendous respect for Vegans and Vegetarians because you truly understand that what you\u00a0eat has a huge impact on our world. Not only do you care, but you actually put your ideals into practice. It takes a rare kind of person to actually change how they live their daily lives solely for the benefit of the environment, and animals. Those admirable traits are the reason I have addressed this article to Vegans and Vegetarians. Everyone needs to hear this message, but Im starting with vegetarians because you are at the cutting edge of food activism. You are more likely to care about the facts I present, and act upon them.\n\nI encourage you to read with an open mind. But I also hope that you will not accept my arguments unless they are logical, and ethically sound.\n\nIf you stick with me through this article I guarantee you will learn lots that you did not know before. If you read the supporting articles you will also leave with an\u00a0excellent understanding of sustainable agriculture, proper livestock management, desert restoration, and more.\n\nOver the past several decades new food production techniques have been developed which allow us to produce food while simultaneously regenerating the environment and local ecosystems.\n\nContrary to the mainstream narrative it\u00a0is possible to \u201cfeed the world\u201d sustainably, without destroying our soils, biodiversity, water quality, or health.\n\nThese techniques are spreading\u00a0like wildfire. Sustainable agriculture requires livestock (which I will explain later) and so the fact that sustainable agriculture is quickly becoming dominant has major implications for the ethics of eating meat (also discussed later in this article).\n\nAre you somewhere in North America, Europe, or Australia right now? If so, there is almost definitely a sustainable farmer within an hours drive of wherever you are.\n\nThere is no central database containing all the sustainable farmers in the world, unfortunately, so they can be hard to find. But I have created this list to help people find regenerative farmers nearby.\n\nWhy Is Sustainable Agriculture Taking Over?\n\nThe primary reason sustainable agriculture is\u00a0gaining so much momentum is that\u00a0sustainable\u00a0farming is generally\u00a0more profitable\u00a0than conventional farming.\n\nWhy are sustainable techniques more profitable? (profit = revenue \u2013 expenses)\n\nNote: If you have any doubts about the claims I am making about sustainable agriculture please refer to my post \u201cEvidence For Regenerative Agriculture\u201d.\n\nSo, sustainable farming produces equal, or better, revenues while significantly reducing costs. This means more profit for the farmer.\n\nSo sustainable farming is the way of the future. It will take over because it is more profitable for the farmer. And it will take over because it has to (fossil fuels, the bedrock of conventional agriculture, will not last forever).\n\n*Note:\u00a0For ease of understanding I use the word \u201csustainable agriculture\u201d throughout this article. But what I am specifically talking about can be better defined as \u201cregenerative agriculture\u201d. Some techniques often labelled as \u201csustainable\u201d (like Organic farming) are not actually sustainable. Everything that falls under the label of \u201cregenerative agriculture\u201d, on the other hand,\u00a0is\u00a0 sustainable, that is where the distinction lies.\n\nSo in this new world of sustainable agriculture, should we eat meat? Lets take a look at that issue\u2026\n\nFull Article:\u00a0Why Livestock Are Necessary For Food Production To Be Sustainable\n\n1) A Healthy Soil Food Web Is Necessary For Sustainable Food Production\n\nSupporting Article:\u00a0The Foundation Of Everything: The Soil Food Web\n\nIn brief, the health of the soil food web determines:\n\nIf the soil food web falls below a certain threshold of health it is actually impossible to sustainably produce enough food for humans on the Earth. Livestock are necessary for keeping the soil food web above this minimum threshold over the long term.\n\n2) Livestock Are Necessary To Maintain The Health Of The Soil Food Web\n\nLivestock maintain the soil food web through\u00a0trampling and through grazing.\n\nWild animals, or technology, cannot replace the trampling and grazing action of livestock.\n\nLivestock are also necessary for sustainable food production because plants require the increased nutrient availability provided by animal manure. Wild animals can no longer fill this role, and technology cannot replicate it on the scale required for global food production.\n\nLivestock are also necessary for moving nutrients from lowlands to highlands. A cow eats a plant from a valley, walks up hill (or is moved uphill by a human), and then deposits those nutrients on the hill when it defecates. If this ecosystem function is not provided all nutrients eventually move to the ocean. Wild animals can no longer provide this technology, livestock provide this service reliably and controllably.\n\nThe more livestock present, the more we will receive the benefits mentioned above (lower food costs, less land, less water, better nutrition, etc). So there are good reasons to maximize the number of livestock on our landscapes.\n\nBut, as vegetarians, you are probably wondering \u201cwhat is the\u00a0bare\u00a0minimum number of livestock needed for sustainable food production?\u201d The truthful answer is we don\u2019t know for sure because no one has done the required research. However, if we look at the evolutionary principles that influence our ecosystems we can infer that we will need quite a lot of livestock. I have estimated a minimum of around 22 million cattle are needed on a permanent basis to maintain the basic productivity of the current cropland in the US and Canada in the absence of fertilizers. This is excluding the millions of cattle needed to restore the brittle landscapes in North America and the millions which are currently on grasslands and need to stay there.\n\nProperly Managed Livestock: Livestock must be managed properly\u00a0or they will degrade ecosystems instead of being essential for their basic health. Read to find out what this means.\n\nClimate Change: Some of you may be concerned about livestock causing climate change. Please read this article entitled Properly Managed Livestock Are The Key To Stopping Climate Change.\n\nBrittle Environments are simply areas of the world where humidity is not distributed evenly throughout the year. Please refer to The Climate Brittleness Scale\u00a0info-graphic\u00a0for a better understanding of what exactly a Brittle Environment is and why it matters.\n\nFull Article:\u00a0Why Properly Managed Livestock Are Necessary In Brittle Environments\n\nHave you watched Allan Savory\u2019s TED talk yet? It is probably the best way to get a quick understanding of why livestock are necessary in Brittle Environments (for more in-depth information please read his book).\n\nIt is especially important for livestock to be properly managed in Brittle Environments.\u00a0Read to find out what this means.\n\nJust in case you are concerned about Climate Change and missed the previous link to this article, here it is again:\u00a0Properly Managed Livestock Are The Key To Stopping Climate Change.\n\nFood production in Brittle Environments requires large numbers of livestock to be sustainable. Plant food production must necessarily be a small proportion of food production in these areas compared to livestock production. On all other productive land we also need large numbers of livestock, but probably not as many as in Brittle Environments.\n\nLarge numbers of livestock all over the world will die no matter what, if we develop a sustainable civilization.\n\nWhat is the most ethical and logical way for these livestock to die?\n\nI am going to try to give you all of the information that has ethical consequences, and I will propose what I think the logical choice is. But really, this is an open question, and I encourage you to think it through carefully. It will be a real issue in the near future.\n\nThere are a few different ways that the average animal could die in a world filled with sustainable agriculture.\n\nLet us examine all of these options to see what the most ethical choice might be:\n\nClearly killing these livestock ourselves, with modern and humane methods, is by far the best way for them to die in terms of the amount of suffering involved.\n\nThere are only a few things that can happen to the body of an animal after it dies:\n\nLets examine the benefits and drawbacks of each option:\n\n*Note: If the dead livestock are going to be eaten by humans the animal would need to be butchered and processes quickly after death, in a hygienic way. This would be most easily accomplished by killing the animal at some time before it died naturally (that could mean years or hours, I don\u2019t know), instead of trying to find it after it died. \u00a0In the worst case scenario only the muscle meat will be eaten which will provide protein and calories for a single human for about 1 year (cow), about 6 months (pig, goat), about 1 month (deer, sheep), about 10 days (turkey), or 3 days (chicken) assuming they are only eating meat and are consuming 2000 calories per day. (reference)\n\nI have tried to provide a\u00a0complete\u00a0description of all the options available for the animals which we will need to raise for sustainable food production. I cannot tell you what the \u201ccorrect\u201d choice is because that will depend on your personal value system.\n\nIf we are trying to maximize animal welfare, \u00a0our best choice is to:\n\nMy choice\u00a0is only different from current sustainable agriculture practices in two ways:\n\nIt is better for a happy animal to exist, than for no animal to exist.\n\nDo we have an ethical duty to allow an animal to exist on a given piece of land if it will be happy and if its existence will not harm the environment or other animals?\n\nThe ecosystems of the Earth can support staggering numbers of animals, both large and small. Most of the Earth\u2019s ecosystems are currently underpopulated with animals compared with what they could sustain. (reference) We have advanced enough in our understanding of biology and ecology that if any ecosystem on Earth is not supporting its full potential population of animals it is because we have chosen (either through action or through inaction) this fate for the ecosystem.\n\nThe three landscapes on Earth in which this is an especially big issue are cropland, urban areas, and brittle landscapes. Because of our management, these landscapes currently support only a fraction of the biodiversity and animal life they could support.\n\nDoesn\u2019t this have any ethical implications?\n\nDon\u2019t forget that increasing the number of livestock on most of the Earth\u2019s surface will have many benefits for the environment and for people. (see Section 3 and 4 above)\n\nThe best way to maximize the number of intelligent animals living happy lives is to support farms with properly managed livestock. The best way to support them is to buy, and eat, their meat. By eating only plants you are reducing the numbers of intelligent animals living happy lives.\n\nI encourage you to examine this issue carefully and figure out what value\u00a0you place on the potential lives of happy animals.\n\nNote: We\u00a0should definitely be more concerned about the real suffering of the animals which are really alive in factory farms and inhumane conditions right now. Lets stop that crap! But sometimes our choice will not be between supporting factory farms and not supporting factory farms. Sometimes we will have the choice to support, or not support, the existence of happy livestock. So we should spend at least a little time thinking about this issue.\n\nDue to the number of people asking about livestock and their relationship to Climate Change I have added this section to address that issue specifically.\n\nPlease read the full article here:\u00a0\u201cWhy Properly Managed Livestock Are The Key To Stopping Climate Change\u201d\n\nYes\u2026 I\u00a0have\u00a0seen the movie \u201cCowspiracy\u201d. It does a great job at highlighting the destructive power of conventionally raised livestock and the lack of awareness about that issue. However nothing in that movie counters any of the points I make in this article.\n\nMost of the crazy statistics you see about livestock causing tremendous environmental destruction are true\u2026 but these statistics are not talking about\u00a0properly managed livestock! There is a huge difference!\n\nIn brief, properly managed livestock\u00a0are methane neutral (possibly they even\u00a0sequester methane in soils, research in this area is still hard to find) and are the most powerful and practical method for carbon sequestration known as of the writing of this article. (full references provided in the article link above)\n\nSustainable farmers must raise livestock, or employ the services of someone else\u2019s livestock, in order to maintain their basic productivity. There must be relatively large numbers of these animals, especially in Brittle Environments. These animals will die eventually. The most humane death for any animal is to be killed by modern human slaughter techniques near the end of their natural life. Most of these dead animals should be eaten by humans to reduce the total land base required for agriculture, to ensure the animals are given happy lives, to reduce the problems associated with not eating them, and because humans can probably use the calories to do more good in the world than a pack of scavengers or soil organisms can. In my opinion the most ethical diet possible includes meat from sustainable farms.\u00a0What do you think, now that you have read this article? Has your opinion changed from what it was when you started? Please comment and let me know.\n\nIf you want to learn more about sustainable/regenerative agriculture (including the role of livestock, and the soil food web) please visit this resource page with the links I have personally found most useful.\n\nThese books are great places to start:\n\nI have written more posts which are relevant to the issues covered here, but which were not previously listed:"},
{"url": "http://boingboing.net/2017/08/07/man-spraypaints-twitter-office.html", "link_title": "Man spraypaints Twitter office sidewalk with abusive tweets it refuses to delete", "text": "Man spraypaints Twitter office sidewalk with abusive tweets it refuses to delete\n\nTwitter didn't delete them, so he sprayed them on the pavement outside the company's offices in Germany.\n\nPopehat suspended from Twitter for sharing a threat he received (Update: unsuspended) This morning, Twitter covered Ken \u201cPopehat\u201d White\u2019s profile page in balloons to celebrate his birthday. This afternoon, it suspended his account for posting screenshots of threats he\u2019d received from another user. The ranting missive, from a far-right lawyer in Texas whose threatening Twitter postings White had earlier mocked, promises such hatred and cruelty that White [\u2026]\n\nProfile of Lexi Alexander: director, martial arts champ, and the first (only) woman to direct a Marvel movie Lexi Alexander is the German-Palestinian world kickboxing champ who moved to the US when Chuck Norris helped her get a Green Card; after helping the US Army develop its unarmed combat training program and working as a stuntwoman, she became a virtuoso action-film director, starting with indie movies and working her way up to directing [\u2026]\n\nThe Memory Hole has set up a page dedicated to the many tweets Anthony Scaramucci deleted shortly after being named Trump\u2019s Communications Director.\n\nThe Ockel Sirius B is a PC that fits in your pocket Working remotely often means using a full-size laptop, or forcing a tablet to do things it was never intended to do. Depending on your job, each may be a reasonable, if somewhat compromising solution, or an impossibly frustrating one. Either way, you\u2019ll be stuck with a tiny screen and a form factor that will destroy [\u2026]\n\nThis interactive bootcamp will give you a solid intro to web development Web technology has matured considerably in the last decade, and developers are continually in demand. If you\u2019re looking to add some skills to your resume, or are just interested in exploring the possibilities of the web, check out this Interactive Web Developer Bootcamp.In this course, you\u2019ll get a comprehensive overview of full-stack development using modern [\u2026]\n\nThis PC bundle features some top-rated apps to make you more productive Even if you only use your PC for web browsing, media playback, or light document creation, default software can sometimes come up short. To give your Windows PC a bit of a boost, we\u2019ve compiled a variety of helpful, paid apps that can enhance your user experience and make you more productive.In the\u00a0Premium PC Power [\u2026]"},
{"url": "https://github.com/willin/beian-domain", "link_title": "\u53ef\u5907\u6848\u57df\u540d\u540e\u7f00\u67e5\u8be2(Node.js)", "text": ""},
{"url": "https://www.nytimes.com/2017/08/07/business/dealbook/why-tesla-motors-is-fueling-up-on-debt.html", "link_title": "Why Tesla Motors Is Fueling Up on Debt", "text": "Elon Musk is an entrepreneur in a bubble.\n\nForced to choose between issuing a bit more of Tesla\u2019s turbocharged stock or tapping the overheated junk-bond market to finance the Model 3 ramp-up, Mr. Musk, the company\u2019s founder, opted for the latter. It raises execution risk for the $60 billion electric-car maker, but not by enough to persuade the chief executive to loosen his grip on the wheel.\n\nTesla has just over $3 billion in cash, but it is burning through roughly $1 billion a quarter as it embarks on one of the most daunting gambits in automotive history: taking production of its mass-market vehicle from zero to 400,000 or more a year in just 18 months.\n\nFortunately for Mr. Musk, investors can\u2019t seem to shower his ambitions with too much money. Tesla\u2019s stock has risen by 67 percent this year. The company is valued at some 27 times 2020 earnings, implying the kind of growth that even the most bullish of analysts don\u2019t expect, according to Reuters Breakingviews calculations.\n\nThe textbook financing solution would be to issue more of that high-priced paper. Selling five million shares at a 15 percent discount to market would raise the same $1.5 billion and dilute Mr. Musk\u2019s 20.4 percent stake by only 3 percent, assuming he didn\u2019t pitch in more himself.\n\nBut he doesn\u2019t have to when the high-yield bond market is on a tear. Investors desperate for income have depressed the yield gap between single-B-rated junk bonds and United States Treasury bonds by nearly 2 percentage points over the past year, to 3.59 points, according to Bank of America Merrill Lynch. Standard & Poor\u2019s Global Ratings affirmed its B-minus rating on Tesla, saying the boost to liquidity should offset the company\u2019s \u201csignificant execution risks.\u201d\n\nThe bond sale will raise debt to a lofty 5.5 times forecast 2017 earnings before interest, taxes, depreciation and amortization, or Ebitda. But Ebitda is set to more than double next year to $2.2 billion and then almost triple by 2020, according to Thomson Reuters data.\n\nEven if Mr. Musk is not as successful as Wall Street estimates, he should sell more than enough cars to make the leverage, and the additional interest bill, easy for bondholders to swallow."},
{"url": "https://www.bloomberg.com/news/articles/2017-08-06/spies-blockchain-and-alibaba-beating-china-s-fake-food-scourge", "link_title": "Inside the Secret World of Global Food Spies", "text": "A bowl of ice cream on a hot day in Shanghai gave American Mitchell Weinberg the worst bout of food poisoning\u00a0he can recall. It also inspired the then-trade consultant to set up Inscatech\u00a0\u2014 a global network of food spies.\n\nIn demand by multinational retailers and food producers, Inscatech and its agents scour supply chains around the world hunting for evidence of food industry fraud and malpractice. In the eight years since he founded the New York-based firm, Weinberg, 52, says China continues to be a key growth area for fraudsters as well as those developing technologies trying to counter them.\n\n\u201cStatistically we\u2019re uncovering fraud about 70 percent of the time, but in China it\u2019s very close to 100 percent,\u201d he said. \u201cIt\u2019s pervasive, it\u2019s across food groups, and it\u2019s anything you can possibly imagine.\u201d\n\nWhile adulteration has been a bugbear of consumers since prehistoric wine was first diluted with saltwater, scandals in China over the past decade\u00a0\u2014 from melamine-laced baby formula, to rat-meat dressed as lamb\u00a0\u2014 have seen\u00a0the planet\u2019s largest food-producing and consuming nation become a hotbed of corrupted, counterfeit, and contaminated food.\n\nWeinberg\u2019s company is developing molecular markers and genetic fingerprints to help authenticate natural products and sort genuine foodstuffs from the fakes. Another approach companies are pursuing uses digital technology to track and record the provenance of food from farm to plate.\n\n\u201cConsumers want to know where products are from,\u201d said Shaun Rein, managing director of China Market Research Group, citing surveys the Shanghai-based consultancy conducted with consumers and supermarket operators.\n\nServices that help companies mitigate the reputational risk that food-fraud poses is a \u201cbig growth area,\u201d according to Rein. \u201cIt\u2019s a great business opportunity,\u201d he said. \u201cIt\u2019s going to be important not just as a China play, but as a global play, because Chinese food companies are becoming part of the whole global supply chain.\u201d\n\nSome of the biggest food companies are backing technology that grew out of the anarchic world of crypto-currencies. It\u2019s called blockchain, essentially a shared, cryptographically secure ledger of transactions.\n\nWal-Mart Stores Inc., the world\u2019s largest retailer, was one of the first to get on board, just completing a trial using blockchain technology to track pork in China, where it has more than 400 stores. The time taken to track the meat\u2019s supply chain was cut from 26 hours to just seconds using blockchain, and the scope of the project is being widened to other products, said Frank Yiannas, Wal-Mart\u2019s vice president for food safety, in an interview Thursday.\n\nShanghai-based Zhong An Information and Technology Services Co. said in June it will use the technology to track chickens from the coop to the processing facility and on to the market or store.\n\nAlibaba Group Holding Ltd., too, sees the potential for the eight-year-old technology to provide greater product integrity across its platforms, which accounted for more then 75 percent of China\u2019s online retail sales in 2015. The planned blockchain project will involve the Chinese e-commerce behemoth working with food suppliers in Australia and New Zealand, as well as Australia Post and auditors PricewaterhouseCoopers LLP.\n\n\u201cFood fraud is a serious global issue,\u201d said Maggie Zhou, managing director for Alibaba in Australia and New Zealand. \u201cThis project is the first step in creating a globally respected framework that protects the reputation of food merchants and gives consumers further confidence to purchase food online.\u201d\n\nFraud costs the global food industry as much as $40 billion annually, according to John Spink, director of Michigan State University\u2019s Food Fraud Initiative. In China, where the 2008 melamine milk crisis resulted in the death of at least six babies, it\u2019s a hot-button issue compounded by the country\u2019s growing appetite for higher quality food and swelling middle class. A Pew Research Center study last year found 40 percent of Chinese view food safety as a \u201cvery big problem,\u201d up from 12 percent in 2008.\n\n\u201cThis is not a Chinese issue\u00a0\u2014 it\u2019s a global issue,\u201d said Yongguan Zhu, director general of the Institute of Urban Environment, part of the state-funded Chinese Academy of Sciences. \u201cWhat we have to do is reinforce our regulations to improve the transparency of the administration, for example information-sharing.\u201d\n\nZhu says blockchain could play an important role in improving traceability. Its database of records can be built like a chain and can\u2019t be broken or re-ordered without disrupting the entire connection.\n\nChina strengthened its food safety law in 2015 in response to the spate of scandals. Counterfeiters and food tamperers face tougher penalties, including jail time in some cases, and more than $800 million has been spent hiring more food safety personnel and bolstering monitoring facilities, according to an April report from the Paulson Institute, a Washington-based think tank. Last month, Beijing emphasized to authorities the need to be upfront in disclosing food safety issues.\n\n\u201cFood-fraud will always exist,\u201d said Yongning Wu, chief scientist at the government-run China National Center For Food Safety Risk Assessment. While authorities in China have joined the global fight against the scourge, Wu doesn\u2019t see the problem disappearing.\n\n\u201cWe can only develop technology to detect it,\u201d he said. \u201cHowever, fake-food producers will always update their technology to dodge inspections.\u201d\n\nThe wiliness of fraudsters is what makes Inscatech\u2019s Weinberg less hopeful about blockchain. His firm mainly uses informants on the ground to sniff out where in the production process food-fraud is taking place, and most of his work in China is with western companies that manufacture or source product there.\n\n\u201cThe problem is the data is only as reliable as the person providing the data,\u201d said Weinberg, who recalls seeing everything in China from synthetic eggs to fake shrimp that still sizzle in a wok. \u201cIn most supply chains there is one or more \u2018unreliable\u2019 data provider. This means blockchain is likely useless for protecting against food-fraud unless every piece of data is scrutinized to be accurate.\u201d\n\nA months-long Bloomberg investigation into the global shrimp trade last year showed how unreliable documentation had fanned an illegal transhipping scheme involving Chinese aquaculture exporters.\n\nBut blockchain is \u201clight years\u201d away from the system used by the global food industry today, which relies heavily on paper records, said\u00a0Yiannas, Wal-Mart\u2019s food safety chief. By recording the identity of those who input data into the chain, the technology removes the anonymity that has helped food-fraud to thrive, he said.\n\nThe role of humans in recording the supply chain will also diminish, said Yiannas. \u201cMore and more of these documents will eventually be captured in an automated way.\u201d\n\nChina\u2019s Food and Drug Administration didn\u2019t immediately respond to an email requesting comment on the country\u2019s food safety efforts.\n\nSome companies are already bringing traceability to consumers. Fonterra Cooperative Group Ltd., the world\u2019s biggest dairy exporter, started putting QR codes on cans of infant formula in April, enabling buyers to verify the product\u2019s authenticity.\n\nThe challenges for China\u00a0\u2014 \u201cthe factory of the world\u201d \u2014 are especially vast because of its size, population, multilayered administrative divisions, and \u201cthe willingness of criminals to exploit every corner that they can in order to make money,\u201d said Michael Ellis, who ran Interpol\u2019s trafficking in illicit goods unit until October.\n\nAt Interpol, Ellis, a former detective with Scotland Yard in London, was involved in \u201c Opson,\u201d an operation that led to the seizure of more than 10,000 tons and 1 million liters (264,000 gallons) of hazardous fake-food and drinks across more than 50 countries.\n\nWithout a presence to fight it, food-fraud globally \u201cwill explode,\u201d Ellis said. \u201cIt will just continue to grow, and who knows where it will lead.\u201d\n\n\u2014 With assistance by Emma O'Brien, Jiefei Liu, Lulu Yilun Chen, Yuji Nakamura, Samuel Dodge, Shuping Niu, and Yue Qiu"},
{"url": "https://www.economist.com/blogs/graphicdetail/2017/08/daily-chart-3", "link_title": "Fewer Britons are taking drugs, but more are dying from them", "text": ""},
{"url": "https://iainews.iai.tv/articles/beard-nassem-taleb-twitter-feud-and-dangers-of-scientism-auid-868", "link_title": "Beard vs. Taleb: Scientism and the Nature of Historical Inquiry", "text": ""},
{"url": "https://www.nytimes.com/2017/08/07/climate/climate-change-drastic-warming-trump.html", "link_title": "Government Report Finds Drastic Impact of Climate Change on U.S", "text": "One government scientist who worked on the report, Katharine Hayhoe, a professor of political science at Texas Tech University, called the conclusions among \u201cthe most comprehensive climate science reports\u201d to be published. Another scientist involved in the process, who spoke to The New York Times on the condition of anonymity, said he and others were concerned that it would be suppressed.\n\nThe White House and the Environmental Protection Agency did not immediately return calls or respond to emails requesting comment on Monday night.\n\nThe report concludes that even if humans immediately stopped emitting greenhouse gases into the atmosphere, the world would still feel at least an additional 0.50 degrees Fahrenheit (0.30 degrees Celsius) of warming over this century compared with today. The projected actual rise, scientists say, will be as much as 2 degrees Celsius.\n\nA small difference in global temperatures can make a big difference in the climate: The difference between a rise in global temperatures of 1.5 degrees Celsius and one of 2 degrees Celsius, for example, could mean longer heat waves, more intense rainstorms and the faster disintegration of coral reefs.\n\nAmong the more significant of the study\u2019s findings is that it is possible to attribute some extreme weather to climate change. The field known as \u201cattribution science\u201d has advanced rapidly in response to increasing risks from climate change.\n\nThe E.P.A. is one of 13 agencies that must approve the report by Aug. 18. The agency\u2019s administrator, Scott Pruitt, has said he does not believe that carbon dioxide is a primary contributor to global warming.\n\n\u201cIt\u2019s a fraught situation,\u201d said Michael Oppenheimer, a professor of geoscience and international affairs at Princeton University who was not involved in the study. \u201cThis is the first case in which an analysis of climate change of this scope has come up in the Trump administration, and scientists will be watching very carefully to see how they handle it.\u201d\n\nScientists say they fear that the Trump administration could change or suppress the report. But those who challenge scientific data on human-caused climate change say they are equally worried that the draft report, as well as the larger National Climate Assessment, will be publicly released.\n\n\u201cThe National Climate Assessment seems to be on autopilot because there\u2019s no political that has taken control of it,\u201d said Myron Ebell, a senior fellow at the Competitive Enterprise Institute. He was referring to a lack of political direction from the Trump administration.\n\nThe report says significant advances have been made linking human influence to individual extreme weather events since the last National Climate Assessment was produced in 2014. Still, it notes, crucial uncertainties remain.\n\nIt cites the European heat wave of 2003 and the record heat in Australia in 2013 as specific episodes where \u201crelatively strong evidence\u201d showed that a man-made factor contributed to the extreme weather.\n\nIn the United States, the authors write, the heat wave that broiled Texas in 2011 was more complicated. That year was Texas\u2019 driest on record, and one study cited in the report said local weather variability and La Ni\u00f1a were the primary causes, with a \u201crelatively small\u201d warming contribution. Another study had concluded that climate change made extreme events 20 times more likely in Texas.\n\nBased on those and other conflicting studies, the federal draft concludes that there was a medium likelihood that climate change played a role in the Texas heat wave. But it avoids assessing other individual weather events for their link to climate change. Generally, the report described linking recent major droughts in the United States to human activity as \u201ccomplicated,\u201d saying that while many droughts have been long and severe, they have not been unprecedented in the earth\u2019s hydrologic natural variation.\n\nWorldwide, the draft report finds it \u201cextremely likely\u201d that more than half of the global mean temperature increase since 1951 can be linked to human influence.\n\nIn the United States, the report concludes with \u201cvery high\u201d confidence that the number and severity of cool nights have decreased since the 1960s, while the frequency and severity of warm days have increased. Extreme cold waves, it says, are less common since the 1980s, while extreme heat waves are more common.\n\nThe study examines every corner of the United States and finds that all of it was touched by climate change. The average annual temperature in the United States will continue to rise, the authors write, making recent record-setting years \u201crelatively common\u201d in the near future. It projects increases of 5.0 to 7.5 degrees Fahrenheit (2.8 to 4.8 degrees Celsius) by the late century, depending on the level of future emissions.\n\nIt says the average annual rainfall across the country has increased by about 4 percent since the beginning of the 20th century. Parts of the West, Southwest and Southeast are drying up, while the Southern Plains and the Midwest are getting wetter.\n\nWith a medium degree of confidence, the authors linked the contribution of human-caused warming to rising temperatures over the Western and Northern United States. It found no direct link in the Southeast.\n\nAdditionally, the government scientists wrote that surface, air and ground temperatures in Alaska and the Arctic are rising at a frighteningly fast rate \u2014 twice as fast as the global average.\n\n\u201cIt is very likely that the accelerated rate of Arctic warming will have a significant consequence for the United States due to accelerating land and sea ice melting that is driving changes in the ocean including sea level rise threatening our coastal communities,\u201d the report says.\n\nHuman activity, the report goes on to say, is a primary culprit.\n\nThe study does not make policy recommendations, but it notes that stabilizing the global mean temperature increase to 2 degrees Celsius \u2014 what scientists have referred to as the guardrail beyond which changes become catastrophic \u2014 will require significant reductions in global levels of carbon dioxide.\n\nNearly 200 nations agreed as part of the Paris accords to limit or cut fossil fuel emissions. If countries make good on those promises, the federal report says, that will be a key step toward keeping global warming at manageable levels.\n\nMr. Trump announced this year that the United States would withdraw from the Paris agreement, saying the deal was bad for America."},
{"url": "https://www.cnbc.com/2017/08/07/it-may-be-illegal-for-google-to-punish-engineer-over-anti-diversity-memo-commentary.html", "link_title": "Why it may be illegal for Google to punish that engineer over his now viral memo", "text": "Comments are pouring in over an internal memo an unnamed male software engineer at Google sent to co-workers on Friday challenging some of the tech giant's diversity efforts, such as mentoring programs open only to people of a certain race or gender. Gizmodo posted the memo on the internet on Saturday and it since has gone viral. The memo contains such provocative statements as:\n\n\"Women, on average, have more: Openness directed towards feelings and aesthetics rather than ideas. Women generally also have a stronger interest in people rather than things, relative to men (also interpreted as empathizing vs. systemizing). [\u00b6] These two differences in part explain why women relatively prefer jobs in social or artistic areas. More men may like coding because it requires systemizing and even within SWEs (software engineers), comparatively more women work on front end, which deals with both people and aesthetics.\" (Read 10 of the most shocking quotes from thememo.)\n\nMany inside and outside of Google have called for the man's dismissal. However, there are at least three ways the law may keep the company from imposing any discipline.\n\nFirst, federal labor law bars even non-union employers like Google from punishing an employee for communicating with fellow employees about improving working conditions. The purpose of the memo was to persuade Google to abandon certain diversity-related practices the engineer found objectionable and to convince co-workers to join his cause, or at least discuss the points he raised.\n\nIn a reply to the initial outcry over his memo, the engineer added to his memo: \"Despite what the public response seems to have been, I've gotten many personal messages from fellow Googlers expressing their gratitude for bringing up these very important issues which they agree with but would never have the courage to say or defend because of our shaming culture and the possibility of being fired.\" The law protects that kind of \"concerted activity.\"\n\nSecond, the engineer's memo largely is a statement of his political views as they apply to workplace policies. The memo is styled as a lament to \"Google's Ideological Echo Chamber.\" California law prohibits employers from threatening to fire employees to get them to adopt or refrain from adopting a particular political course of action.\n\nDanielle Brown, Google's newly installed vice president of Diversity, Integrity, & Governance, made it clear that the engineer's memo does not reflect \"a viewpoint that I or this company endorses, promotes or encourages.\"\n\nAn employee does not have free reign to engage in political speech that disrupts the workplace, but punishing an employee for deviating from company orthodoxy on a political issue is not allowed either. Brown acknowledged that when she wrote that \"an open, inclusive environment means fostering a culture in which those with alternative views, including different political views, feel safe sharing their opinions.\"\n\nThird, the engineer complained in parts of his memo about company policies that he believes violate employment discrimination laws. Those policies include support programs limited by race or gender and promotional and hiring scoring policies that consider race and gender. It is unlawful for an employer to discipline an employee for challenging conduct that the employee reasonably believed to be discriminatory, even when a court later determines the conduct was not actually prohibited by the discrimination laws. In other words, the engineer doesn't have to be right that some of Google's diversity initiatives are unlawful, only that he reasonably believes that they are.\n\nBrown is correct that an employee has no right to engage in workplace discourse that offends anti-discrimination laws; employees may not engage in unlawful harassment under the guise of protected concerted activity or political grievances.\n\nThe lawful response to this software engineer's memo, however, appears to be continuation of the dialogue he started rather than termination of his employment.\n\nCommentary by Dan Eaton, a partner with the San Diego law firm of Seltzer Caplan McMahon Vitek, where his practice focuses on defending and advising employers. He also is a professor at the San Diego State University College of Business Administration where he teaches classes in business ethics and employment law. Follow him on Twitter @DanEatonlaw.\n\nFor more insight from CNBC contributors, follow @CNBCopinion on Twitter."},
{"url": "https://techcrunch.com/2017/08/07/researchers-use-radio-waves-to-wirelessly-monitor-sleep-patterns/", "link_title": "Researchers use radio waves to wirelessly monitor sleep patterns", "text": "MIT researchers have unveiled a new method for wirelessly monitoring sleep. The whole thing works a bit like echolocation \u2014 radio waves are beamed off a sleeping subject and changes in the body are detected when they bounce back.\n\nThe research is the latest in a number of different activities the team has been monitoring using low-power radio waves. Thanks to new AI technology, the system is now able to translate subtle movement into meaningful information\u00a0about the subject\u2019s sleep patterns, including sleep stages (light/deep/R.E.M.), movement and breathing rate.\n\nTesting was conducted on 25 volunteers over 100 nights of sleep. Study lead professor Dina Katabi tells TechCrunch that the system was able to detect sleep patterns with around an 80 percent accuracy rate \u2014 roughly the same as industry-standard EEG sleep tests.\n\nWhile little conclusive research has been done on sleep tracking through wearables like Fitbit and the Apple Watch, those products rely almost exclusively on movement detected by an accelerometer to determine sleep patterns.\n\n\u201cWearables are great, but our vision was to have something we call \u2018invisibles,\u2019 \u201d says Katabi. \u201cIt\u2019s a device that disappears into the background of your home, but at the same time, monitors any sort of health problems, just using wireless signals.\u201d\n\nThe wireless system takes a much more comprehensive variety of factors into account than wearables, including movement, breathing and pulse rate. It does all that while staying out of the way, placed on a shelf or mounted on a wall a few meters away from the sleeper.\n\nThe system uses much less power than Wi-Fi and is robust enough to be used in different locations with different patients, without requiring any recalibration. All of that means it could be ideal for use in monitoring patients at home in their natural sleep setting.\n\nThat\u2019s the next step for the system \u2014 researching the ways in which sleep patterns impact diseases like Parkinson\u2019s and Alzheimer\u2019s, both of which are linked closely to various sleep disorders. Of course, it\u2019s still early stages, and actually utilizing such a system for public use would require approval from organizations like the FDA. But for now, the results seem promising."},
{"url": "https://www.servethehome.com/trends-flash-memory-summit-2017/", "link_title": "Trends for Flash Memory Summit 2017 \u2013 ServeTheHome", "text": "Heading into the week of Flash Memory Summit 2017 we wanted to highlight a few key trends that we are expecting to see at this year\u2019s show. There are going to be a ton of announcements, and let us face it. You are busy and need a summary especially as many of the technologies announced this week will be years away from a product. Here is the preview summary.\n\nIt is no secret in the industry that the popular 2280 m.2 form factor and even 22110 do not provide the best data center form factors. Vendors see these form factors as too small to put enough NAND behind a controller. When a goal is to minimize controller costs, bigger form factors are better. U.2 has been the traditional answer, but they take up significant space in a server.\n\nSTH showed that Samsung is pushing a \u201cm.3\u201d standard a few months ago:\n\nWhich allows power loss protection circuitry as well as larger capacities than traditional m.2 SSDs.\n\nA few months ago, Samsung was already touting 16GB m.3 devices.\n\nWhile that may be the Samsung view, we expect other vendors to start addressing this trend at FMS 2017.\n\nThis is going to be, by far, the hottest trend at FMS 2017. Name a vendor and we expect them to be involved with at least one NVMe over Fabrics demo at FMS 2017. The reason this is hot has more to do with the networking side. As SFP28 and QSFP28 networking bring 25/50/100GbE along with legacy 40GbE and Infiniband/ Omni-Path, the groundwork is there to start using NVMe devices over fabrics. Remember, most of today\u2019s NVMe SSDs are x4 devices so networking is starting to catch up with maximum throughput.\n\nAbove is an example of NVMe over fabrics using Intel Omni-Path 100Gbps fabric and an Intel Optane PCIe SSD running 70/30 random 4K workloads over the fabric. There is surely more to tune, however, you can easily see the promise.\n\nLast year I asked several vendors when we were finally going to see the transition from SATA to PCIe. For those that are not aware, most numbers from analyst firms such as IDC place SATA still in the leadership role for data center SSDs. There are a few factors that are pushing PCIe and NVMe to the forefront this year.\n\nFirst, we have just started a major platform refresh cycle. With AMD EPYC and Intel Xeon Scalable processor launches, we have two new platforms that offer something that we have not seen since the Intel Xeon E5 V1 series, major new server re-designs.\n\nThese server re-designs mean more U.2, m.2 and emerging form factors are supported in server. For example, Dell EMC is upping the number of NVMe drives supported in its PowerEdge 14th generation servers. As slots swap from SATA/ SAS to PCIe, that will have a major impact on adoption figures.\n\nJust a bunch of flash (JBOF) is a term that many vendors do not like, but the new use case is really cool. What we are going to see at FMS 2017 is a number of JBOFs\u00a0that support SR-IOV. By doing so, they can have multiple servers access the drives at any given time.\n\nThe impact of this is huge. When drives sit in a chassis one has two options. They can be locally utilized, but with a low utilization figure in terms of performance and capacity. Alternatively, they can be part of a distributed storage system (vSAN, Ceph, GlusterFS) and then one adds a new layer of complexity. With the new JBOFs we are seeing one can essentially connect a number of servers to a shared NVMe shelf. These servers can then access the storage arrays instead of using local storage.\n\nHere is where the use case is interesting: shared chassis. If you have a 4 node 2U system or denser, you are trading storage capacity for compute density. Using JBOFs and PCIe switches you can add more NVMe storage and get higher utilization. NVMe flash is a major cost driver so optimizing its use is another step in composable infrastructure.\n\nThere are going to be a number of new storage technologies in FMS with two main themes. First, there are a number of technologies aimed at increasing density and capacity. After TLC was adopted in the enterprise, vendors are hopeful QLC will be adopted quickly. With 3D NAND vendors feel they can bring to market a solution that will bridge the need to store data on SSDs instead of hard drives. There are a large number of workloads that are heavily read focused and for those workloads, QLC is going to be the next step.\n\nAt the other end of the spectrum, the high-performance side is completely bound by a PCIe 3.0 x4 bus common on traditional NVMe form factors. One answer is to move to higher-bandwidth PCIe 3.0 x8 or x16 form factors for NVMe drives. The next-generation is moving to PCIe 4.0 to get more bandwidth per lane. The fact that neither AMD nor Intel adopted PCIe 4.0 in their mid-2017 generation products means that these products are going to be focused on more niche platforms. One example will be SoCs that allow for PCIe 4.0 connections and NVMe over fabrics without a traditional x86 server.\n\nBeyond the traditional form factors, the next-generation storage in this space will be dominated by moving persistent storage toward the memory. Intel said its Optane DIMMs will be a 2018 product. Other technologies like NV-DIMMs are being pushed heavily with OEMs such as Dell EMC and HPE. More exotic technologies will push these boundaries even further.\n\nFMS 2017 has some cool new technologies but there are a few \u201ckey themes\u201d as Patrick calls them. One segment of the market is driving toward replacing the next tier of disks as primary storage. That entails lower write endurance, lower cost per byte and higher densities. We are going to see mainstream SATA SSDs surpass HDD capacities, not just futuristic halo products. That also means using NVMe in a more cost effective manner either through JBOFs or NVMe over Fabrics. Another segment is aggressively pursuing RAM and trying to add a high capacity/ lower latency tier just below RAM. Finally, there is going to be the natural evolution of the status quo for mainstream SAS, SATA and NVMe whith the transition to NVMe take hold as we move toward next-gen system platforms."},
{"url": "https://github.com/OmkarPathak/pygorithm", "link_title": "Pygorithm \u2013 a Python module for learning all major algorithms", "text": "It's that easy. If you are using Python 2.7 use pip instead. Depending on your\n\npermissions, you might need to use to install."},
{"url": "https://iq-research.info/en/page/average-iq-by-country", "link_title": "Which country has the highest IQ?", "text": ""},
{"url": "https://hackernoon.com/node-js-emerging-as-the-universal-development-framework-for-a-diversity-of-applications-c2e788290f5f", "link_title": "Node.js Emerging as the Universal Development Framework", "text": "Last year and at the beginning of this year, we asked you, Node.js users, to help us understand where, how and why you are using Node.js. We wanted to see what technologies you are using alongside Node.js, how you are learning Node.js, what Node.js versions you are using, and how these answers differ across the globe.\n\nThank you to all who participated.\n\n1,405 people from around the world (85+ countries) completed the survey, which was available in English and Mandarin. 67% of respondents were developers, 28% held managerial titles, and 5% listed their position as \u201cother.\u201d Geographic representation of the survey covered: 35% United States and Canada, 41% EMEA, 19% APAC, and 6% Latin and South America.\n\nThere was a lot of incredible data collected revealing:\n\nThe report also painted a detailed picture of the types of technologies being used with Node.js, language preferences alongside Node.js, and preferred production and development environments for the technology.\n\n\u201cGiven developers\u2019 important role in influencing the direction and pace of technology adoption, surveys of large developer communities are always interesting,\u201d said Rachel Stephens, RedMonk Analyst. \u201cThis is particularly true when the community surveyed is strategically important like Node.js.\u201d\n\nIn September, we will be releasing the interactive infographic of the results, which will allow you to dive deeper into your areas of interest. For the time being, check out our blog on the report below and download the executive summary here.\n\nThe Benefits of Node.js Grow with Time No Matter the Environment\n\nWith more than 8 million Node.js instances online, three in four users are planning to increase their use of Node.js in the next 12 months. Many are learning Node.js in a foreign language with China being the second largest population outside of the United States using Node.js.\n\nThose who continue to use Node.js over time were more likely to note the increased business impact of the application platform. Key data includes:\n\nMost Node.js users found that the application platform helped improve developer satisfaction and productivity, and benefited from cost savings and increased application performance.\n\nThe growth of Node.js within companies is a testament to the platform\u2019s versatility. It is moving beyond being simply an application platform, and beginning to be used for rapid experimentation with corporate data, application modernization and IoT solutions. It is often times the primary focus for developers with the majority of developers spending their time with Node.js on the back-end, full stack, and front-end. Although Node.js use is beginning to rise in the Ops/DevOps sector and mobile as well.\n\nNode.js Used Less Than 2 Years, but Growing Rapidly In Businesses\n\nExperience with Node.js varied\u200a\u2014\u200aalthough many have been using Node.js less than 2 years. Given the rapid pace of Node.js adoption with a growth rate of about 100% year-over-year this isn\u2019t surprising.\n\nCompanies that were surveyed noted that they were planning to expand their use of Node.js for web applications, enterprise, IoT, embedded systems and big data analytics. Conversely, they are looking to decrease the use of Java, PHP and Ruby.\n\nLarge Mix of Technology and Tools Used Alongside Node.js for Digital Transformation\n\nModernizing systems and processes are a top priority across businesses and verticals. Node.js\u2019 light footprint and componentized nature make it a perfect fit for microservices (both container and serverless based) for lean software development without the need to gut out legacy infrastructure.\n\nThe survey revealed that 47% of respondents are using Node.js for container and serverless-based architectures across development areas:\n\nThe use of Node.js expands well beyond containers and cloud-native apps to touch development with databases, front-end framework/libraries, load balancing, message systems and more.\n\nAnd for developers of all focus areas, Node.js has versatile usage.\n\n68% of survey respondents who are using Node.js for serverless are using Amazon Web Services for production. 47% of survey respondents using Node.js and serverless are using Amazon Web Services for development.\n\nDevelopers who use Node.js and serverless used it across several development areas with the most popular being: back-end, full stack, front-end and DevOps.\n\nRevenues for big data and business analytics are set to grow to more than $203 billion in 2020. Vendors in this market require distributed systems within their products and rely on Node.js for data analysis.\n\nThe survey revealed that big data/business analytics developers and managers are more likely to see major business impacts after instrumenting Node.js into their infrastructure with key benefits being productivity, satisfaction, cost containment, and increased application performance.\n\nWith the creation of the long-term support plan in 2015, there has been an increase of enterprise development work with Node.js. The Long-Term support plan provides a stable (LTS) release line for enterprises that prioritize security and stability, and a current release line for developers who are looking for the latest updates and experimental features. The survey revealed:\n\nThe Long-Term Support versions of Node.js tend to be the most highly sought after with development.\n\n*Node.js 4 and 6 are the LTS versions and best suited for the enterprise of those that favor stability and security over new features.\n\nIf you want to continue to learn more about Node.js, sign up for our monthly community newsletter, which will continue to pepper you with data for months to come, and also provide you with information on cool new projects using Node.js. The Node.js Foundation will also hold its annual conference from October 4\u20136 in Vancouver, Canada. Come join us at Node.js Interactive.\n\n*Mark Hinkle, the Node.js Foundation\u2019s Executive Director, will be talking about this data and provide an update on the Foundation during his keynote at NodeSummit."},
{"url": "https://github.com/Hack-with-Github/Awesome-Hacking", "link_title": "Hack with GitHub \u2013 collection of awesome lists for hackers, security researchers", "text": "Your contributions are always welcome !\n\nFollow Hack with GitHub on your favorite social media to get daily updates on interesting GitHub repositories related to Security.\n\nPlease have a look at contributing.md"},
{"url": "http://www.nature.com/news/first-genetically-engineered-salmon-sold-in-canada-1.22116", "link_title": "First genetically engineered salmon sold in Canada", "text": "Genetically engineered salmon has reached the dinner table. AquaBounty Technologies, the company in Maynard, Massachusetts, that developed the fish, announced on 4 August that it has sold some 4.5 tonnes of its hotly debated product to customers in Canada.\n\nThe sale marks the first time that a genetically engineered animal has been sold for food on the open market. It took AquaBounty more than 25 years to get to this point.\n\nThe fish, a variety of Atlantic salmon (Salmo salar), is engineered to grow faster than its non-genetically modified counterpart, reaching market size in roughly half the time \u2014 about 18\u00a0months. AquaBounty sold its first commercial batch at market price: US$5.30\u00a0per pound ($11.70\u00a0per kilogram), says Ron Stotish, the company\u2019s chief executive. He would not disclose who bought it.\n\nAquaBounty raised the fish in tanks in a small facility in Panama. It plans to ramp up production by expanding a site on Canada\u2019s Prince Edward Island, where local authorities gave the green light for construction in June. In the same month, the company also acquired a fish farm in Albany, Indiana; it awaits the nod from US regulators to begin production there.\n\nThe sale of the fish follows a long, hard-fought battle to navigate regulatory systems and win consumer acceptance. \u201cSomebody\u2019s got to be first and I\u2019m glad it was them and not me,\u201d says James West, a geneticist at Vanderbilt University in Nashville, Tennessee, who co-founded AgGenetics, a start-up company in Nashville that is engineering cattle for the dairy and beef industries. \u201cIf they had failed, it might have killed the engineered livestock industry for a generation,\u201d he says.\n\nAquaBounty\u2019s gruelling path from scientific discovery to market terrified others working in animal biotechnology, and almost put the company out of business on several occasions. Scientists first demonstrated the fast-growing fish in 1989. They gave it a growth-hormone gene from Chinook salmon (Oncorhynchus tshawytscha), along with genetic regulatory elements from a third species, the ocean pout (Zoarces americanus). The genetic modifications enable the salmon to produce a continuous low level of growth hormone.\n\nAquaBounty formed around the technology in the early 1990s and approached regulators in the United States soon after. It then spent almost 25 years in regulatory limbo. The US Food and Drug Administration (FDA) approved the salmon for consumption in November 2015, and Canadian authorities came to the same decision six months later. Neither country requires the salmon to be labelled as genetically engineered.\n\nBut unlike in Canada, political battles in the United States have stalled the salmon\u2019s entry into the marketplace. The law setting out the US government\u2019s budget for fiscal year 2017 includes a provision that instructs the FDA to forbid the sale of transgenic salmon until it has developed a programme to inform consumers that they are buying a genetically engineered product. Senator Lisa Murkowski (Republican, Alaska), who inserted the provision, has called AquaBounty\u2019s salmon \u201cfake fish\u201d.\n\nActivists in both the United States and Canada have demanded that regulators reconsider their decisions, and some have filed lawsuits. The Center for Food Safety, an environmental-advocacy group in Washington DC, sued the FDA last year in an attempt to overturn its salmon decision. The group says the agency lacks the legal authority to oversee genetically engineered animals, and that it made its decision without fully considering the environmental risks.\n\nThe announcement that AquaBounty\u2019s fish are landing on Canadian tables is sure to dredge up opposition, says Stotish. He argues that the genetically engineered fish are good for the economy \u2014 attractive because they can be grown near metropolitan areas rather than being flown in from overseas, bringing salmon-farming jobs back to the United States and Canada. And because the AquaBounty salmon are grown in tanks, he adds, they don\u2019t encounter many of the pathogens and parasites that often afflict salmon raised in sea cages.\n\n\u201cI think the larger market is viewing it as a more predictable, sustainable source of salmon,\" Stotish says. \u201cAs a first sale this was very positive and encouraging for us.\u201d"},
{"url": "https://ibm.co/IML_Flights1", "link_title": "Predicting Flight Cancellations Using Weather Data", "text": "This is the first in a series of blog posts where we\u2019ll explore a use case and a few different machine learning platforms to see how we might build a model, using platforms that can help predict flight cancellations. In part one, we\u2019ll talk about the use case, how and why we limited the scenario, and about the data we gathered to start the data science / machine learning process.\n\nFor our use case, we chose flight cancellations and weather data for a few different reasons. We wanted a project that\u2026\n\n\u2022 Would already have reasonably large amount of data, but not so much that we\u2019d need more than just our laptop to do the data processing.\n\n\u2022 Would require federating data from more than just one source.\n\nWould require the various steps in a real data science/machine learning project. CRISP-DM is one such process.\n\nMany people think that \u201ctraining\u201d a model is all that a machine learning project consists of. It doesn\u2019t take much reading about data science to know that things like Data Collection, Data Preparation, Data Exploration and Data Engineering can take up the largest amount of your time on such a project. So, we wanted a use case and data set that required all of this.\n\nAnd so, we decided to see how well we can predict airline flight cancellations if we include weather data with historical flight data. This required all of the things we were looking for, but also ended up including something we didn\u2019t think about ahead of time: the fact that this data is heavily imbalanced. Specifically, of all the flights in the data set only a small percentage are actually cancelled.\n\nThis fact pushed us to a greater understanding of how to deal with heavily imbalanced classes in our data. First is that for this problem, \u201caccuracy\u201d is a terrible measure. Just predicting that the flight won\u2019t be cancelled will give us great accuracy, but isn\u2019t a good model. We needed to look to measures like the Confusion Matrix, Precision, Recall and ROC Curves. Next, we wanted to try different algorithms and techniques like oversampling and undersampling, penalizing wrong classification of our rare class, and a few other things like the SMOTE algorithm. Heavily imbalanced data makes analysis difficult, but we realized that it\u2019s also pretty common with real-life scenarios.\n\nWe decided that running our analysis for every airport in the world would be too big in scope. Even limiting to airports in the USA would be more than needed for our project. So we decided to limit to the top 10 airports most affected by weather. That left us with a manageable amount of data\u200a\u2014\u200aand we suspected the data itself would be less imbalanced. A quick search gave us this site, 10 Most Weather-Delayed U.S. Major Airports, and the 10 airports we would use.\n\nTo get US flight data, we used this United States Department of Transportation site, whose filters let us isolate the features we wanted. Unfortunately, the site can only deliver data for a particular month at a time. So, we had to gather twelve separate files for the twelve months of 2016, which increased the complexity of the data engineering since we had to first merge the 12 data files and then filter out all but the 10 desired airports. Not difficult, but a real-world task. The twelve files held over 5 million records so it wasn\u2019t something that could be done in Excel.\n\nNext, we used The Weather Company API to get historical weather data for those 10 airport sites, for 2016. Our plan was to combine these two data sources as a part of the data preparation and data engineering.\n\nOur goal for this use case was to come up with an exercise for creating a machine learning model using a few different platforms.\n\nIn the next post in the series, we\u2019ll use IBM\u2019s SPSS Modeler, which is ideal for beginners because of its visual graphical interface, many different machine learning algorithms including one that finds the best machine learning algorithm to use, and easy ways to explore, prepare and transform data.\n\nIn the third post, we\u2019ll try replicating our efforts using IBM\u2019s DSX Cloud platform with Watson Machine Learning (WML). Creating a Jupyter notebook using the Python programming language might give us more flexibility in code versus the GUI interface in SPSS. Admittedly, it\u2019s also likely a harder task if you\u2019re not a wizard with Python, so it could take a bit longer. WML is still in beta but we\u2019ll see what it can do.\n\nFor the final post, we\u2019ll try converting the SPSS model we did first into a \u201cflow\u201d\u200a\u2014\u200awhich is a new capability coming soon to IBM\u2019s DSX, which provides SPSS Modeler capabilities directly within DSX. Trying to recreate our original SPSS model using a flow in the cloud should prove interesting.\n\nTo be clear, we aren\u2019t trying to create a production quality model. That would require a lot more work and time. Instead we want to create something that works reasonably well and that can be done using the different platforms described. At the same time, with a little more work and expertise, the project could possibly be tuned to the point of being production quality. If so, we can imagine hotels using it to generate real-time advertising in the airports where it predicts flights will be cancelled. Or Uber might use it to gear up more cars for stranded passengers. Or perhaps the airport itself could use the model to prepare better for cancellations, and offer better experiences for flyers. Let us know of any other ideas that come to mind.\n\nStay tuned for part 2 where we\u2019ll talk about our process for IBM SPSS Modeler."},
{"url": "http://www.cryptocurrencymarkets.net/mastercard-pays-back-in-coins/", "link_title": "Cryptocurrency News: Mastercard Pays Back in Coins", "text": "The giant credit company just joined the cryptocurrency world by making a patent application to the United States Patent and Trademark Office (USPTO). The title of the patent is \u201cInformation Transaction Infrastructure,\u201d was filed on sometime in January, but it took until August 3rd for USPTO to approve the application and publish it on their website.\n\nThe primary purpose of these patent is to explore ways to refund cryptocurrency users. The patent also shows Vladimir Goloshchuk, a former senior analyst at Mastercard and current CEO of Brightest Minds research firm, as the only person involved in the invention of this new system. It is unclear what will Mr Goloshchuk\u2019s involvement will be with the credit card giant now that he is no longer associated with the company.\n\nRegardless of the relation between the creator and Mastercard, the patent brings forward an interesting concept. The application explains the working of an infrastructure in which users can verify their identities through the creation of a new wallet. The wallet will work as an account that is linked to the coin address they elect to reveal. The reason for Mastercard to create this system is pretty simple, refunds.\n\nIf someone is making a payment from a crypto wallet associated with an exchange, mining pool, cloud mining, or any other related service, their funds might be combined with payments from other customers for simplicity. That means that if a merchant has to return the money, the exchange or website will be forced to look for the funds\u2019 origin. That is, mainly, so the company where the wallet resides can guarantee that their services are not financing money laundering or any other illicit activity. This method creates a giant inconvenience and longer delays.\n\nHere is where Mastercard proposal comes into play. There will be a shared service that will smooth the refund process and shorten delays. A Mastercard representative talked with Coindesk and explained it in further detail.\n\n\u201cThe basic principle of the arrangement is that a user of the shared wallet service has two types of wallet. Firstly, they have a \u2018public\u2019 wallet for on-the-chain publicly visible and verified transactions. The user will make and receive cryptocurrency payments external to the shared wallet service using a public wallet. Using this approach the refund problem can be addressed \u2013 payment received from the public wallet can be refunded by an equal payment back to the public wallet.\u201d\n\nMastercard wants to be a synonym for cryptocurrency. This patent is the latest in a series of moves the company has made in the past years. Unlike its rivals, Visa and AMEX, Mastercard has acknowledged the value and the power of cryptocurrency. Their commitment is so great that the credit card giant has developed several projects which focus on blockchain technology. Last fall, Mastercard became the first of the big 3 to release a set of dedicated application program interfaces (API) that deal with the blockchain."},
{"url": "https://www.theatlantic.com/magazine/archive/2017/09/innocence-is-irrelevant/534171/?single_page=true", "link_title": "Innocence Is Irrelevant", "text": "It had been a long night for Shanta Sweatt. After working a 16-hour shift cleaning the Tennessee Performing Arts Center, in Nashville, and then catching the 11:15 bus to her apartment, she just wanted to take a shower and go to sleep. Instead, she wound up having a fight with the man she refers to as her \u201cso-called boyfriend.\u201d He was a high-school classmate who had recently ended up on the street, so Sweatt had let him move in, under the proviso that he not do drugs in the apartment. Sweatt has a soft spot for people in trouble. Over the years, she had taken in many of her two sons\u2019 friends, one of whom who had been living with them since his early teens. Listen to the audio version of this article: Feature stories, read aloud: download the Audm app for your iPhone.\n\nWhen Sweatt got home that night, early in November of last year, she realized that her boyfriend had been smoking marijuana, probably in front of the kids. She was furious, words were exchanged, and he left. Sweatt finally crawled into bed after midnight, only to be awakened at about 8:30 in the morning by an insistent knock at the door. She assumed that her boyfriend was coming to get his stuff and get out of her life. When she opened the door, police officers filled the frame, and more were waiting at her back door. She could see that squad cars were swarming the parking lot. \u201cThere were 12 to 15 cars,\u201d she told me. \u201cFor us.\u201d An officer asked whether they could enter. As a resident of public housing, she wasn\u2019t sure whether she had the right to say no. (She did.) But she was certain that if she refused them, they would come back. She had nothing to hide, so she let them in. \u201cI didn\u2019t get smart or give them a rough time,\u201d she said. \u201cI cooperated.\u201d Sweatt, who is black, didn\u2019t know what had led the police to her door. Their report says a complaint had been made about drug dealing from the apartment. After entering, they began systematically searching her apartment. One officer yanked open a junk drawer in her bedroom dresser, and inside he found small baggies of marijuana, containing a total of about 25 grams\u2014a weight equivalent to about six packets of sugar. There was also marijuana paraphernalia in the apartment. When the officer showed the baggies to her, Sweatt immediately knew they had to belong to her boyfriend, who\u2014in addition to having just been smoking in her home\u2014had past drug convictions.\n\nSweatt, 36 years old, left high school in 11th grade, but she has the kind of knowledge of the law that accrues to observant residents of James A. Cayce Homes, a housing project in East Nashville. \u201cI\u2019m the lease owner,\u201d she told me. \u201cWhatever was there, I would get blamed.\u201d It seemed useless to her to say that the drugs must have belonged to her absent boyfriend, who had a common name and no fixed address. She believed that this would result in the police pinning the crime on her sons. Her 17-year-old was at school, but her 18-year-old, who worked on the cleaning crew with her, was home, along with the friend of his who lived with them. Sweatt told me, \u201cI\u2019ve seen that where I lived: The parents said no, so everyone in the house gets charged. I\u2019m not going to let my children go down for someone else\u2019s mistake. A parent should take ownership of what happens in the house.\u201d So she made a quick and consequential decision. To protect her sons, she told the police that the marijuana belonged to her. \u201cI said it was mine, and me and my homegirls were going on vacation to California. I said we were going to take the marijuana with us\u2014I heard it was legal there\u2014and we were going to smoke for a week or two, then come back to normal life.\u201d Sweatt told me this two months after her arrest. She and I were sitting in a conference room at the Metropolitan Public Defender\u2019s Office, in downtown Nashville. She was dressed for work in a black sweatshirt, sweatpants, and sneakers. A large ring of keys attached to her belt bespoke her responsibilities as a janitorial supervisor at the arts center, just a few blocks away. I asked how she had come up with such a specific story on the spot. \u201cIt\u2019s a dream,\u201d she said. \u201cI heard California is more lively, more fun, than Nashville. The beaches are pretty. The palm trees.\u201d For a moment she looked as if she could actually see the surf. She was born and raised in East Nashville and has spent almost her entire life within the same few square miles. She had no plans to vacation in California, or anywhere else. \u201cAll I do is work and take care of my sons,\u201d she said. Some 97 percent of federal felony convictions are the result of plea bargains. The police seemed to believe her story (the arrest warrant noted her upcoming trip) and drove her downtown, where they put her in a holding room. By 1 o\u2019clock that afternoon, her bail had been set at $11,500. To be released, she needed to get $1,150 to a bail bondsman. She contacted a friend, and they each paid half. (\u201cThat\u2019s gone,\u201d she says.) She assumed she\u2019d be out in time to get to work that evening, but the money didn\u2019t clear until almost nine, minutes before she was to be sent to jail in shackles. A court date was set for January. Sweatt was facing serious charges with serious consequences, and she was advised to get an attorney. The fallout began even before the court rendered judgment in her case. Under the rules of the housing agency, her arrest prompted her eviction, which scattered her family. Sweatt moved into a cheap motel, and her sons moved in with her mother, although she still managed to see them every day. She tried to get enough money together to hire what she calls \u201ca regular lawyer,\u201d meaning a private attorney, but failed. So in January she turned to the public defender\u2019s office\u2014a choice that many people in her situation make reluctantly. That\u2019s because of the common misperception, I was told by Dawn Deaner, the head of the office, that public defenders are nothing more than \u201cpublic pretenders\u201d who are \u201cpaid to plead [their clients] guilty.\u201d\n\nSweatt\u2019s case was assigned to a lawyer named Ember Eyster. At their first meeting, Sweatt felt reassured. As she put it to me, \u201cEmber wears a dress that says, I\u2019m going to take you down!\u201d During their 75-minute discussion, Eyster asked Sweatt what her goals were, and Sweatt responded with a big one: no incarceration. She couldn\u2019t bear the idea of being away from her boys. At Eyster\u2019s request, Sweatt gathered her time sheets from work and dropped them off at Eyster\u2019s office. Eyster planned to use them as evidence that Sweatt was too busy mopping the floors at the arts center day and night to be a drug trafficker. The next time Eyster and Sweatt saw each other was two weeks later, in court. Sweatt had been charged with a Class D felony, which carried a two-to-12-year prison sentence, and a misdemeanor related to the paraphernalia. Exactly what punishment she would face depended largely on how the district attorney\u2019s office weighed several factors. First, there was her confession. Second, there was the police account of the circumstances of the arrest. Third, there was the fact that she lived within 1,000 feet of an elementary school, which meant it was possible that the charges against her would be \u201cenhanced.\u201d Finally, there was the fact that she already had a criminal history. In years past, she had pleaded guilty to several minor misdemeanors (most for driving with a suspended license) and one felony. The felony conviction resulted from her involvement in a 2001 robbery at a Jack in the Box. As Sweatt tells it, friends had discussed committing a robbery at the restaurant, where she worked, and then surprised her by actually carrying one out. She was arrested and pleaded guilty to a charge of \u201cfacilitation,\u201d and in exchange got three years of probation. \u201cI have never gotten into trouble since,\u201d she told me, \u201cexcept for driving without a license.\u201d She now relies on the bus. Sweatt embraced her attorney and wept with joy. Then she stood before the judge and pleaded guilty to a crime she says she did not commit. Eyster believed that Sweatt was innocent of the drug charges against her. \u201cThis is a hardworking woman who lived in a heavily policed community for 10 years,\u201d she told me. \u201cIf she were a drug dealer, she would have already been evicted. She doesn\u2019t have a history of drug use.\u201d But the idea of taking this case to trial was a nonstarter. The best path forward, Eyster decided, was to humanize Sweatt to the prosecutor\u2014hence those time sheets\u2014and then try to negotiate a plea bargain. In exchange for a guilty plea, the prosecutor might not recommend a prison sentence.\n\nThe strategy worked. The prosecutor reduced the charge from a felony to a Class A misdemeanor and offered Sweatt a six-month suspended sentence (meaning she wouldn\u2019t have to serve any of it) with no probation. Her paraphernalia charge was dismissed, and her conviction would result in a fine and fees that totaled $1,396.15. Upon hearing the news, Sweatt embraced Eyster and wept with joy. Then she stood before the judge and pleaded guilty to a crime she says she did not commit. This is the age of the plea bargain. Most people adjudicated in the criminal-justice system today waive the right to a trial and the host of protections that go along with one, including the right to appeal. Instead, they plead guilty. The vast majority of felony convictions are now the result of plea bargains\u2014some 94 percent at the state level, and some 97 percent at the federal level. Estimates for misdemeanor convictions run even higher. These are astonishing statistics, and they reveal a stark new truth about the American criminal-justice system: Very few cases go to trial. Supreme Court Justice Anthony Kennedy acknowledged this reality in 2012, writing for the majority in Missouri v. Frye, a case that helped establish the right to competent counsel for defendants who are offered a plea bargain. Quoting a law-review article, Kennedy wrote, \u201c\u2009\u2018Horse trading [between prosecutor and defense counsel] determines who goes to jail and for how long. That is what plea bargaining is. It is not some adjunct to the criminal justice system; it is the criminal justice system.\u2019\u2009\u201d\n\nIdeally, plea bargains work like this: Defendants for whom there is clear evidence of guilt accept responsibility for their actions; in exchange, they get leniency. A time-consuming and costly trial is avoided, and everybody benefits. But in recent decades, American legislators have criminalized so many behaviors that police are arresting millions of people annually\u2014almost 11 million in 2015, the most recent year for which figures are available. Taking to trial even a significant proportion of those who are charged would grind proceedings to a halt. According to Stephanos Bibas, a professor of law and criminology at the University of Pennsylvania Law School, the criminal-justice system has become a \u201ccapacious, onerous machinery that sweeps everyone in,\u201d and plea bargains, with their swift finality, are what keep that machinery running smoothly. Because of plea bargains, the system can quickly handle the criminal cases of millions of Americans each year, involving everything from petty violations to violent crimes. But plea bargains make it easy for prosecutors to convict defendants who may not be guilty, who don\u2019t present a danger to society, or whose \u201ccrime\u201d may primarily be a matter of suffering from poverty, mental illness, or addiction. And plea bargains are intrinsically tied up with race, of course, especially in our era of mass incarceration. As prosecutors have accumulated power in recent decades, judges and public defenders have lost it. To induce defendants to plead, prosecutors often threaten \u201cthe trial penalty\u201d: They make it known that defendants will face more-serious charges and harsher sentences if they take their case to court and are convicted. About 80 percent of defendants are eligible for court-appointed attorneys, including overworked public defenders who don\u2019t have the time or resources to even consider bringing more than a tiny fraction of these cases to trial. The result, one frustrated Missouri public defender complained a decade ago, is a style of defense that is nothing more than \u201cmeet \u2019em and greet \u2019em and plead \u2019em.\u201d\n\nAccording to the Prison Policy Initiative, 630,000 people are in jail on any given day, and 443,000 of them\u201470 percent\u2014are in pretrial detention. Many of these defendants are facing minor charges that would not mandate further incarceration, but they lack the resources to make bail and secure their freedom. Some therefore feel compelled to take whatever deal the prosecutor offers, even if they are innocent. Writing in 2016 in the William & Mary Law Review, Donald Dripps, a professor at the University of San Diego School of Law, illustrated the capricious and coercive nature of plea bargains. Dripps cited the case of Terrance Graham, a black 16-year-old who, in 2003, attempted to rob a restaurant with some friends. The prosecutor charged Graham as an adult, and he faced a life sentence without the possibility of parole at trial. The prosecutor offered Graham a great deal in exchange for a guilty plea: one year in jail and two more years of probation. Graham took the deal. But he was later accused of participating in another robbery and violated his probation\u2014at which point the judge imposed the life sentence. Beyond the age of mass incarceration\n\nRead more What\u2019s startling about this case, Dripps noted, is that Graham faced two radically different punishments for the same crime: either be put away for life or spend minimal time behind bars in exchange for a guilty plea. In 2010, the Supreme Court ruled, in Graham v. Florida, that the punishment Graham faced at trial was so cruel and unusual as to be unconstitutional. The Court found that a juvenile who did not commit homicide cannot face life without parole. Thanks in part to plea bargains, millions of Americans have a criminal record; in 2011, the National Employment Law Project estimated that figure at 65 million. It is a mark that can carry lifetime consequences for education, employment, and housing. Having a record, even for a violation that is trivial or specious, means a person can face tougher charges and punishment if he or she again encounters the criminal-justice system. Plea bargaining has become so coercive that many innocent people feel they have no option but to plead guilty. \u201cOur system makes it a rational choice to plead guilty to something you didn\u2019t do,\u201d Maddy deLone, the executive director of the Innocence Project, told me. The result, according to the late Harvard law professor William J. Stuntz, who wrote extensively about the history of plea bargains in The Collapse of American Criminal Justice (2011), is a system that has become \u201cthe harshest in the history of democratic government.\u201d\n\nTo learn more about how plea bargaining works in America today, I went to Nashville, where Shanta Sweatt entered her plea. A blue county in a red state, Davidson County, which includes Nashville, has a population of about 680,000. According to District Attorney Glenn Funk, Nashville\u2013Davidson County handles about 100,000 criminal cases a year, 70 percent of which are misdemeanors, 30 percent felonies. Last year, attorneys in the public defender\u2019s office dealt with 20,000 misdemeanors and 4,900 felony cases. Of all the defendants processed in Nashville\u2013Davidson County last year, only 86 had their cases resolved at trial. During my week in Nashville, I attended hearings at the courthouse on a full range of cases. I sat in on the plea discussions between an assistant district attorney and two public defenders. I observed a public defender in conversation with jailed defendants facing felony charges. I saw justice meted out courtroom by courtroom, often determined in part by the attitude, even the mood, of the prosecutor. My experience may not have been representative, but over the course of five days, I saw few defendants who had harmed someone else. Those who were facing felony charges had been arrested for drug offenses; some were clearly addicts with mental-health problems. I started with the misdemeanor-citation docket, which covers the lowest-level offenses. The defendants on the courtroom benches were white, black, and Latino. Sartorial guidelines were posted on the doors: no \u201csee-through blouses,\u201d no \u201cexposed underwear,\u201d no \u201csagging pants.\u201d Ember Eyster, Shanta Sweatt\u2019s attorney, was at the courthouse, but very few of the defendants in court that day had requested the services of a public defender or were accompanied by a lawyer.\n\nMisdemeanors are lesser offenses than felonies and are supposed to result in limited penalties. In Tennessee, Class A misdemeanors are sometimes referred to as 1129s: convictions that carry a maximum sentence of 11 months and 29 days. Many people convicted of misdemeanors are given probation or a suspended sentence or simply \u201ctime served\u201d\u2014that is, the amount of time they spent waiting in jail for their case to be heard because they couldn\u2019t make bond. The most-minor offenses can result in being required to take a class or do community service. Getting put through the system often also means accruing fines, fees, and court costs, which in a single case can run to more than $1,000. The punishments are not designed to be severe, or to create long-lasting consequences. But for many people they do. Millions of people each year are now processed for misdemeanors. In a 2009 report titled \u201cMinor Crimes, Massive Waste,\u201d the National Association of Criminal Defense Lawyers described a system characterized by \u201cthe ardent enforcement of crimes that were once simply deemed undesirable behavior and punished by societal means or a civil infraction punishable by a fine.\u201d In Nashville, I was struck by how many people were in court because they had been picked up for driving with a suspended license. It\u2019s a common practice, I learned, for states to suspend the licenses of people who have failed to pay court costs, traffic fines, or child support. In 2011, for example, Tennessee passed a law requiring the suspension of licenses for nonpayment of certain financial obligations. Both Glenn Funk, who must enforce this law, and Dawn Deaner, the head of the public defender\u2019s office, agree that it\u2019s absurd, in part because the scheme is almost perfectly designed to prevent the outcome it seeks. If people stop driving when their licenses are suspended, they may no longer be able to reliably get to work, which means they risk losing their jobs and going deeper into debt. As a result, many people whose licenses have been suspended drive anyway, putting themselves in constant jeopardy of racking up misdemeanor convictions. It is common for defendants charged with such minor infractions to represent themselves, even if they don\u2019t understand the consequences of pleading guilty, and even if there might be some mitigating circumstances that an attorney could argue on their behalf. Plead guilty to enough suspended-license misdemeanors, and a subsequent charge can be a felony.\n\nFunk, who was elected in 2014, has stopped routinely jailing defendants arrested for driving with a suspended license. \u201cMost of the time, driver\u2019s licenses are revoked because of poverty,\u201d he told me. \u201cI want people to have a license. It gives them ownership in society.\u201d Deaner told me that about two-thirds of the people listed on the citation docket are on there because of a driver\u2019s-license violation. And once their names are on the docket, the system strongly encourages them to plead guilty. \u201cIt\u2019s a hamster wheel of bureaucracy,\u201d she said, \u201cthat does no one any good.\u201d Plea bargains didn\u2019t exist in colonial America. Law books, lawyers, and prosecutors were rare. Most judges had little or no legal training, and victims ran their own cases (with the self-evident exception of homicides). Trials were brief, and people generally knew one another. By the 19th century, however, our modern criminal-justice system was coming into its own: Professional prosecutors emerged, more defendants hired lawyers to represent them, and the courts developed more-formal rules for evidence. Trials went from taking minutes or hours to lasting days. Calendars became clogged, which gave judges an incentive to start accepting pleas. \u201cSuddenly, everybody operating inside the system is better off if you have these pleas,\u201d Penn\u2019s Stephanos Bibas told me. The advantages of plea bargains became even clearer in the latter part of the 20th century, after the Supreme Court, under Chief Justice Earl Warren, issued a series of decisions, between 1953 and 1969, that established robust protections for criminal defendants. These included the landmark Gideon v. Wainwright and Miranda v. Arizona decisions, the former of which guaranteed the Sixth Amendment right to counsel in felony cases (since expanded to some misdemeanor cases), and the latter of which required that police inform those in their custody of the right to counsel and against self-incrimination. The Court\u2019s rulings had the inevitable effect of making trials lengthier and more burdensome, so prosecutors began turning more frequently to plea bargains. Before the 1960s, according to William J. Stuntz, between one-fourth and one-third of state felony charges led to a trial. Today the figure is one-twentieth.\n\nPolice officers have wide discretion in deciding whether a person is breaking the law, and they sometimes arrest people for such offenses as sleeping in public and sitting too long on a bench. One case involved a woman whose crime seemed to have been, in the words of the officer who filed the report, \u201cwalking down the road around 1:30 a.m.\u201d with \u201cno legitimate reason.\u201d Casey told me before this meeting that she hoped to get all such cases dismissed. \u201cWalking down the street!\u201d she said. \u201cImagine if it was you.\u201d For many of the cases, the assistant D.A. was making her decision in less than a minute. It was justice dispensed at the pace of speed dating. Ember Eyster told me it\u2019s sometimes possible to get misdemeanor cases dismissed with a bit of investigation. Maybe a trespassing charge doesn\u2019t hold up, for example, because the property owner hadn\u2019t posted a no trespassing sign. But this takes time, and clients who can\u2019t make bond have to sit in jail until the job is done. It\u2019s a choice few are willing to make for the small chance of avoiding a conviction. Many clients tell Eyster as soon as they meet her that they want to plead guilty and get time served. The choice makes sense under the circumstances. But anybody who makes it is incurring a debt to society that\u2019s hard, sometimes impossible, to repay. Those with a conviction in the United States can be denied public housing, professional licenses, and student loans. Many employers ask whether job applicants have been convicted of a crime, and in our zero-tolerance, zero-risk society, it\u2019s rational to avoid those who have.\n\nPeople with a misdemeanor conviction who get picked up for another minor offense are more likely to face subsequent conviction\u2014and that, according to Issa Kohler-Hausmann, an associate professor of law and sociology at Yale, is part of a deliberate strategy. Kohler-Hausmann made this case in a provocative 2014 Stanford Law Review article, \u201cManagerial Justice and Mass Misdemeanors,\u201d about the rise of misdemeanor arrests in New York City, which occurred even as felony arrests fell. Authorities, she argued, tend to pay \u201clittle attention\u201d to assessing \u201cguilt in individual cases.\u201d Instead, they use a policy of \u201cmass misdemeanors\u201d to manage people who live in \u201cneighborhoods with high crime rates and high minority populations.\u201d These defendants, she wrote, are moved through the criminal-justice system with little opportunity to make a case for themselves. They are simply being processed, and the \u201cmode of processing cases\u201d is plea bargaining. (This year, New York City settled a federal class-action lawsuit against it for issuing hundreds of thousands of unjustified criminal summonses.) Sitting at the prosecutor\u2019s table that morning, I watched Todoran, Casey, and Geer read from the police reports and make deals. Such a ritual takes place, in one form or another, in the courts of each of the country\u2019s more than 3,000 counties, which make up what the Fordham University law professor John Pfaff has described in his book Locked In as \u201ca vast patchwork of systems that vary in almost every conceivable way.\u201d We know little about what happens in these negotiations. Trials leave copious records, but many plea bargains leave little written trace. Instead, they are sometimes worked out in hurried hallway conversations\u2014or, as I witnessed, in brief courtroom conferences. : He was lying across a sidewalk over a vent, because it was cold.\n\n : Dismiss it. You\u2019ve got to sleep somewhere.\n\n : This one is for standing in front of a liquor store.\n\n : Dismiss. For so many of these things, a few hours in jail is punishment enough.\n\n : This defendant was found in a car with marijuana and 0.7 grams of crack.\n\n : I guess we\u2019ll do time served.\n\n : This man was at Tiger Mart. He was warned to leave earlier, and then came back.\n\n : Thirty days suspended and stay away from Tiger Mart.\n\n : This case, an officer heard him yelling and cussing and arrested him by the rescue mission.\n\n : Dismiss.\n\n : This is my favorite\u2014the woman who was walking down the road.\n\n : Dismiss. For many of the cases, Todoran was making her decision in less than a minute. I felt I was watching justice dispensed at the pace of speed dating.\n\nTo break the cycle, the United States will need to address the disparity in funding for the two sides of its legal system. According to Fordham\u2019s John Pfaff, of the $200 billion spent on all criminal-justice activities by state and local governments in 2008, only 2 percent went to indigent defense. But the system needs more than just money, says Jonathan Rapping, who in 2014 won a MacArthur genius grant for his work as the founder of Gideon\u2019s Promise, which trains and supports public defenders around the country\u2014including those in Nashville. What\u2019s necessary, Rapping argues, is a new mind-set. Defenders need to push back against the assumption that they will instantly plead out virtually every client, rubber-stamping the prosecutor\u2019s offer. Ember Eyster did ultimately negotiate a plea bargain for Shanta Sweatt, but in doing so she pushed back, using all the tools at her disposal to ensure that Sweatt was not incarcerated. The U.S. should also reform the bail system. We are holding people in jail simply because they lack the funds to secure their own release. Making these sorts of changes would allow authorities at the federal, state, and local levels to allocate more resources to the underlying social problems that drive so many arrests. But reform will not be easy. Even though crime rates remain near historic lows nationally, Donald Trump\u2019s administration has professed a desire to return to the days of \u201claw and order.\u201d U.S. Attorney General Jeff Sessions has announced, for instance, that he wants federal prosecutors to use maximum possible charges for crimes and to enforce mandatory minimums, which would result in harsh plea bargains. Almost all crime is handled not by the federal government but by the states, but with both the president and the country\u2019s highest law-enforcement official inflaming public fears, advocates for change worry about the fate of the reform efforts set in motion during Barack Obama\u2019s administration.\n\nThe United States is experiencing a criminal-justice crisis, just not the one the Trump administration talks about. By accepting the criminalization of everything, the bloat of the criminal-justice system, and the rise of the plea bargain, the country has guaranteed that millions of citizens will not have a fair shot at leading ordinary lives. Before I left Nashville, I visited Shanta Sweatt at the Tennessee Performing Arts Center. It\u2019s an enormous building of glass and concrete with multiple stages. Sweatt gave me a tour that started in the basement. As we made our way to the upper floors and the theaters, she gestured toward the banks of restrooms that she has to keep sparkling. \u201cThirty-eight stalls for women,\u201d she said. \u201cThirty-eight stalls for men.\u201d Sweatt is still struggling with the consequences of her arrest. \u201cIf it weren\u2019t for my boys,\u201d she told me, \u201cI would have given up a long time ago.\u201d At the time of her arrest, she told her employers about her situation, and they rallied to support her. \u201cThey stood behind me. They said, \u2018I got prayers for you.\u2019\u2009\u201d Because she wasn\u2019t incarcerated, Sweatt was able to keep her job, and her dream is that one day she might be able to buy a house, which would allow her to live together again with her sons. In her mind\u2019s eye, the house has three bedrooms, two bathrooms, and a yard, and it promises her and her family privacy and freedom. \u201cPolice mess with you in the projects,\u201d she said. \u201cYou get off the bus, they follow you. They don\u2019t mess with you in a house. I want to live like an average Joe.\u201d"},
{"url": "https://www.cnbc.com/2017/08/07/who-would-be-willing-to-fly-in-a-pilotless-plane-hardly-anyone.html", "link_title": "Who would be willing to fly in a pilotless plane? Hardly anyone", "text": "With \"robo-taxis\" being tested without drivers in cities around the world, you knew it would only be a matter of time until someone suggested a commercial airplane could fly without a pilot. \n\n\n\nWhile there could be enormous cost savings for airlines, and perhaps consumers, would anyone want to take a flight without a pilot in the cockpit?\n\n\n\n No.\n\n\n\nAt least that's the conclusion reached by a new survey of 8,000 people in the U.S., Europe and Australia. UBS commissioned the survey in June and found just 17 percent would be willing to fly on a pilotless plane. By comparison, more than half said they would be unlikely to buy a ticket.\n\n\n\n Considering every person has their price where something is so cheap they can't pass up the offer, UBS asked consumers how much cheaper a pilotless flight ticket would need to be for them to get on board without a crew in the cockpit. \n\n\n\n\"Perhaps surprisingly, half the respondents said they would not buy the pilotless ticket even if it was cheaper,\" said UBS Evidence Lab report.\n\n\n\n The results come at a time when airlines around the world face a shortage of pilots due to the addition of flights and the fact many pilots are retiring. That combination is driving up costs and forcing airlines to pay more to keep pilots and to attract new ones.\n\nIn fact, airlines in China and the Middle East, where the airline industry is growing far faster than in Europe and the U.S., are readily paying hefty bonuses to hire pilots.\n\n\n\n UBS estimates the industry could save $35 billion and pass the savings along to passengers through lower fares if airlines could operate pilotless planes. The technology to do so could be developed by 2025, according to the report.\n\n\n\n Still, passengers and regulators would have to become comfortable with the idea of nobody sitting in the cockpit, which would be a dramatic change from the current rules for much of the world, which require at least two people in the cockpit at all times.\n\n\n\n UBS researchers admit cargo planes would be more likely than commercial airlines to try pilotless flights.\n\n\n\n \"Unlike passengers, cargo is not concerned with the status of its pilots (human or autonomous). For this reason, pilotless cargo aircraft may happen more swiftly than for passengers,\" the report concluded."},
{"url": "https://www.theatlantic.com/photo/2017/08/upgrading-the-power-grid-in-remote-tibet/536121/?single_page=true", "link_title": "Upgrading the Power Grid in Remote Tibet", "text": "In Tibet\u2019s Hengduan Mountains, workers are hauling thousands of parts over challenging terrain by mule and by hand to build huge transmission towers for the Tibet Electric Power Networking Project. The complex power transmission project is designed to join and upgrade Tibet\u2019s current disconnected and underpowered power transmission systems. The project is due to be completed in 2018."},
{"url": "https://www.theatlantic.com/health/archive/2017/08/ice-cream-military/535980/?single_page=true", "link_title": "How Ice Cream Helped America at War", "text": "In 1944, a Warner Bros. cartoon euphemized World War II through Bugs Bunny and ice cream. Marooned in the Pacific under Japanese attack, Bugs commandeers an ice-cream truck and begins handing out \u201cGood Rumor\u201d bars, which turn out to be chocolate-covered grenades. The bars explode, and Bugs drives off. \u201cBusiness is booming,\u201d he cracks. There\u2019s a lot wrong with this infamous cartoon. The dialogue includes racial epithets and the animated Japanese soldiers are depicted as yellow-faced. One thing it does get right, though, is the notion of ice cream \u201cbooming\u201d as America\u2019s secret weapon during the war. Ice cream in fact played a significant role in the nation\u2019s wartime efforts\u2014and would be used for support in the military-industrial complex for decades. Before World War II, the military\u2019s food concerns were largely relegated to ensuring that soldiers consumed enough calories to march (and that civilians and refugees consumed enough to endure). During the First World War, this was the job of Herbert Hoover, the first administrator and wartime consigliere of the U.S. Food Administration. He succeeded on the platform that \u201cfood will win the war,\u201d persuading American households to \u201cHooverize\u201d meals by sacrificing wheat, sugar, meat, and fat (the origin of Meatless Mondays and Wheatless Wednesdays). The result was a rapid tripling of food exports, yielding more than 18 million tons of food staples for the war effort in America\u2019s first full year of war alone. The crew abandoned ship\u2014but not before breaking into the freezer and eating all the ice cream. But the ice-cream industry, still in its infancy, demanded even more for the boys overseas: not just calories, but comfort. An editorial in the May 1918 issue of The Ice Cream Review, a monthly trade magazine, spooned out sharp criticism for the scant availability of ice cream overseas: \u201cIf English medical men knew what ours do every hospital would keep ice cream on hand for patients.\u201d It cried for Washington to intervene by subsidizing Allied ice-cream factories throughout Europe: In this country every medical hospital uses ice cream as a food and doctors would not know how to do without it. But what of our wounded and sick boys in France? Are they to lie in bed wishing for a dish of good old American ice cream? They are up to the present, for ice cream and ices are taboo in France. It clearly is the duty of the Surgeon General or some other officer to demand that a supply be forthcoming. The ice-cream industry didn\u2019t have much lobbying power. Few Americans had refrigeration. Worse, Hoover had downplayed the scarcity of domestic sugar supplies, hoping to avoid a panic. There was hardly any sugar left for America, let alone for allies in France and England\u2014and the promotion of ice cream as a wartime cure-all wasn\u2019t helping. Instead of bolstering ice cream production, Hoover\u2019s Food Administration ordered a reduction of manufacturing domestically\u2014ruling in the summer of 1918 that \u201cice cream is no longer considered so essential as to justify the free use of sugar in its manufacture.\u201d\n\nThat stance would change drastically during the next two decades, however\u2014owing partially to the unlikely contributions of Prohibition and the Great Depression. When the 18th Amendment outlawed the sale of spirits in 1920, many early American breweries, including Yuengling and Anheuser-Busch, turned to soda and ice cream to stay afloat. By the end of the decade, Americans were consuming more than a million gallons of ice cream per day\u2014and, crucially, associating it with the comfort and diversion formerly assigned to alcohol. The ice-cream maker William Dreyer helped further this sentiment in 1929 when he marketed Rocky Road as a culinary metaphor aimed at helping people cope with the crash of the stock market. The term now refers to just chocolate with chopped nuts and chunks of marshmallow, but it used to be symbolic of comfort\u2014a sweet indulgence juxtaposed with broken, \u201crocky\u201d pieces. When World War II hit, countries on either side of the conflict once again banned ice cream, with Britain adding insult to injury by endorsing carrots on sticks as a wartime substitute. But the United States doubled down. Ice cream had become inseparable from the American way of life\u2014and, from that point forward, from military tactics. In 1942, as Japanese torpedoes slowly sank the U.S.S. Lexington, then the second-largest aircraft carrier in the Navy\u2019s arsenal, the crew abandoned ship\u2014but not before breaking into the freezer and eating all the ice cream. Survivors describe scooping ice cream into their helmets and licking them clean before lowering themselves into the Pacific. By 1943, American heavy-bomber crews figured out they could make ice cream over enemy territory by strapping buckets of mix to the rear gunner\u2019s compartment before missions. By the time they landed, the custard would have frozen at altitude and been churned smooth by engine vibrations and turbulence\u2014if not machine-gun fire and midair explosions. Soldiers on the ground reported mixing snow and melted chocolate bars in helmets to improvise a chocolate sorbet. George Washington spent about $200 on ice cream in a single summer. The U.S. Navy spent $1 million in 1945 converting a concrete barge into a floating ice-cream factory to be towed around the Pacific, distributing ice cream to ships incapable of making their own. It held more than 2,000 gallons of ice cream and churned out 10 gallons every seven minutes. Not to be outdone, the U.S. Army constructed miniature ice-cream factories on the front lines and began delivering individual cartons to foxholes. This was in addition to the hundreds of millions of gallons of ice-cream mix they manufactured annually, shipping more than 135 million pounds of dehydrated ice cream in a single year\n\nThe cherry on top came a decade later during the Korean War, when General Lewis B. Puller tried to convince the Pentagon that ice cream was a \u201csissy food\u201d and that troops would be tougher if indulged with other products of American culture, namely beer and whiskey. The Pentagon responded with an official statement ensuring soldiers were served ice cream a minimum of three times a week. Beyond its military \u201cboom,\u201d America\u2019s comfortable connotation of ice cream goes back to its founding. George Washington spent about $200 on ice cream in a single summer\u2014more than $5,000 in today\u2019s dollar\u2014and Thomas Jefferson studied ice cream production in France before returning to Monticello with a sorbeti\u00e8re, four ice-cream molds, and a handwritten recipe for vanilla ice cream that\u2019s still archived in the Library of Congress. Immigrants to Ellis Island were traditionally fed ice cream as part of their first American meal\u2014a gesture ordered by the island\u2019s commissioner and preserved in a headline from the summer of 1921: \u201cEllis Island Authorities Gently Lead Immigrants to Appreciation of Good Points of America by Introducing Them to the Pleasures of Ice Cream Sandwiches.\u201d So why ice cream? In her book Much Depends on Dinner: The Extraordinary History and Mythology, Allure and Obsessions, Perils and Taboos of an Ordinary Meal, Margaret Visser suggests that there are two types of ice cream-induced nostalgia. The first is for childhood memories, which \u201cmakes people feel young and at least temporarily secure and innocent,\u201d and the second is more complex: what she calls a nostalgia \u201cfor Elsewhere.\u201d For some, that might mean memories of summer vacations or seaside boardwalks\u2014or, for fans of H\u00e4agen-Dazs, images of Scandinavian luxury, even though the name and umlaut are gibberish. An Austrian study on the neurological effects of food seems to support this. Researchers found that only ice cream lowered the human startle response in men and women (at least when ingested by syringe), whereas chocolate and yogurt did not produce statistically significant outcomes across genders. This indicates that the comfort of ice cream runs deeper than the physiological effects of sugar, fat, temperature, and perceived sweetness. The phenomenon, it appears, is largely psychological, a result of the learned associations pairing ice cream with childhood birthday cakes; first dates; and, for soldiers on deployment, the comfortable \u201celsewhere\u201d of home."},
{"url": "https://www.infoq.com/articles/high-performance-dotnet", "link_title": "High Performance Application in .NET", "text": "Interest about performance in .NET have increased recently with the advent of .NET Standard and new platforms opened to .NET applications. .NET is not traditionally a platform recognized for writing high performance applications; it is instead generally associated to enterprise applications.\n\nThere are several trends over the past years changing the landscape significantly. First and foremost, .NET Standard opens the door for a wide range of platforms such as mobile phones and IoT devices. These devices offer new challenges, being much more resource constrained than desktops and servers.\n\nMeanwhile, being able to run server applications on Linux represents both a significant opportunity and uncharted territory for developers. .NET Core has yet to accumulate success stories for running high performance applications. Several new trends are also picking up steam in the .NET ecosystem: microservices, containers and serverless. Each brings their own set of performance requirements.\n\nInfoQ: From your experience, where is .NET at on the performance aspect? How does it compare to the other mainstream platforms?\n\nMaoni Stephens: GC is without a doubt one of the most mentioned areas when talking about performance. Since I work on it, my answers on this panel will be mostly focused around the context of the GC. While some folks might have the impression that .NET was associated with high productivity and not high performance, there have been many products written on .NET that have very high performance requirements. To make sure our GC can handle these requirements, before we ship a significant feature we always test it with our internal partner teams that handle some of the most stressful workloads in the world like Bing or Exchange. This way we don\u2019t only rely on micro/macro benchmarks which, while very useful, can be seriously lacking in properly showing the performance in real world scenarios. The .NET GC has been highly tuned for more than a decade and is a very competitive GC compared to the ones in other mainstream platforms. Of course, GC is only part of the framework. When someone talks about GC performance, they really mean \"How this GC handles the workload I give to it\". Naturally, if a workload puts a lot of stress on a GC, it means longer and/or more frequent GC pauses will likely occur than another workload that makes it not so stressful for that GC. And historically the way the .NET framework was implemented and how folks tend to write code in .NET can pretty easily put very much stress on our GC. While we always keep making improvements to our GC so it can handle more types of workloads and more stressful ones, folks who have high performance requirements always need to measure and understand what factors are affecting performance in their applications. This is the case no matter what framework/libraries you use; in other words, you do need to understand how your workload behaves if you want to achieve and maintain high performance. This blog entry explains comparing only the GC performance if you are interested in that. The fact that the GC is part of a mature framework does present challenges that newer frameworks may not have. We already have a lot of libraries written and in use for a long time, and we generally try very hard not to break existing scenarios with new features. And if we must because we are providing something that\u2019s used only by a small percentage of our users who are willing to put in more effort for high performance, we still want to make it as safe as possible as that\u2019s one of the main qualities of our framework. I\u2019ve seen some of the newer frameworks advocating features that sound performant but really put unrealistic reliability burden on the users. That\u2019s not the approach we want to take. Our customers are always asking for more performance from us because they are pushing the limit on their side. I\u2019ve heard many folks who used to use .NET say that they really missed it when they had to switch to another framework for reasons like job changes. So I am very excited that now .NET is running on more OSs. As mentioned above, due to the way .NET libraries and applications were/are written, they do tend to generate a lot of work for the GC so I believe we need to keep improving both the GC performance and tooling that helps our users in measuring perf in the foreseeable future. At the meantime, we are also looking at investing in more features that can reduce pressure on the GC.\n\nAaron Stannard: Most of the \u201cperformance\u201d discrepancies come down to frameworks implemented on top of it. JavaScript is not an inherently \u201cfast\u201d language for a multitude of reasons, yet Node.JS smokes ASP.NET in all of the TechEmpower benchmarks. Why? Largely because ASP.NET is built on technology that is decades old, specific to Windows, and originally implemented using a synchronous design. Node.JS, on the other hand, was designed to be asynchronous from the start and incorporated many other performance-first choices from the beginning. There\u2019s nothing technically stopping .NET from doing the same, which is the raison d\u2019etre for Kestrel and ASP.NET Core. The C# language and the CLR itself are immensely fast, even though ASP.NET might be slow. The CLR supports value types, which are a huge advantage over the JVM when it comes to performance since we can have essentially free allocations for those types (up to a certain extent\u2026) The CLR has had many improvements made to it over the past several years that make it much smarter at runtime; for instance, the garbage collector has become very good about defragmenting memory as it collects garbage which helps speed up subsequent memory access significantly. The CLR JIT can also use profiling information to make bets about how best to compile a local function at runtime, such as whether to use a jump table or a simple if statement on a C# interface that has only a small number of implementations. The C# language itself also has the benefit of being able to give the developer more control over how the compiler behaves - you can provide directives with respect to inlining functions, you can pin memory off heap and out of range of the garbage collector, and you can even treat CLR allocated objects as unsafe and go back to C-style pointer arithmetic if needed. The point being, with C# and the CLR you get more flexibility with respect to performance tuning than any other managed language.\n\nInfoQ: What challenges did you face writing high performance applications in .NET?\n\nMaoni Stephens: GC is driven by allocations and survivals. And we want to help our framework assembly authors and customers find out info on these so they can in turn help make the GC\u2019s job easier. Over the years we talked about various ideas of presenting allocations to our customers. I remember at one point we talked about giving APIs \u201callocation scores\u201d but that didn\u2019t go anywhere simply because when you take different parameters or code paths in an API the allocations it does can be drastically different. We definitely have developed tools that help you with allocations (we have ETW events and tools that present those events in a meaningful way) but it would be really nice to be able to see how many bytes I\u2019ve allocated when I make an API call during development; or from point A to point B how many bytes I\u2019ve allocated myself. The runtime does provide this info but I have not seen an IDE exposing it. Another thing we worked on was a view in PerfView to help with seeing how older generation objects hold onto objects in younger generations, i.e., make them survive. This play a big part in ephemeral GC cost. I don\u2019t think we exposed this externally yet (but the code is in PerfView, search for CROSS_GENERATION_LIVENESS) as we didn\u2019t have time to make the implementation solid enough. This would allow you to find out fields of objects that you can set to null to let the GC know you are done with holding something alive when a GC happens naturally (i.e., not when you induce a GC yourself which doesn\u2019t reflect what inter-generational references GC sees accurately).\n\nAaron Stannard: In general I\u2019ve found that the underlying CLR itself is further behind on performance instrumentation than other platforms. This is one of the reasons why I wrote NBench, a performance testing framework for .NET applications. One example: the Java Runtime Environment exposes the Java Management Extensions (JMX) toolchain which makes it very easy for developers to access detailed performance statistics at runtime either via a GUI or via the JMX API. Many of the big management and monitoring tools for large scale JVM applications are built directly on top of this; DataStax OpsCenter for Apache Cassandra springs to mind as an example. You can\u2019t even have a conversation about \u201cgood,\u201d \u201cbad,\u201d or \u201cbetter\u201d performance without quantifying it and my experience doing this on the CLR has been painful, to say the least. The current built-in solutions for performance on .NET, Event Tracing for Windows (ETW) and Performance Counters, are a bit awkward to implement correctly and reliably at scale compared to the JMX. And you won\u2019t be able to use either of those on .NET Core as they\u2019re Windows-specific technologies, so as far as I know there\u2019s not much in the way of built-in options for monitoring .NET Core apps today. The other big challenge I had with building high-performance systems in .NET was understanding the performance of the other .NET libraries and components I depended on; since most developers don\u2019t know how to performance test or even performance profile their code you\u2019re taking a big risk anytime you include a third party library in your application. I remember installing a StatsD monitoring component in one of my large scale .NET applications (100s of millions of transactions per day) and the library spiked my CPU utilization by about 30%. Turns out it was doing some weird stuff with the underlying UDP socket and I had to patch it to bring the performance back inline with what we deemed acceptable.\n\nInfoQ: How well does .NET handle parallelism and concurrency? Is there room for major improvements?\n\nInfoQ: Does .NET Core bring any advantage over the desktop .NET framework at the performance level?\n\nInfoQ: What part does .NET Native play in the high performance area? In what scenarios can it outperform its managed counterpart?\n\nInfoQ: Several performance related features were recently added to the .NET languages, such as value tuples and ref returns. What feature would you like to see next?\n\nInfoQ: While high-performance is usually associated with server applications, other platforms such as mobile and IoT devices also exhibit unique performance requirements. Can .NET meet the requirements of these non-traditional platforms?\n\n.NET and its languages have evolved significantly performance wise since .NET 4.0, making long-standing assumptions obsolete. Aside from improvements on the GC, JIT and BCL, new additions such as .NET Native and the TPL widen the scenarios handled by .NET. Many more improvements on all front are planned or being implemented, warranting keeping an eye on updates and new benchmarks.\n\nBen Watson has been a software engineer at Microsoft since 2008. As a core developer of the Bing platform, he has been integral in building one of the world\u2019s leading .NET-based, high-performance server applications, handling high-volume, low-latency requests across tens of thousands of machines for millions of customers. He is passionate about performance and spends much of his time educating teams on best-practices in high-performance .NET. In his spare time, he enjoys geocaching, LEGO, reading, classical music, and spending time with his wife and children outdoors in the beautiful Pacific northwest. He is the author of the books Writing High-Performance .NET Code and C# 4.0 How-To.\n\nSasha Goldshtein is the CTO of Sela Group, a Microsoft C# MVP and Regional Director, a Pluralsight author, and an international consultant and trainer. Sasha is the author of \"Introducing Windows 7 for Developers\" (Microsoft Press, 2009) and \"Pro .NET Performance\" (Apress, 2012), a prolific blogger and open source contributor, and author of numerous training courses including .NET Debugging, .NET Performance, Android Application Development, and Modern C++. His consulting work revolves mainly around distributed architecture, production debugging and performance diagnostics, and mobile application development.\n\nMatt Warren is a C# dev who loves nothing more than finding and fixing performance issues. He's worked with Azure, ASP.NET MVC and WinForms on projects such as a web-site for storing government weather data, medical monitoring devices and an inspection system that ensured kegs of beer didn't leak! He\u2019s an Open Source contributor to BenchmarkDotNet and the CoreCLR. Matt currently works on the C# production profiler at CA Technologies and blogs at www.mattwarren.org.\n\nMaoni Stephens is the main developer of the .NET GC. During her spare time she enjoys reading GC papers and watching anime/animals. In general, she likes to make things she cares about more efficient.\n\nAaron Stannard is Founder and CTO at Petabridge, a .NET data platform company. Aaron's one of the original contributors to Akka.NET, authoring the Akka.Remote module and Helios, the underlying socket server. Prior to working at Petabridge Aaron founded MarkedUp Analytics, a Cassandra and Akka.NET-powered real-time app analytics and marketing automation company for app developers."},
{"url": "https://www.youtube.com/watch?v=R9UoFyqJca8", "link_title": "1976 Matrix Singular Value Decomposition Film", "text": "This film about the matrix singular value decomposition was made in 1976 at the Los Alamos National Laboratory. Today the SVD is widely used in scientific and engineering computation, but in 1976 the SVD was relatively unknown. A practical algorithm for its computation had been developed only a few years earlier and the LINPACK project was in the early stages of its implementation. The 3-D computer graphics involved hidden line computations. The computer output was 16mm celluloid film.\n\n --- Cleve Moler December 4, 2012"},
{"url": "https://talkpython.fm/episodes/show/124/python-for-ai-research", "link_title": "Python for AI research podcast with Vicarious", "text": "We all know that Python is a major player in the application of Machine Learning and AI. That often involves grabbing Keras or TensorFlow and applying it to a problem. But what about AI research? When you're actually trying to create something that has yet to be created? How do researchers use Python here?Today you'll meet Alex Lavin, a Python developer and research scientist at Vicarious where they are trying to develop artificial general intelligence for robots.Links from the show:"},
{"url": "https://medium.com/@chihhsuan_wu/the-quiet-change-happening-to-chinese-mobile-payment-services-creation-of-online-payment-clearing-2d7edaf0bd5d", "link_title": "The Quiet Change Happening to Chinese Mobile Payment Services\u200a\u2013\u200a (\u7f51\u94f6)", "text": "While most people in the US are just starting to understand Chinese mobile payment services like Alipay and WeChat Pay and its ecosystems via word of mouth from friends who traveled to China, occasionally from subtle mentioning in articles like this in the likes of USA Today. But you rarely see in depth analysis pieces like this from the New York Times when talking about the Alibaba (Alipay) and Tencent (WeChat Pay) services.\n\nThough most outside of China just starting to grasp the concept, using one of these Mobile Payment Services (MPS) is a fact of life for the 1+ billion people living in China. And it seems like now the Chinese Central Bank wants in on a piece of that, by creating an Online Payment Clearing House (\u7f51\u8054 in Chinese) in the name of tracking and monitoring the capital flow for safer payment online.\n\nThere are still very limited English coverage on this as of August 7th, 2017, with the exception by the Alibaba owned, Hong Kong based South China Morning Post, but it has garnered a lot of coverage from different Chinese websites. The Online Payment Clearing House will look a lot like the internet version of UnionPay\u2019s clearing service for bank-to-bank transactions, targeting transactions involving MPS and banks. Below is what it looks like with the change:\n\nSo why is this worth writing about? Online Payment Clearing House is going to be a company.\n\nFrom this image circulating on Weibo (China\u2019s hybrid of sites Facebook and Twitter), 45 institutions would be the shareholders of this company, and they break down in a very interesting way:\n\nJust on the list of the top 10 investors at face value (not including minority holdings) the Chinese government has 37% of the control.\n\nCreating the Online Payment Clearing House also enables the Chinese government access to the large amount of data that will be made available by the aggregation.\n\nHowever, for the two dominate MPS providers\u200a\u2014\u200aAlibaba\u2019s AliPay taking 54% of the market, while Tencent\u2019s WeChat Pay has 40%, this creates the ceiling for their mega finance ambitions. Marking them the biggest losers.\n\nDoes this mean other internationally accepted financial services companies like Visa/Mastercard can now make a play for the once dominated field? I guess time will tell.\n\nOne thing is very clear to me though is that the Chinese government governs in a specific way. You may be the darling today, but that could change overnight. The only thing constant is change."},
{"url": "http://danielsz.github.io/2017/08/07/Timed-idempotency", "link_title": "Timed idempotence with side-effects in Lisp style", "text": "A procedure with side-effects is idempotent if successive invocations don\u2019t repeat the side-effects beyond its initial run.\n\nThis is not the same as idempotency in the mathematical or functional programming sense, where computing a value is considered to be the primary effect and any other effects, well, side-effects.\n\nIdempotency is an overloaded term. In Mathematics, it is a function that satisfies f (f (x)) = f (x). or f (x) = |x| is idempotent , while or f (x) = x\u00b2 is not.\n\nIn computing, the focus shifts from functional composition to sequential composition. This makes sense. Operations often need to be repeated. The question becomes: if I run function f twice with the same input, will it:\n\nPure functions are defined through this lens. Not all pure functions are idempotent in the mathematical sense, but all pure functions are idempotent with regards to their return value. And so are eligible for memoization, an optimization technique in which the return value is computed once for a given input and cached henceforth.\n\nProcedures, on the other hand, are referentally opaque and much less amenable to optimization. It is up to the programmer to propose an implementation with idempotency guarantees.\n\nWhen implementing idempotent HTTP verbs (PUT or DELETE, for example), we provide an implementation that executes a side-effect on the first invocation but not on subsequent runs. Those implementations are often ad-hoc. Moreover, we often need to reconsider the desirability of a side-effect, and act accordingly.\n\nIn the following sections of this article, we will see how we can derive an abstraction from a specific use case. And at the very end, we will introduce a library that provides that abstraction ready for (re)use.\n\nConsider the following snippet of code:\n\nIf the command failed midway, we will need to resend the newsletter to some users. But who has gotten our newsletter, and who hasn\u2019t?\n\nSuppose was procedurally idempotent, we would be able to repeat the operation, and still ensure that our users receive one newsletter and not duplicates.\n\nBut one month later, with the latest newsletter hot off the press, nobody would receive it!\n\nWhat we want really is procedural idempotency for a period of time. should be procedurally idempotent in the interval between newsletter issues, but not beyond.\n\nAnother way to put it is that we want self-cancelling idempotency. Or think of it as cyclical idempotency. Or something along the lines of conditional idempotency, controlled idempotency or parametrisable idempotency. It doesn\u2019t really matter what you call it. To keep it simple, I call it timed idempotency.\n\nThere are numerous examples of this mode of operation in the physical world. Take an elevator, for example. When you press the button to the sixth floor, you can press the button again without fear of being catapulted to the twelth floor. After a roundtrip, the button becomes enabled again.\n\nHow do we emulate this type of functionality in our programs?\n\nOne way to do it is with a logbook. It looks like this:\n\nOver time, we will send many newsletters, and our logbook will look like this:\n\nA logbook is flexible because you can record all kinds of events that occured with regards to an entity.\n\nAt first glance, we can tell that we have sent our user a welcome email, two subscription reminders and three newsletters.\n\nWe can now write:\n\nOur newsletter is monthly, so the predicate tells us if it is time to send a new edition of our newsletter.\n\nOn the other hand, our welcome email gets sent only once, so an idempotent would look a little bit different:\n\nLet\u2019s standardize our predicates and make them operate uniformly on one logbook entry at the time:\n\nThis allows us to compose them neatly with logbook sequences.\n\nWe architecture our solution around events and predicates. In our example, the events are , and . Now let\u2019s look back at our newsletter sending operation.\n\nSomething is missing, isn\u2019t it? If the newsletter is sent successfully, we need to write an entry in the logbook.\n\nIf you have some experience writing applications, you probably recognize this pattern. And if you have read SICP, you may remember how we muster means of abstraction to generalize a problem. This allows us in turn to devise a solution for a whole class of related problems. What our solution lacks is generality.\n\nThis ad-hoc snippet really boils down to:\n\nWith proper design and the magic of macros, we can actually write the following:\n\nWe are left with the entity ( ), the body with the side-effect ( ) and the event (from which we derive the predicate). All the rest has been made implicit: how we retrieve the logbook for an entity, how we determine if the side-effect was successful, how we append events to the logbook.\n\nInstead of cluttering the calling site, the user makes those assumptions explicit via a configuration mechanism. From then on, the macro will do the reshuffle and the wiring behind the scenes.\n\nIt is useful to have that kind of functionality decoupled from application code because the need for timed idempotency pops up time and again. This is the value proposal of Benjamin, which I\u2019m open sourcing today. It\u2019s a small library, 9 lines of macro code and the same amount for a helper function. Quite Lispy in style."},
{"url": "https://energy.gov/sites/prod/files/NSF_Stemming%20the%20Tide%20Why%20Women%20Leave%20Engineering.pdf", "link_title": "Why Women Leave Engineering (2011) [pdf]", "text": ""},
{"url": "https://scupper.io", "link_title": "Show HN: Backup your SoundCloud account", "text": ""},
{"url": "https://6guts.wordpress.com/2017/07/02/optimizing-reading-lines-from-a-file/", "link_title": "Perl 6: Optimizing Reading Lines from a File", "text": "Reading lines from a file and processing them one at a time is a hugely common scripting task. However, to date our performance at this task has been somewhat underwhelming. Happily, a grateful Perl 6 fan stepped up in response to my recent call for funding, offering 25 hours of funding to work on whatever I felt was most pressing, but with a suggestion that perhaps I could look at some aspect of I/O performance. Having recently been working on refactoring I/O anyway, this was very timely. So, I crafted a benchmark and dug in.\n\nPerl 5 is considered to have very good I/O performance, so I figured I\u2019d use that as a rough measure of how close Perl 6 was to performing well at this task. A completely equivalent benchmark isn\u2019t quite possible, but I tried to pick something representative of what the average programmer would write. The task for the benchmark was to take a file with one million lines, each having 60 characters, loop over them, and add up the number of characters on each line. That number would then be printed out at the end (it\u2019s important that benchmarks calculating results return or consume the result in some way, as a sufficiently smart optimizer may otherwise manage to eliminate the work we think we\u2019re measuring). The rules were that:\n\nThe Perl 5 benchmark for this came out as follows:\n\nWith the Perl 6 one looking like this:\n\nI\u2019ll note right off that in Perl 6 there are likely ways, today, to do a bit better. For example, the variable could be given a native type, and it\u2019s entirely possible that a while loop might come out faster than the loop. Neither of those are representative of what a typical programmer looking at the documentation and diving in to implementing stuff would do, however. I suspect that Perl 5 experts could similarly point out some trick I\u2019ve missed, but I\u2019m trying to benchmark typical use.\n\nOne slight unfairness is that the Perl 6 solution will actually count the number of grapheme clusters, since strings are at grapheme level. This entails some extra processing work, even in the case that there are no multi-codepoint clusters in the input file (as there were not in this case). But again, the average user making comparisons won\u2019t much care for such technicalities.\n\nAll measurements were made on modern hardware with an Intel Xeon 6-core CPU and a fast SSD, and on Linux.\n\nAt the point I started work, the Perl 6 solution clocked in at 2.87s, to just 1.13s for Perl 5. This made Perl 6 a factor of 2.5 times slower.\n\nThe whole I/O stack recently got a good overhaul, and this was the first time I\u2019d looked at a profile since that work was completed. Looking at the output from immediately showed up some rather disappointing numbers. Of all callframes, 57.13% were JIT-compiled. Worse, basically nothing was being inlined.\n\nAt this point, it\u2019s worth recalling that Perl 6 is implemented in Perl 6, and that there\u2019s quite a bit going on between the code in the benchmark and ending up in either things implemented in C or a system call. The call to returns an object. Reading a line means calling the method on that . That in turn calls the method on a object, and that method is what actually calls down to the VM-backed decoder to read a line (so there\u2019s a level of indirection here to support user provided decoders). The return value of that method then has called on it to check we actually got a line back. If yes, then it can be returned. If not, then should be called in order to fetch data from the file handle (given buffering, this happens relatively rarely). Running the loop body is a further invocation, passing the read line as a parameter. Getting the count is also a method call (which, again, actually calls down to the VM guts to access the string\u2019s grapheme count).\n\nThat\u2019s quite a lot of method calling. While the VM provides I/O, decoding, and finding input up to a separator, the coordination of that whole process is implemented in Perl 6, and involves a bunch of method calls. Seen that way, it\u2019s perhaps not surprising that Perl 6 would come in slower.\n\nThere are, however, things that we can do to make it fast anyway. One of them is JIT compilation, where instead of having to interpret the bytecode that Perl 6 is compiled in to, we further translate it into machine code that runs on the CPU. That cuts out the interpreter overhead. Only doing that for 57% of the methods or blocks we\u2019re in is a missed opportunity.\n\nThe other really important optimization is inlining. This is where small methods or subroutines are taken and copied into their callers by the optimizer. This isn\u2019t something we can do by static analysis; the point of methods calls is polymorphism. It is something a VM doing dynamic analysis and type specialization can do, however. And the savings can be rather significant, since it cuts out the work of creating and tearing down call frames, as well as opening the door to further optimization.\n\nThere are a couple of useful logs that can be written by MoarVM in order to get an idea of how it is optimizing, or failing to optimize, code. The JIT log\u2019s main point of interest for the purpose of optimization is that it can indicate why code is not being JIT-compiled \u2013 most commonly because it contains something the JIT doesn\u2019t know about. The first thing in this case was the call into the VM-backed decoder to extract a line, which was happily easily handled. Oddly, however, we still didn\u2019t seem to be running the JIT-compiled version of the code. Further investigation uncovered an unfortunate mistake. When a specialized version of a method calls a specialized version of another method, we don\u2019t need to repeat the type checks guarding the second method. This was done correctly. However, the code path that was taken in this case failed to check if there was a JIT-compiled version of the target rather than just a specialized bytecode version, and always ran the latter. I fixed that, and went from 57.13% of frames JIT-compiled to 99.86%. Far better.\n\nMy next point of investigation is why the tiny method to grab a line from the decoder was not being inlined. When I took a look at the post-optimization code for it, it turned out to be no surprise at all: while the logic of the method was very few instructions, it was bulked out by type checking of the incoming arguments and return values. The method looks like this:\n\nSpecializations are always tied to a callsite object, from which we can know whether we\u2019re being passed a parameter or not. Therefore, we should be able to optimize out those checks and, in the case the parameter is being passed, throw out the code setting the return value. Further, the that all methods get automatically should have been optimized out, but was not being.\n\nThe latter problem was fixed largely by moving code, although tests showed a regression that needed a little more care to handle \u2013 namely, that a sufficiently complex default value might do something that causes a deoptimization, and we need to make sure we can fall back into the interpreter and have things work correctly in that case.\n\nWhile these changes weren\u2019t enough to get inlined, they did allow an inlining elsewhere, taking the inline ratio up to 28.49% of call frames.\n\nThese initial round of changes took the Perl 6 benchmark from 2.87s to 2.77s, so about 3.5% off. Not much, but something.\n\nThe code we were producing even pre-optimization was disappointing in a few ways. Firstly, even though a simple method like , or , would never possibly do a , we were still spitting out a return exception handler. A little investigation revealed that we were only doing analysis and elimination of this for s but not s. Adding that analysis for methods too took the time down to 2.58s. Knocking 7% off with such a small change was nice.\n\nAnother code generation problem lay in . Access to a native lexical can be compiled in two ways: either just by reading the value (fine if it\u2019s only used as an r-value) or by taking a reference to it (which is correct if it will be used as an l-value). Taking a reference is decidedly costly compare to just reading the value. However, it\u2019s always going to always have the correct behavior, so it\u2019s the default. We optimize doing so away whenever we can (in fact, all the most common l-value usages of it never need a reference either).\n\nLooking at again:\n\nWe can see the read of here is, since is not marked , an r-value. Unfortunately, it was compiled as an l-value because the conditional compilation lost that context information. So, I addressed that and taught Rakudo to pass along return value\u2019s r-value context.\n\nA native reference means an allocation, and this change cut the number of GC runs enormously, from 182 or them to 41 of them. That sounds like it should make a sensational difference. In fact, it got things down to 2.45s, a drop of just 5%. Takeaway lesson: allocating less stuff is good, but MoarVM\u2019s GC is also pretty good at throwing away short-lived things.\n\nWith the worst issues of the code being fed into MoarVM addressed, it was back to seeing why the specializer wasn\u2019t doing a better job of stripping out type checks. First of all, it turned out that optional named arguments were not properly marking the default code dead when the argument was actually passed.\n\nUnfortunately, that wasn\u2019t enough to get the type tests stripped out for the named parameters to . In fact, this turned out to be an issue for all optional parameters. When doing type analysis, and there are two branches, the type information has to be merged at join points in the control flow graph. So it might see something like this in the case that the argument was not passed:\n\nOr maybe this in the case that it was passed:\n\nIn both cases, the types disagree, so they merge to unknown. This is silly, as we\u2019ve already thrown out one of the two branches, so in fact there\u2019s really no merge to do at all! To fix this up, I marked variables (in single static assignment form) that died as a result of a basic block being removed. To make the dead basic blocks from argument analysis actually be removed, we needed to do the dead code removal earlier as well as doing it at the very end of the optimization process. With that marking in place, it was then possible to ignore now-dead code\u2019s contribution to a merge, which meant a whole load of type checks could now be eliminated. Well, in fact, only in the case where the optional was passed; a further patch to mark the writers of individual instructions dead for the purpose of merges was needed to handle the case where it was not.\n\nThat left the return type being checked on the way out, which also seemed a bit of a waste as we could clearly see it was a . After a tweak to Rakudo to better convey type information in one of its VM extension ops, that check was optimized out too.\n\nAnd for all of this effort, the time went from\u20262.45s to 2.41s, just 2% off. While it\u2019s cheaper to not type check things, it\u2019s only so costly in the first place.\n\nA further win was that, with the code for now being so tiny, it should have been an inlining candidate. Alas, it was not, because the optional arguments was still having tracking information recorded just in case we needed to deoptimize. This seemed odd. It turned out that my earlier fix for this was too simplistic: it would leave them in if the method would ever deoptimize, not just if it would do it while handling arguments. I tightened that up and the time dropped to 2.37s, another 2% one. Again, very much worth it, but shows that invocation \u2013 while not super cheap \u2013 is also only so costly.\n\nWith inlining now conquered, another area of the code we were producing caught by eye: boolification was, in some cases, managing to box an into an only to them immediately unbox it and turn it into a Bool. Clearly this was quite a waste! It turned out that an earlier optimization to avoid taking native references had unexpected consequences. But even nicer was that my earlier work to pass down r-value context meant I could delete some analysis and just use that mechanism instead. That was worth 4%, bringing us to 2.28s.\n\nNone of these optimizations so far were specific to I/O or touched the I/O code itself. Instead, they are general optimization and code quality improvements that will benefit most Perl 6 programs. Together, they had taken the lines benchmark from 2.87s to 2.28s. Each may have been just some percent, but together they had knocked 20% off.\n\nBy this point, the code quality \u2013 especially after optimization \u2013 was far more pleasing. It was time to look for some other sources of improvement.\n\nPerhaps one of the easiest wins came from spotting the method of the lines iterator seemed to be doing two calls to the method. See if you can spot them:\n\nThe operator calls to test for definedness. Buy why two calls in the common case? Because of associativity! Two added parentheses:\n\nWere worth a whopping 8%. At 2.09s, the 2 second mark was in sight.\n\nMy next idea for an improvement was a long-planned change to the way that simple loops are compiled. With being defined in terms of , this is also how it had been implemented. However, for simple cases, we can just compile:\n\nBut instead in to something more like:\n\nWhy is this an advantage? Because \u2013 at least in theory \u2013 now the and loop body should become possible to inline. This is not the case if we call , since that is used with dozens of different closures and iterator types. Unfortunately, however, due to limitations in MoarVM\u2019s specializer, it was not actually possible to achieve this inlining even after the change. In short, because we don\u2019t handle inlining of closure-y things, and the way the on-stack replacement works means the optimizer is devoid of type information to have a chance to doing better with . Both of these are now being investigated, but were too big to take on as part of this work.\n\nEven without those larger wins being possible (when they are, we\u2019ll achieve a tasty near-100% inlining rate in this benchmark), it brought the time down to the 2.00s mark. Here\u2019s the patch.\n\nProfiling at the C level (using callgrind) showed up some notable hot spots in the string handling code inside of MoarVM, which seemed to offer the chance to get further wins. At this point, I also started taking measurements of CPU instructions using too, which makes it easier to see the effects of changes that may come out as noise on a simple time measurement (even with taking a number of them and averaging).\n\nFinding the separator happens in a couple of steps. First, individual encodings are set up to decode to the point that they see the final character of any of the line separators (noting these are configurable, and multi-char separators are allowed). Then, a second check is done to check if the multi-char separator was found. This is complicated by needing to handle the case where a separator was not found, and another read needs to be done from a file handle.\n\nIt turns out that this second pass was re-scanning the entire buffer of chars, rather than just looking close to the end of it. After checking there should not be a case where just jumping to look at the end would ever be a problem, I did the optimization and got a reduction from 18,245,144,315 instructions to 16,226,602,756, or 11%.\n\nA further minor hot-spot was re-resolving the CRLF grapheme each time it was needed. It turned out caching that value saved around 200 million instructions. Caching the maximum separator length saved another 78 million instructions. The wallclock time now stood at 1.79s.\n\nThe identification of separators when decoding chars seemed the next place to find some savings. CPUs don\u2019t much like having to do loops and dereferences on hot paths. To do better, I made a compact array of the final separator graphemes that could be quickly scanned through, and also introduced a maximum separator codepoint filter, which given the common case is control characters works out really quite well. These were worth 420 million and 845 million instructions respectively.\n\nNext, I turned to the UTF-8 decoding and NFG process. A modest 56 million instruction win came from tweaking this logic given we can never be looking for a separator and have a target number of characters to decode. But a vast win came from adding a normalization fast path for the common case where we don\u2019t have any normalization work to do. In the case we do encounter such work, we simply fall into the slow path. One nice property of the way I implemented this is that, when reading line by line, one line may cause a drop into the slow path, but the next line will start back in the fast path. This change was worth a whopping 3,200 million decrease in the instruction count. Wallclock time now stood at 1.37s.\n\nAnother look at the profile now showed / as significant costs. Could anything be done to reduce the number of those we did?\n\nYes, it turned out. Firstly, keeping around a decoding result data structure instead of freeing and allocating it every single line saved a handy 450 million instructions. It turned out that we were also copying the decoded chars into a new buffer when taking a line, but in the common case that buffer would contain precisely the chars that make up the line. Therefore, this buffer could simply be stolen to use as the memory for the string. Another 400 million instructions worth dropped away by a call less to / per line.\n\nA few futher micro-optimizations in the new UTF-8 decoding fast-path were possible. By lifting some tests out of the main loop, reading a value into a local because the compiler couldn\u2019t figure out it was invariant, and moving some position updates so they only happen on loop exit, a further 470 million instructions were removed. If you\u2019re thinking that sounds like a lot, this is a loop that runs every single codepoint we decode. A million line file with 60 chars per line plus a separator is 61 million iterations. These changes between them only save 7 cycles per codepoint; that just turns out to be a lot when multiplied by the number of codepoints!\n\nWith these improvements, the Perl 6 version of the benchmark now ran in 1.25s, which is just 44% of the time it used to run in. The Perl 5 version still wins, but by a factor of 1.1 times, not 2.5 times. While an amount of the changes performed during this work were specific to the benchmark in question, many were much more general. For example, the separator finding improvements will help with this benchmark in all encodings, and the code generation and specializer improvements will have far more cross-cutting effects.\n\nThere\u2019s still a decent amount of room for improvement yet. Once MoarVM\u2019s specializer can perform the two inlinings it is not currently able to, we can expect a further improvement. That work is coming up soon. And beyond that, there will be more ways to shave off some instructions here and there. Another less pleasing result is that if Perl 5 is not asked to do UTF-8 decoding, this represents a huge saving. Ask Perl 6 for ASCII or Latin-1, however, however, and it\u2019s just a small saving. This would be a good target for some future optimization work. In the meantime, these are a nice bunch of speedups to have."},
{"url": "https://redfin.engineering/organize-your-desktop-for-web-development-e452b3058ebf", "link_title": "Organize your desktop for web development", "text": "I cannot count how many times I\u2019ve lost my keys or glasses and ran around the house looking for them. Searching for a lost item usually takes two or three minutes. When measured in time, the cost of these small events is trivial. However, these small interruptions collectively stress my mind and decrease my productivity and satisfaction throughout the day.\n\nComputers have similar problems. Searching for lost windows frustrates me. It is difficult for the human brain to track the location of a moving object. Not only do we move windows, but we minimize them, maximize them, resize them, and layer them on top of one another. When I lose my terminal window, I move many windows in order to find the terminal. Once I find it, my entire desktop will be a mess. Next time I look for a window, I will have to do that all over again.\n\nOn a flight from San Francisco to Atlanta, I was reading The Organized Mind by Daniel Levitin. The author lists the negative impact of these small daily interruptions. Luckily, he also presents a solution to the problem of losing my keys. The solution is to buy a key holder and attach it to my door. As simple as it sounds, it is extremely effective. The key holder adds a restriction to the key location space. Now, I grab the keys on my way out without giving it a thought.\n\nI set forth looking for the perfect holder for my windows. After trying different applications, I chose BetterSnapTool. My favorite features are the hot-keys and the snap areas. There are hot-keys for almost anything, like moving a window to one-third or two-thirds of the screen. Snap areas allow me to designate a part of a monitor to hold a window.\n\nI use a three-monitor setup for web development at Redfin. The image above shows my screen\u2019s layout. The left screen is for fiddling with the browser. Chrome usually takes up the left two-thirds of the screen, and the developer tools take up the remaining third. My text editor or IDE is always on the central monitor. The right monitor has Slack for chat, Notational Velocity for note-taking, and Terminal app for the terminal. BetterSnapTool\u2019s snap areas make it easy to place the notes app and the terminal app in their respective locations.\n\nLike with my eye glasses, I want to be able to take a window and \u201cwear\u201d it. When I am done, I want to be able to put it back in its case. BetterSnapTool lets me expand a window to full height, expand it to full-screen, or move it to the central monitor and put it back in place easily.\n\nSince I adapted to this setup, I have never lost a window again. Now that I do not have to worry about windows, I can cruise through my work and ship projects faster."},
{"url": "http://www.tabletmag.com/jewish-arts-and-culture/242257/mrs-herskovitzs-kimono", "link_title": "Mrs. Herskovitz's Kimono", "text": "My father was a soldier in George S. Patton\u2019s army, European theater, World War II. Dad was a corporal, military police, born hard-edged and by disposition stubborn.\n\nMy father-in-law was also a soldier, an equally tough combatant, deployed to Asia by the Heavenly Sovereign Emperor Hirohito of Japan. Father-in-law was polite and introspective, at least in the years I knew him. During the war he was a foot soldier, bivouacked in China, where most of the imperial army reposed. He never revealed much about his wartime years, although I do recall him speaking once about the consistency and color of the dirt he slept on, and the fear that hung above, like a fog that wouldn\u2019t scatter.\n\nMy father was a Jew of East European stock, whose forbears worshipped God, spoke Yiddish, and endured as best they could the hard shtetl life.\n\nMy father-in-law was samurai, with genuine warrior roots. Discipline and industriousness ran in his blood. He was not a religious man unless you consider the golf ball to be a postwar deity. But he valued tradition, was faithful to his family, and revered Japan, prewar and post.\n\nMy father also was a reverent man, a patriot who honored his country. But he also honored the diaspora of European Jewry whose ruin he saw sketched, in harrowing relief, when he and his U.S. Army comrades freed Buchenwald.\n\nEmancipating that place and feeding its survivors was a defining event in my father\u2019s life. The experience left a stomach-churning taste that lingered, a taste that affected the way he viewed the world. It elevated my father\u2019s sense of Jewishness, and it fed a preexisting arrogance that, in the postwar years, turned him into something of a xenophobe. He preferred the insular world of people who looked and thought like him; indeed, he raised his children in a suburban Jewish ghetto where diversity was non-existent, where intolerance of non-Jewishness flourished, where Israel stood, like Mount Sinai, above all else, with the possible exception of the American flag.\n\n\u201cI dislike foreigners,\u201d my father told me one day, out of the blue, when I was 17. It was a disturbing and profoundly disappointing thing to hear. It sent me reeling in the opposite direction, toward a life of foreign languages and cultures, and into the arms of my wife, who is from Japan, from the other side of my father\u2019s war.\n\nMy father\u2019s wartime name was Rudy. My father-in-law was Kennichi. They were born months apart, in 1925, on different halves of the planet. Each man died, eight decades later, without having met the other. I made sure of that. But for me, the two were interlinked by my marriage, to be sure, but even more so by other more powerful things: the horoscopic tenor of contemporary births; the subsequent decades that shaped their lives and mind-sets; and within those confines and above all else, the war in which they served. And because of these connections, I found meaning in their deaths and inspiration in the manner each was laid to rest.\n\nMy father was interred in a Jewish cemetery named Shalom Memorial Park. A military honor guard showed up, ramrod in posture and dressed in finery. A young army corpsman played taps. Another saluted my mother. They retrieved the American flag that draped my father\u2019s coffin and folded it gently\u2014the better to exalt the spirit of the man who had died\u2014and meticulously, the better to acknowledge the disciplined veteran and his military service.\n\nWe took the flag home and placed it near my father\u2019s safe, in which, upon inspection, I discovered an old envelope filled with a dozen photographs he had taken in Buchenwald. The photos depicted the usual Holocaust fare, horrific piles of corpses and bones, skeleton survivors with shattered faces. My father had kept the photos secret, under lock and key, so important were their memories and meaning to him. And now, to me.\n\nMy father-in-law was cremated, as Buddhism requires; his family and I surrounded his coffin as it slid into the crematory oven. We were there afterward as well, when Kennichi-san\u2019s remains, once cooled, were rolled out, all ashes and shards of bone, and presented before us, the better to honor his soul. And as Japanese tradition commands, the immediate family\u2014the widow, the daughters and son\u2014used giant chopsticks to pick up skeletal bits and pieces, depositing them into a cremation urn that, as it happened, was the clone of an oversized golf ball specially tailored for the occasion.\n\nThe ceremony is called kotsuage, and the idea, explained to me afterward, is noble: Loved ones watch over the deceased until the very, very end, when the soul of the fallen enters the enlightened eternal life. I wasn\u2019t aware of the rationale as the event transpired, and except for the golf ball, all I could think of was Buchenwald and Auschwitz.\n\nThose thoughts surprised and disturbed me. I knew, of course, where they came from because I was my father\u2019s son and had been raised as a Jew in a place called Skokie, Illinois, where synagogues and temples outnumbered churches, and where thousands of Holocaust survivors lived among us interspersed.\n\nI grew up next door to a Mrs. Herskovitz, who had experienced Auschwitz. My first glimpse at the numbers tattooed on her left forearm is a childhood snapshot memory.\n\nMy reaction to Kennichi\u2019s cremation surprised and disturbed me mostly because it exposed ignorance of things Japanese, despite my marriage and multiple visits to Japan, and also because I had conjured up, in the presence of Kennichi-san\u2019s remains, a memory of evil that falsely equated a righteous and praiseworthy Buddhist tradition with Hitler\u2019s Final Solution, a most profoundly malevolent proceeding.\n\nYet there was, in this unseemly juxtaposition, something else at play: a reminder of the simple truth that vision is shaped by experience, and that Judaism tints the lens through which I view Japan.\n\nIn the summer of 2007, while in Tokyo on a journalism fellowship, I came across a photograph during an internet search for story ideas. The photo was taken days after the Aug. 6, 1945 atomic bombing of Hiroshima. It showed a young woman who had survived the blast. Her face was hidden, but her shoulders and back were exposed. Burnt there into her skin, by the force of the explosion, was the pattern of the kimono she was wearing when the A-bomb went off.\n\nThe photo was subtle, lyrical, and horrifying. A woman, in a flash, was branded forever with the crosshatch pattern of a summertime kimono, the consequence of gamma rays and simple science: dark colors absorb heat, but light colors reflect. Black stripes of kimono fabric took in thermal radiation and penetrated her flesh. White patches of kimono cloth reflected the heat, sparing the skin. What remained on the woman was a gruesome blueprint of the garment Japan holds most dear.\n\nFor me, the markings on her flesh recalled the concentration camp tattoos of Auschwitz and summoned up the memory of my childhood neighbor, Mrs. Herskovitz, who, like the woman in the photo, bore the uncommon stain of war on her skin. It seemed to me that the photo of this unidentified woman from Hiroshima easily could rest beside snapshots of Jewish Holocaust victims in the gallery of 20th-century suffering.\n\nI knew Mrs. Herskovitz. I knew nothing about the woman in the Hiroshima photo. What was her name? What was her story? How long did she live after the war?\n\nDid her life or death add value to my understanding of the Holocaust, or of Japan; or was my connection of this woman with Mrs. Herskovitz entirely subjective and without broader merit?\n\nI traveled to Hiroshima in search of answers. First stop\u2014the Hiroshima Peace Memorial Museum. It is Japan\u2019s Yad Vashem, a repository of memories, artifacts, pain, and suffering that illustrates, with archeological precision, the human fallout of an atomic attack.\n\nA copy of the photograph was displayed there. The picture was in color. \u201cImprinted kimono pattern,\u201d a caption read. \u201cThe heat rays burned the dark parts of the kimono pattern into her skin.\u201d The photographer was identified as a man named Gonichi Kimura. He was said to have taken the picture at the \u201cUjina Branch of the Hiroshima First Army Hospital.\u201d Approximate date of the photo: Aug. 15, 1945.\n\nIt was a beginning, scraps of information that suggested pathways for further research. But the prize\u2014details about the woman\u2014was not there. She was not identified. The photo, in fact, appeared staged to maintain her anonymity: she was seated, looking away from the camera, precluding facial recognition, presumably because the snapshot was painfully intimate: The flowery blue dress she wore on the day of the photo shoot was immodestly pulled down to the waist on her right side, fully exposing a scarred torso.\n\nLater that day, I visited the peace museum\u2019s archive. There, hidden inside a weather-beaten volume of wartime photographs, I found the woman\u2019s name.\n\nA curator gently placed the book on a table in front of me. She handled it with white gloves. \u201cIt\u2019s very valuable,\u201d she said. The book was part of a medical report that a Japanese investigative team had prepared after the Hiroshima bombing. Its pages were yellowed and torn with age, conveying a fragility appropriate to the subject matter.\n\nA photo of the kimono-scarred woman was pasted onto an unnumbered page in the middle of the volume. The curator told me it was one of a handful of original prints. There was a second photo on the same page, equally stark, if not more so. It showed a single, charred, tattered scrap of cloth, taken from the kimono the woman had worn when the atomic bomb exploded.\n\nBeneath the first photo, in handwritten text, was the woman\u2019s name, as well as a statement that the burns on her body were \u201cin accordance with the black stripes of her clothing.\u201d Above the second photo, the one with the frayed piece of kimono cloth: a simple notation that the photo exhibited \u201cthe relationship between clothing and burns.\u201d\n\nThe curator told me that this book was the only known source linking the woman\u2019s first and last names to the photo. At the curator\u2019s insistence, I agreed to protect the woman\u2019s privacy\u2014posthumously if she were dead, contemporaneously if she were still alive\u2014by using her first initial only. The woman\u2019s name was \u201cS. Ushio.\u201d\n\nThe search for S. Ushio\u2019s story required multiple visits to Hiroshima over the course of several years. Even then, only snippets of Ushio-san\u2019s life seemed clear and subject to confirmation.\n\nOn July 10, 1977, Hiroshima\u2019s main newspaper, the Chugoku Shimbun, published a list of yukuefumeisha\u2014people who had gone missing after the atomic bombing. The list was based on information provided post-1945 by surviving family members who were looking for their loved ones. S. Ushio was on the list. She was reported to have been 22 years old on the day of the bombing, and she lived in the Kakomachi section of Hiroshima, a downtown area some 1.3 kilometers (eight-tenths of a mile) south of ground zero, also known as the hypocenter. There was no information regarding her whereabouts at the time of the blast.\n\nTwo weeks earlier, on June 29, 1977, the Chugoku Shimbun had printed another list, of muenbotoke\u2014bomb victims who had been identified but whose remains were never collected by family members. S. Ushio was not on this list.\n\nTaken together, the two lists showed that somebody in S. Ushio\u2019s family had survived the bombing and had notified authorities that the woman was missing. There was no confirmation, however, that she was dead or alive.\n\nThere are two large repositories of information in Hiroshima regarding A-bomb casualties: the National Peace Memorial Hall for the Atomic Bomb Victims, and Hiroshima City Hall. These buildings store all relevant, extant written records. I visited both places and asked staff researchers to look for any mention of an S. Ushio. A young victim\u2014\u201cSakuko Ushio\u2013age 7\u201d\u2014came up in both databases. There were no references to S. Ushio, however, and, therefore, still no proof of her whereabouts, or whether she had died.\n\nThe Chugoku Shimbun article from 1977 was the only available public acknowledgment of S. Ushio that I could find. Additional information about her life, however, could reasonably be inferred from other sources.\n\nI showed the Ushio photos to Professor Yuko Tanaka, a Japanese culture and kimono expert at Tokyo\u2019s Hosei University. Tanaka said the crosshatched garment that left its imprint on S. Ushio\u2019s back was a yukata, a commonly worn summertime kimono made of cotton. \u201cIt was very casual and inexpensive, the daily style of normal people during WWII,\u201d Tanaka said. At age 22, Ushio-san would probably have been married. \u201cI believe this lady was most likely a normal housewife.\u201d\n\nThe curator at the Hiroshima Memorial Peace Museum provided information about Gonichi Kimura, the man who took S. Ushio\u2019s picture. Kimura worked as a staff photographer for the Japanese Army\u2019s Ship Training Division, which was based in Hiroshima (the city was an important port in southern Japan). The Ujina military hospital, where the photo was taken, was situated some two-and-a-half miles from the A-bomb hypocenter. It was equally far from Kakomachi, the area where S. Ushio lived.\n\nThe Ushio photo was one of dozens Kimura took in the days and months after the atomic bombing. Kimura wrote a brief accounting of his activities after the U.S. attack, which the curator made available to me. In it, Kimura made no mention of a kimono-scarred woman. He did, however, take photographs in Kakomachi. The neighborhood was flattened. \u201cThere were no buildings, only a solitary, strangely tall chimney standing in the midst of the burned-out area,\u201d Kimura wrote. \u201cIt was terribly eerie.\u201d\n\nUshio was probably elsewhere when the A-bomb fell. Had she been in Kakomachi, she most certainly would have died.\n\nBut she remained alive, at least for the weeks immediately after the bombing. The caption on the Ushio photo displayed in the Peace Museum said Kimura shot the picture approximately Aug. 15, 1945, 9 days after the bombing. Masami Nishimoto, a Chugoku Shimbun reporter who has written about Kimura\u2019s photographs, told me that was wrong.\n\nNishimoto explained that the photo was taken with color film, which was not available in Japan at the time. It was, however, used in the United States. Nishimoto said that Kimura must have gotten the film from an American research group that had arrived in Hiroshima, and visited Ujina hospital, in September 1945. That is when Kimura would have snapped his photo, or perhaps even later. In either case, S. Ushio lived for at least a month after the Hiroshima attack.\n\nBut how long could she have stayed alive? For me, that was the most intriguing question. She had made it through the bombing and, despite her injuries, had somehow found her way to the Ujina military hospital. That alone suggested a certain tenacity. Could she have hung on for months or even years longer? Or did her injuries and exposure to radiation cut short her life?\n\nIn Japanese, the word genbaku means \u201catomic bomb.\u201d It is as central to describing Japan\u2019s postwar experience as the word Holocaust is to the Jews, or Pearl Harbor, D-Day, and Stalingrad to the lexicon of WWII.\n\nThe Genbaku Retirement Home is nestled in the hills outside Hiroshima, a 25-minute train ride and a 10-minute taxi from the city\u2019s central rail station. The home is modern, the area quiet. There is no sign of the urban bustle that now thrives in contemporary Hiroshima, whose population now exceeds 1 million. The Genbaku Retirement Home enjoys tranquility.\n\nIn 1945, at least 350,000 people lived in Hiroshima. When I visited the retirement facility, some 300 A-bomb survivors\u2014collectively known as hibakusha\u2014resided there. Dr. Nanao Kamada was their chief medical officer.\n\nDr. Kamada was a hematologist and radiation biologist. Since 1962, he had specialized in treating hibakusha who suffered from leukemia and cancer. He was especially interested in helping people who were within a 500-meter radius of ground zero and, despite the odds, had managed to survive.\n\nNobody knows exactly how many people died in the immediate firestorm of the Hiroshima A-bomb. The Radiation Effects Research Foundation, a joint U.S.-Japanese scientific group that monitors the health of hibakusha, estimates that between 90,000 and 160,000 people perished Aug. 6, 1945, or within the subsequent two to four months.\n\nDr. Kamada said that in 2010, about 240,000 people who had experienced the Hiroshima attack were still alive. Their average age was 76. I showed Dr. Kamada S. Ushio\u2019s photograph and told him Ushio-san was 22 years old on the day it was taken. \u201cAs a medical doctor specializing in treating survivors, can you diagnose how seriously this woman was injured and give me your opinion on how long she was likely to have lived?\u201d I asked.\n\nDr. Kamada said the burns on S. Ushio\u2019s body were not uniformly serious. Her neck and elbow were \u201cvery severely injured.\u201d Her arm and upper back were not.\n\n\u201cSo your question is how long she could have survived?\u201d he asked.\n\nHe paused. \u201cIf she had been properly treated, she can still be alive for more than 20 years. Do you know the distance she was from the hypocenter?\u201d\n\n\u201cNo,\u201d I said. \u201cBut she was in Hiroshima city. This picture was taken at the Ujina military hospital.\u201d\n\n\u201cSo she was exposed nearer to the hypocenter, and then she moved to Ujina hospital.\u201d\n\n\u201cYes. I\u2019m not sure where she was exactly when the bomb went off. But she lived somewhere in the city.\u201d\n\n\u201cThe survival time depends on the dose of exposure,\u201d Dr. Kamada explained. \u201cFrom this picture, I would speculate she was exposed around 2 kilometers from the hypocenter. So the radiation doses were not so hot, probably 5 to 10 rads. In those cases, someone can live 10 or 20 years. So unless she had a malignant tumor, she can live.\u201d\n\n\u201cUnless she had a malignant tumor, she could have survived?\u201d\n\n\u201cSo it is possible that this woman could be alive today?\u201d\n\n\u201cMaybe this is not a fair question, but could you give what the odds are that she could still be alive?\u201d\n\nDr. Kamada paused again to think.\n\n\u201c80 percent she could still be alive?\u201d\n\n\u201cYes. You said she was 22 years old?\u201d\n\n\u201cShe would be about 85 years old now.\u201d\n\nI thanked Dr. Kamada for his time \u00a0and asked if I could take his photograph. He agreed.\n\nA large bouquet of flowers sat on his desk. Dr. Kamada insisted that I include the flowers in the photo. \u201cI love flowers,\u201d he told me.\n\nWe shook hands, and I returned to Hiroshima.\n\nJapanese names that sound alike can be spelled in different ways. The variations depend on the choice of kanji, the Chinese characters that make up the bulk of Japan\u2019s writing system. Two completely different kanji characters can have identical pronunciations. That means the surname \u201cUshio\u201d can be spelled in dissimilar ways.\n\nThe Hiroshima telephone directly listed nearly 150 Ushios with kanji that matched the spelling of S. Ushio\u2019s surname, as recorded in the old book of photos I saw in the peace-museum archive. I called them all\u2014with an interpreter\u2019s assistance\u2014hoping to find S. Ushio herself, or a relative who might have known her.\n\nA woman named Motoko answered one of the calls. She said she knew an S. Ushio who was 84 years old, only one year shy of Dr. Kamada\u2019s estimation. This S. Ushio was the sister of Motoko\u2019s husband. She married at some point and went by the name of S. Kiyokawa.\n\nI explained that I was looking for the woman in the kimono burn photo, and asked if I could telephone S. Kiyokawa to inquire whether she had been treated at the Unija military hospital in August or September 1945, and if so, whether she recalled posing for a photographer named Kimura.\n\n\u201cI don\u2019t know if it is a good idea to give you the number,\u201d Motoko said. \u201cEven if you call, she is asleep most mornings and nights, so she probably wouldn\u2019t pick up the phone. She\u2019s old now and cannot hear well.\u201d\n\nMotoko agreed to contact her sister-in-law on my behalf.\n\nMeanwhile, I contacted a researcher at the Hiroshima National Peace Memorial Hall, and asked whether they had any information about a survivor named S. Kiyokawa. They did not.\n\nMotoko called back later that day. She had spoken with S. Kiyokawa.\n\n\u201cShe told me politely but firmly that she will not speak with you. She said she is very old and will not be calling you back. And she said, \u2018It cannot be me in that picture.\u2019 \u201d\n\nJust as well, because I had forgotten to ask whether S. Kiyokawa spelled her maiden name with the same kanji used in my S. Ushio\u2019s surname. That oversight alone would have precluded my ability to confirm whether the two hibakusha might have been one and the same.\n\nAll of the other Ushios I spoke with said they had never heard of an Ushio survivor whose first name was S. Based on the tone of some of those conversations, information would not have been forthcoming even if knowledge existed.\n\nI do not know what became of S. Ushio. Inconspicuousness is a garment that history seems to have imposed on her, whether she wanted to wear it or not.\n\nMasami Nishimoto, the Chugoku Shimbun expert on genbaku photography, has looked into the Ushio photo. He told me with certainty\u2014based on records he claims to have seen\u2014that S. Ushio died at age 22, and that her body was probably transferred from Hiroshima to Ninoshima, a nearby island in the Seto Inland Sea that has been called the place of the sleeping dead. Ninoshima served as a quarantine station and emergency field hospital after the bombing. Tens of thousands of casualties were moved there, and many thousands died, their identities lost in the mayhem of emergency cremations and burials.\n\nOver the postwar years, remains of the Ninoshima dead were moved back to Hiroshima. Unidentified ashes, along with identified but unclaimed remains, were stored in the Atomic Bomb Memorial Mound, an underground repository of lost souls that is situated near the Peace Memorial Museum. Nishimoto surmises that S. Ushio\u2019s remains\u2014identified or not\u2014rest inside the Mound.\n\nShinji Uemoto, a Hiroshima City Hall official responsible for the Memorial Mound, told me he had no record of S. Ushio\u2019s remains ever having been placed there.\n\nNishimoto said it would not have been unusual, given the confusion of the times, for S. Ushio\u2019s name\u2014which authorities had recorded when her photograph was taken\u2014to have disappeared entirely from government registers. If that is what happened, and if S. Ushio\u2019s ashes are in the Mound, they probably sit unlabeled, resting with the many thousands of other unidentified remains that reside there.\n\nThere has never been a full accounting of everyone who died in Hiroshima. The A-bomb destroyed the public records of private lives many thousands of times over. The details of S. Ushio\u2019s life vanished in that whirlwind.\n\nBut S. Ushio\u2019s photograph is alive. It is eloquent and it is powerful. And it has the ability to rouse memories and to stir emotions, particularly among those Japanese of S. Ushio\u2019s generation who experienced the war themselves.\n\nI met Miyoko Watanabe one November morning outside a basement conference room in the Hiroshima Peace Memorial Museum. I had just visited the museum\u2019s archive, where I found the old book, yellowed with time, which provided the name and age of S. Ushio.\n\nWatanabe-san, a Hiroshima hibakusha, was in her late 70s. She had just lectured some high school students about the day the bomb went off.\n\n\u201cIt is not easy for me to talk about my experience,\u201d she said. \u201cFor me, it is like airing my dirty linen in public.\u201d She speaks to children in order to honor the victims and to promote peace, she said. She has told her story more than 1,000 times.\n\nMiyoko Watanabe was a teenager in 1945. She had been mobilized as a laborer in the Japan Steel works, which made defense-related supplies for the Japanese navy. Aug. 6 was a no-electricity day at the plant, so its workers had time off. Watanabe-san was in her home, about 2.5 kilometers south of ground zero, when the bomb exploded. Her house collapsed, exposing bamboo frames. She was knocked unconscious. Her injuries were not life-threatening, although she suffered severe diarrhea. Her father, who was closer to the hypocenter, was horribly burned.\n\n\u201cOn the day of Japan\u2019s surrender,\u201d Watanabe-san recalled, \u201che mumbled, \u2018Japan lost the war.\u2019 He died undramatically the next day, complaining of the cold.\u201d\n\nThe fire and heat, Miyoko Watanabe remembered, is what tormented most other victims. \u201cThere came a drove of people whose faces and clothes were burned black; almost naked and burned beyond recognition, they came tottering along, dangling their arms in front of them like ghosts; some had their work pants burned away save the elastic strings; others had all their clothes burned except for the front part. They all kept chanting, \u2018Water! Give me water!\u2019 \u201d\n\nTheir flesh was wet and \u201cexposed juicily,\u201d Watanabe-san said. Peeled skin hung \u201cfrom their fingertips like seaweed.\u201d\n\nI told her I had come to Hiroshima to search for the story behind the kimono-scarred woman, and that I had just made some progress. I pulled out my wallet and removed a small piece of paper, on which I had written S. Ushio\u2019s name.\n\n\u201cShe is the woman in the photo?\u201d\n\n\u201cAhh.\u201d Watanabe-san jumped back in her chair, and touched her shoulder, indicating, from memory, the spot where the pattern of S. Ushio\u2019s kimono was most pronounced.\n\n\u201cI always use the photo in my talks,\u201d Watanabe-san told me. \u201cIt is a picture that helps children understand the power of the bomb.\u201d\n\nMiyoko Watanabe had a serious and scholarly way of engaging visitors. On the day I met with her, she was dressed for business. She spoke briskly and with confidence. Her pitch-black hair was shortly cropped, just so, and was thinning on top. She wore a navy blue blazer. A gold brooch in the shape of a flower was pinned to the lapel.\n\nThere was, to my eye, a patina of sadness about Watanabe-san, despite her self-assurance. The features of her face seemed to sag, drawn by all she had lived through. Even the flower on her lapel pointed downward.\n\nAs we got to talking, I explained that the Ushio photo, in my opinion, was especially compelling because a kimono\u2014the garment that symbolizes Japan\u2014had scarred her skin. I asked Watanabe-san if she had any thoughts on the matter.\n\nMy question had the unexpected effect of transforming the nature of our conversation.\n\nKimonos meant something to her. Her voice softened. Her business demeanor eased. And she went back in time, to her childhood in prewar Japan.\n\nWearing a kimono was one of her earliest memories, she said. She recalled how her family, which was well-to-do, had collected a sizable number of kimonos, not everyday fare like S. Ushio\u2019s, but expensive, elegant garments; and how once, as a child, she dressed in a beautiful kimono and marched in a parade.\n\n\u201cWatanabe-san,\u201d I said, \u201cdo you remember the first time you wore a nice kimono after the atomic bombing?\u201d\n\n\u201cIt was for a New Year\u2019s Day celebration, in 1948,\u201d she said. \u201cI was very happy. I got to wear makeup for the first time ever, and even curled my hair.\u201d\n\nThe memory was powerful, and Miyoko Watanabe delighted in the pleasure of recounting it. She hugged herself at the recollection of how excited she was to wear that garment. \u201cIt was a red kimono with long sleeves. It was so beautiful. I couldn\u2019t wait to put it on. It made feel me so proud.\u201d\n\n\u201cDo you know where the red kimono is today?\u201d I asked.\n\n\u201cMy mother gave it away to my niece. Without my permission! I was furious!\u201d\n\nWatanabe-san paused, then, unexpectedly, covered her face. She began to cry. Her mother had acted badly, and decades later, the pain and anger remained.\n\nShe explained that the kimono transfer occurred when she was childless. Since the niece was the only young girl around, her mother presented the garment to her.\n\nWatanabe-san later adopted a daughter but lacked a kimono to give her. So she bought a brand new one, equally beautiful. The purchase was her only option. It would have been inappropriate, she said, to ask her niece to return the original item.\n\nWhen Miyoko Watanabe and I parted, I thanked her for sharing the red kimono tale with me. I pulled out a copy of the S. Ushio photograph and pointed to it.\n\n\u201cI don\u2019t want this to be the only kimono story I tell.\u201d\n\nDuring my conversation with Watanabe-san, I mentioned that a reporter for the Chugoku Shimbun, the local newspaper, had told me that S. Ushio\u2019s remains might be resting in the Atomic Bomb Memorial Mound.\n\nIt was an unanticipated revelation. The Mound is a revered spot, one of the most solemn in Hiroshima. The remains of 70,000 unidentified souls lay inside its vault, along with several thousand others whose identity is known but whose ashes are unclaimed.\n\nThe vault is closed to the public. \u201cNo one can go in,\u201d a City Hall spokesman told me. \u201cNot even government officials. Only family members claiming remains are allowed inside.\u201d\n\nJapan is a rigidly bureaucratic country where rules are hardly ever broken. Watanabe-san\u2019s visit to the Mound was extraordinary.\n\n\u201cThere was an old woman who took care of the Mound,\u201d Watanabe-san explained. \u201cShe cleaned it outside and inside. Even though it was prohibited, she let me inside the vault just one time because I knew her well. Her name was Toshiko Saeki.\u201d\n\nI visited the Mound at lunchtime one autumn day. It was smaller than I had imagined. A stone pagoda sat on its highest point, and trees hovered on the periphery.\n\nI was hoping to see Toshiko Saeki, but she wasn\u2019t there. But a man on a motorcycle drove up. He wore a helmet, black suit, and tie. He parked the vehicle off to the side, dismounted, and approached the Mound, where he bent over and lit a candle. Straightening himself up, he put his hands together, closed his eyes, bowed, and prayed. When he was finished, he hopped back onto the motorcycle, fixed his helmet, and drove off, back to the business of the day.\n\nAll the while, directly behind the Mound, a large memorial bell tolled. Its resonant, metallic tones drifted through the trees and settled above the man, anointing his prayers in sound.\n\nI asked Watanabe-san to describe what was inside the Mound. She told me its underground vault was lined with shelves. The shelves held rows of containers filled with remains\u2014the unidentified in wooden vessels, the identified in white china, with names written in black ink. The containers stood shoulder to shoulder, like soldiers at attention.\n\n\u201cI was so scared to go in,\u201d Watanabe-san said, \u201cbecause ashes were really there.\n\n\u201cMy mother once told me a story,\u201d she continued, \u201chow one of her friends found a container inside the vault with the ashes of her son. She refused to take the remains. She felt it would be nice to keep them in the Mound, a place where so many people would come to pray for him. That would be good for his spirit.\u201d\n\nToshiko Saeki, Watanabe-san\u2019s friend, was a hibakusha. People called her the \u201cguardian of the Mound\u201d because of her constant presence there. She lost more than a dozen family members in the A-bomb attack, including her mother and younger sister. I never got to meet her, but I did read something she once said.\n\n\u201cThe dead still cling to me,\u201d she declared. \u201cSo I decided to live with the victims.\u201d\n\nWhen I first visited Hiroshima, I didn\u2019t expect to meet somebody who grew up in my hometown, let alone somebody who had made a career thinking, writing, and teaching about Japan. Robert Jacobs, who goes by the nickname Bo, is a professor at the Hiroshima Peace Institute of Hiroshima City University. He describes himself as \u201ca historian of nuclear technologies and radiation technopolitics.\u201d He is interested in how exposure to radiation has affected people, families, and communities. And he has thought a lot about S. Ushio\u2019s photograph, and the symbolism of her kimono-scarred back.\n\nBo and I shared the DNA of having been raised in Skokie, Illinois, a village north of Chicago. When we were kids, Skokie had a sizable Jewish population that included roughly 7,000 Holocaust survivors. It was one of the largest clusters in the U.S. of eyewitnesses to the atrocities of Hitler\u2019s Final Solution.\n\nWe also had similar connections to WWII, albeit on different fronts. My father fought in Europe. Bo\u2019s dad served in the Pacific theater. At a gathering in Germany after the Nazi capitulation, my father stood next to Gen. Dwight Eisenhower, leader of the Allied forces. Bo\u2019s father stood on a ship in Tokyo harbor when Japanese representatives, on board the USS Missouri, signed the Instrument of Surrender.\n\nBo and I came from common stock, and he also was one of the most articulate men I knew. I was curious to get his opinion about the Ushio photograph. We met one morning in downtown Hiroshima, less than a mile from ground zero.\n\nBo told me that in the years immediately following Japan\u2019s surrender, the U.S. suppressed images from Hiroshima and Nagasaki. \u201cThere was an extremely concerted effort to keep any photograph of actual victims or survivors out of the public domain,\u201d he said. The purpose was to skirt the issue of civilian casualties and to bolster President Truman\u2019s assertion that the Japanese cities were military targets.\n\nLife\u00a0magazine, in its issue of Sept. 29, 1952, published the first graphic Hiroshima photos, calling them \u201cAtom Blasts Through Eyes of Victims.\u201d S. Ushio was not among them. With time, however, her picture was published widely. \u201cIt\u2019s one of the photographs you encounter when you begin to read anything about Hiroshima,\u201d Bo said.\n\nIf that was the case, I asked, why had S. Ushio remained an anonymous victim, her name hidden in the archives, the story of her life, and of her death, unknown?\n\nIf, in fact, she had died, Bo said, surviving family members would not have publicized her case. \u201cPerhaps she was buried in a mass grave and wasn\u2019t honored properly. It might be easier just not to talk.\u201d Silence would avoid any embarrassment or shame at the impropriety, albeit forced by circumstances, of having failed to say a proper goodbye.\n\nThere was also the issue of humility. \u201cI think there could be elements of not wanting to stand out and claim anything special about having her as a family member, compared to the other tens of thousands, hundreds of thousands who died. And,\u201d he added parenthetically, \u201cthere is a great stigmatization of hibakusha.\u201d\n\nI halted the conversation at that point. How, I wondered, could anybody view A-bomb survivors disapprovingly? They were, it seemed to me, a hallowed class of noncombatants who had lived through the unimaginable, much like the Holocaust survivors Bo and I grew up with in Skokie. Our neighbors from Auschwitz may, on occasion, have kept to themselves, but not because they were shunned.\n\n\u201cMany hibakusha,\u201d Bo explained, \u201chave tried to hide their exposure to radiation because, along with radiation, comes fear. People feared that hibakusha were still in some unknown way contaminated, that hibakusha were likely to have children with genetic problems who would make undesirable marriage partners. A-bomb survivors had reasons to downplay their identification. That may have kept the Ushio family from publicly embracing her victimhood.\u201d\n\nMost Hiroshima and Nagasaki survivors could cloak their status, if they so chose, simply by keeping silent. There was nothing about the way they looked that gave them away.\n\nBut S. Ushio could not hide. Her kimono scars guaranteed that. It was the same for my childhood neighbor, Mrs. Herskovitz. The numbers tattooed on her arm identified her as a survivor whenever her shirtsleeve slipped up.\n\n\u201cThat\u2019s what drew me to the Ushio photo,\u201d I told Bo. \u201cWhen I first saw the kimono pattern burned onto her skin, I immediately thought of German concentration-camp tattoos, and of the numbers on my neighbor\u2019s arm, Mrs. Herskovitz. Both imprints were a sort of wartime branding.\n\n\u201cWhat happened to Ushio took place in entirely different circumstances from what happened to Mrs. Herskovitz,\u201d I said. Mrs. Herkovitz\u00a0was a victim of genocide\u00a0unleashed by a vile aggressor regime. S.\u00a0Ushio was\u00a0victimized by having been born in Japan, the\u00a0aggressor nation responsible for unleashing war in the Pacific. Historians may argue whether Harry Truman\u2019s decision to use an atomic bomb on Hiroshima was justifiable, but there is no doubt that Japan, like Nazi Germany, was a belligerent nation that had to be stopped.\n\n\u201cBut still,\u201d I added, \u201cthe Holocaust and the bombing of Hiroshima were equally horrifying for the victims, and in that sense, the branding of S. Ushio and Mrs. Herskovitz were equally appalling.\u201d\n\n\u201cIt\u2019s the impersonal nature of the horror of war,\u201d Bo said. \u201cThe Jews were given those numbers to replace their individual identity, which was wiped from the slate as part of a mechanized process of killing. And the same is true for the woman with the kimono pattern burned on her. She was merely part of a city of hundreds of thousands of people who were, in a single instance, scorched and killed and irradiated. Her victimhood in war was completely impersonal.\u201d\n\nAt the same time, Bo noted, the kimono scars on S. Ushio\u2019s flesh were personal, in a most distinctive, invasive and intimate way.\n\n\u201cThere were so many people killed,\u201d he said, \u201cso many scorched bodies. But she is marked specifically. She was burned in a specific way, and what that restores to her is her individuality. That\u2019s one of the only reasons that we even think of her. And that\u2019s one of the reasons this picture is important to us.\u201d\n\nYou can help support Tablet\u2019s unique brand of Jewish journalism. Click here to donate today.\n\nRobert Rand is the author of several books, including My Suburban Shtetl: A Novel about Life in a Twentieth-Century Jewish American Village. His reporting in Japan was supported in part by the Fulbright Program and the Asian Cultural Council."},
{"url": "http://www.shirky.com/writings/herecomeseverybody/group_enemy.html", "link_title": "A Group Is Its Own Worst Enemy", "text": "Good morning, everybody. I want to talk this morning about social software ...there's a surprise. I want to talk about a pattern I've seen over and over again in social software that supports large and long-lived groups. And that pattern is the pattern described in the title of this talk: \"A Group Is Its Own Worst Enemy.\"\n\nIn particular, I want to talk about what I now think is one of the core challenges for designing large-scale social software. Let me offer a definition of social software, because it's a term that's still fairly amorphous. My definition is fairly simple: It's software that supports group interaction. I also want to emphasize, although that's a fairly simple definition, how radical that pattern is. The Internet supports lots of communications patterns, principally point-to-point and two-way, one-to-many outbound, and many-to-many two-way.\n\nPrior to the Internet, we had lots of patterns that supported point-to-point two-way. We had telephones, we had the telegraph. We were familiar with technological mediation of those kinds of conversations. Prior to the Internet, we had lots of patterns that supported one-way outbound. I could put something on television or the radio, I could publish a newspaper. We had the printing press. So although the Internet does good things for those patterns, they're patterns we knew from before.\n\nPrior to the Internet, the last technology that had any real effect on the way people sat down and talked together was the table. There was no technological mediation for group conversations. The closest we got was the conference call, which never really worked right -- \"Hello? Do I push this button now? Oh, shoot, I just hung up.\" It's not easy to set up a conference call, but it's very easy to email five of your friends and say \"Hey, where are we going for pizza?\" So ridiculously easy group forming is really news.\n\nWe've had social software for 40 years at most, dated from the Plato BBS system, and we've only had 10 years or so of widespread availability, so we're just finding out what works. We're still learning how to make these kinds of things.\n\nNow, software that supports group interaction is a fundamentally unsatisfying definition in many ways, because it doesn't point to a specific class of technology. If you look at email, it obviously supports social patterns, but it can also support a broadcast pattern. If I'm a spammer, I'm going to mail things out to a million people, but they're not going to be talking to one another, and I'm not going to be talking to them -- spam is email, but it isn't social. If I'm mailing you, and you're mailing me back, we're having point-to-point and two-way conversation, but not one that creates group dynamics.\n\nSo email doesn't necessarily support social patterns, group patterns, although it can. Ditto a weblog. If I'm Glenn Reynolds, and I'm publishing something with Comments Off and reaching a million users a month, that's really broadcast. It's interesting that I can do it as a single individual, but the pattern is closer to MSNBC than it is to a conversation. If it's a cluster of half a dozen LiveJournal users, on the other hand, talking about their lives with one another, that's social. So, again, weblogs are not necessarily social, although they can support social patterns.\n\nNevertheless, I think that definition is the right one, because it recognizes the fundamentally social nature of the problem. Groups are a run-time effect. You cannot specify in advance what the group will do, and so you can't substantiate in software everything you expect to have happen.\n\nNow, there's a large body of literature saying \"We built this software, a group came and used it, and they began to exhibit behaviors that surprised us enormously, so we've gone and documented these behaviors.\" Over and over and over again this pattern comes up. (I hear Stewart [Brand, of the WELL] laughing.) The WELL is one of those places where this pattern came up over and over again.\n\nThis talk is in three parts. The best explanation I have found for the kinds of things that happen when groups of humans interact is psychological research that predates the Internet, so the first part is going to be about W.R. Bion's research, which I will talk about in a moment, research that I believe explains how and why a group is its own worst enemy.\n\nThe second part is: Why now? What's going on now that makes this worth thinking about? I think we're seeing a revolution in social software in the current environment that's really interesting.\n\nAnd third, I want to identify some things, about half a dozen things, in fact, that I think are core to any software that supports larger, long-lived groups.\n\nPart One: How is a group its own worst enemy?\n\nSo, Part One. The best explanation I have found for the ways in which this pattern establishes itself, the group is its own worst enemy, comes from a book by W.R. Bion called \"Experiences in Groups,\" written in the middle of the last century.\n\nBion was a psychologist who was doing group therapy with groups of neurotics. (Drawing parallels between that and the Internet is left as an exercise for the reader.) The thing that Bion discovered was that the neurotics in his care were, as a group, conspiring to defeat therapy.\n\nThere was no overt communication or coordination. But he could see that whenever he would try to do anything that was meant to have an effect, the group would somehow quash it. And he was driving himself crazy, in the colloquial sense of the term, trying to figure out whether or not he should be looking at the situation as: Are these individuals taking action on their own? Or is this a coordinated group?\n\nHe could never resolve the question, and so he decided that the unresolvability of the question was the answer. To the question: Do you view groups of people as aggregations of individuals or as a cohesive group, his answer was: \"Hopelessly committed to both.\"\n\nHe said that humans are fundamentally individual, and also fundamentally social. Every one of us has a kind of rational decision-making mind where we can assess what's going on and make decisions and act on them. And we are all also able to enter viscerally into emotional bonds with other groups of people that transcend the intellectual aspects of the individual.\n\nIn fact, Bion was so convinced that this was the right answer that the image he put on the front cover of his book was a Necker cube, one of those cubes that you can look at and make resolve in one of two ways, but you can never see both views of the cube at the same time. So groups can be analyzed both as collections of individuals and having this kind of emotive group experience.\n\nNow, it's pretty easy to see how groups of people who have formal memberships, groups that have been labeled and named like \"I am a member of such-and-such a guild in a massively multi-player online role-playing game,\" it's easy to see how you would have some kind of group cohesion there. But Bion's thesis is that this effect is much, much deeper, and kicks in much, much sooner than many of us expect. So I want to illustrate this with a story, and to illustrate the illustration, I'll use a story from your life. Because even if I don't know you, I know what I'm about to describe has happened to you.\n\nYou are at a party, and you get bored. You say \"This isn't doing it for me anymore. I'd rather be someplace else. I'd rather be home asleep. The people I wanted to talk to aren't here.\" Whatever. The party fails to meet some threshold of interest. And then a really remarkable thing happens: You don't leave. You make a decision \"I don't like this.\" If you were in a bookstore and you said \"I'm done,\" you'd walk out. If you were in a coffee shop and said \"This is boring,\" you'd walk out.\n\nYou're sitting at a party, you decide \"I don't like this; I don't want to be here.\" And then you don't leave. That kind of social stickiness is what Bion is talking about.\n\nAnd then, another really remarkable thing happens. Twenty minutes later, one person stands up and gets their coat, and what happens? Suddenly everyone is getting their coats on, all at the same time. Which means that everyone had decided that the party was not for them, and no one had done anything about it, until finally this triggering event let the air out of the group, and everyone kind of felt okay about leaving.\n\nThis effect is so steady it's sometimes called the paradox of groups. It's obvious that there are no groups without members. But what's less obvious is that there are no members without a group. Because what would you be a member of?\n\nSo there's this very complicated moment of a group coming together, where enough individuals, for whatever reason, sort of agree that something worthwhile is happening, and the decision they make at that moment is: This is good and must be protected. And at that moment, even if it's subconscious, you start getting group effects. And the effects that we've seen come up over and over and over again in online communities.\n\nNow, Bion decided that what he was watching with the neurotics was the group defending itself against his attempts to make the group do what they said they were supposed to do. The group was convened to get better, this group of people was in therapy to get better. But they were defeating that. And he said, there are some very specific patterns that they're entering into to defeat the ostensible purpose of the group meeting together. And he detailed three patterns.\n\nThe first is sex talk, what he called, in his mid-century prose, \"A group met for pairing off.\" And what that means is, the group conceives of its purpose as the hosting of flirtatious or salacious talk or emotions passing between pairs of members.\n\nYou go on IRC and you scan the channel list, and you say \"Oh, I know what that group is about, because I see the channel label.\" And you go into the group, you will also almost invariably find that it's about sex talk as well. Not necessarily overt. But that is always in scope in human conversations, according to Bion. That is one basic pattern that groups can always devolve into, away from the sophisticated purpose and towards one of these basic purposes.\n\nThe second basic pattern that Bion detailed: The identification and vilification of external enemies. This is a very common pattern. Anyone who was around the Open Source movement in the mid-Nineties could see this all the time. If you cared about Linux on the desktop, there was a big list of jobs to do. But you could always instead get a conversation going about Microsoft and Bill Gates. And people would start bleeding from their ears, they would get so mad.\n\nIf you want to make it better, there's a list of things to do. It's Open Source, right? Just fix it. \"No, no, Microsoft and Bill Gates grrrrr ...\", the froth would start coming out. The external enemy -- nothing causes a group to galvanize like an external enemy.\n\nSo even if someone isn't really your enemy, identifying them as an enemy can cause a pleasant sense of group cohesion. And groups often gravitate towards members who are the most paranoid and make them leaders, because those are the people who are best at identifying external enemies.\n\nThe third pattern Bion identified: Religious veneration. The nomination and worship of a religious icon or a set of religious tenets. The religious pattern is, essentially, we have nominated something that's beyond critique. You can see this pattern on the Internet any day you like. Go onto a Tolkein newsgroup or discussion forum, and try saying \"You know, The Two Towers is a little dull. I mean loooong. We didn't need that much description about the forest, because it's pretty much the same forest all the way.\"\n\nTry having that discussion. On the door of the group it will say: \"This is for discussing the works of Tolkein.\" Go in and try and have that discussion.\n\nNow, in some places people say \"Yes, but it needed to, because it had to convey the sense of lassitude,\" or whatever. But in most places you'll simply be flamed to high heaven, because you're interfering with the religious text.\n\nSo these are human patterns that have shown up on the Internet, not because of the software, but because it's being used by humans. Bion has identified this possibility of groups sandbagging their sophisticated goals with these basic urges. And what he finally came to, in analyzing this tension, is that group structure is necessary. Robert's Rules of Order are necessary. Constitutions are necessary. Norms, rituals, laws, the whole list of ways that we say, out of the universe of possible behaviors, we're going to draw a relatively small circle around the acceptable ones.\n\nHe said the group structure is necessary to defend the group from itself. Group structure exists to keep a group on target, on track, on message, on charter, whatever. To keep a group focused on its own sophisticated goals and to keep a group from sliding into these basic patterns. Group structure defends the group from the action of its own members.\n\nIn the Seventies -- this is a pattern that's shown up on the network over and over again -- in the Seventies, a BBS called Communitree launched, one of the very early dial-up BBSes. This was launched when people didn't own computers, institutions owned computers.\n\nCommunitree was founded on the principles of open access and free dialogue. \"Communitree\" -- the name just says \"California in the Seventies.\" And the notion was, effectively, throw off structure and new and beautiful patterns will arise.\n\nAnd, indeed, as anyone who has put discussion software into groups that were previously disconnected has seen, that does happen. Incredible things happen. The early days of Echo, the early days of usenet, the early days of Lucasfilms Habitat, over and over again, you see all this incredible upwelling of people who suddenly are connected in ways they weren't before.\n\nAnd then, as time sets in, difficulties emerge. In this case, one of the difficulties was occasioned by the fact that one of the institutions that got hold of some modems was a high school. And who, in 1978, was hanging out in the room with the computer and the modems in it, but the boys of that high school. And the boys weren't terribly interested in sophisticated adult conversation. They were interested in fart jokes. They were interested in salacious talk. They were interested in running amok and posting four-letter words and nyah-nyah-nyah, all over the bulletin board.\n\nAnd the adults who had set up Communitree were horrified, and overrun by these students. The place that was founded on open access had too much open access, too much openness. They couldn't defend themselves against their own users. The place that was founded on free speech had too much freedom. They had no way of saying \"No, that's not the kind of free speech we meant.\"\n\nBut that was a requirement. In order to defend themselves against being overrun, that was something that they needed to have that they didn't have, and as a result, they simply shut the site down.\n\nNow you could ask whether or not the founders' inability to defend themselves from this onslaught, from being overrun, was a technical or a social problem. Did the software not allow the problem to be solved? Or was it the social configuration of the group that founded it, where they simply couldn't stomach the idea of adding censorship to protect their system. But in a way, it doesn't matter, because technical and social issues are deeply intertwined. There's no way to completely separate them.\n\nWhat matters is, a group designed this and then was unable, in the context they'd set up, partly a technical and partly a social context, to save it from this attack from within. And attack from within is what matters. Communitree wasn't shut down by people trying to crash or syn-flood the server. It was shut down by people logging in and posting, which is what the system was designed to allow. The technological pattern of normal use and attack were identical at the machine level, so there was no way to specify technologically what should and shouldn't happen. Some of the users wanted the system to continue to exist and to provide a forum for discussion. And other of the users, the high school boys, either didn't care or were actively inimical. And the system provided no way for the former group to defend itself from the latter.\n\nNow, this story has been written many times. It's actually frustrating to see how many times it's been written. You'd hope that at some point that someone would write it down, and they often do, but what then doesn't happen is other people don't read it.\n\nThe most charitable description of this repeated pattern is \"learning from experience.\" But learning from experience is the worst possible way to learn something. Learning from experience is one up from remembering. That's not great. The best way to learn something is when someone else figures it out and tells you: \"Don't go in that swamp. There are alligators in there.\"\n\nLearning from experience about the alligators is lousy, compared to learning from reading, say. There hasn't been, unfortunately, in this arena, a lot of learning from reading. And so, lessons from Lucasfilms' Habitat, written in 1990, reads a lot like Rose Stone's description of Communitree from 1978.\n\nThis pattern has happened over and over and over again. Someone built the system, they assumed certain user behaviors. The users came on and exhibited different behaviors. And the people running the system discovered to their horror that the technological and social issues could not in fact be decoupled.\n\nThere's a great document called \"LambdaMOO Takes a New Direction,\" which is about the wizards of LambdaMOO, Pavel Curtis's Xerox PARC experiment in building a MUD world. And one day the wizards of LambdaMOO announced \"We've gotten this system up and running, and all these interesting social effects are happening. Henceforth we wizards will only be involved in technological issues. We're not going to get involved in any of that social stuff.\"\n\nAnd then, I think about 18 months later -- I don't remember the exact gap of time -- they come back. The wizards come back, extremely cranky. And they say: \"What we have learned from you whining users is that we can't do what we said we would do. We cannot separate the technological aspects from the social aspects of running a virtual world.\n\n\"So we're back, and we're taking wizardly fiat back, and we're going to do things to run the system. We are effectively setting ourselves up as a government, because this place needs a government, because without us, the place was falling apart.\"\n\nPeople who work on social software are closer in spirit to economists and political scientists than they are to people making compilers. They both look like programming, but when you're dealing with groups of people as one of your run-time phenomena, that is an incredibly different practice. In the political realm, we would call these kinds of crises a constitutional crisis. It's what happens when the tension between the individual and the group, and the rights and responsibilities of individuals and groups, gets so serious that something has to be done.\n\nAnd the worst crisis is the first crisis, because it's not just \"We need to have some rules.\" It's also \"We need to have some rules for making some rules.\" And this is what we see over and over again in large and long-lived social software systems. Constitutions are a necessary component of large, long-lived, heterogenous groups.\n\nGeoff Cohen has a great observation about this. He said \"The likelihood that any unmoderated group will eventually get into a flame-war about whether or not to have a moderator approaches one as time increases.\" As a group commits to its existence as a group, and begins to think that the group is good or important, the chance that they will begin to call for additional structure, in order to defend themselves from themselves, gets very, very high.\n\nPart Two: Why now?\n\nIf these things I'm saying have happened so often before, have been happening and been documented and we've got psychological literature that predates the Internet, what's going on now that makes this important?\n\nI can't tell you precisely why, but observationally there is a revolution in social software going on. The number of people writing tools to support or enhance group collaboration or communication is astonishing.\n\nThe web turned us all into size queens for six or eight years there. It was loosely coupled, it was stateless, it scaled like crazy, and everything became about How big can you get? \"How many users does Yahoo have? How many customers does Amazon have? How many readers does MSNBC have?\" And the answer could be \"Really a lot!\" But it could only be really a lot if you didn't require MSNBC to be answering those readers, and you didn't require those readers to be talking to one another.\n\nThe downside of going for size and scale above all else is that the dense, interconnected pattern that drives group conversation and collaboration isn't supportable at any large scale. Less is different -- small groups of people can engage in kinds of interaction that large groups can't. And so we blew past that interesting scale of small groups. Larger than a dozen, smaller than a few hundred, where people can actually have these conversational forms that can't be supported when you're talking about tens of thousands or millions of users, at least in a single group.\n\nWe've had things like mailing lists and BBSes for a long time, and more recently we've had IM, we've had these various patterns. And now, all of a sudden, these things are popping up. We've gotten weblogs and wikis, and I think, even more importantly, we're getting platform stuff. We're getting RSS. We're getting shared Flash objects. We're getting ways to quickly build on top of some infrastructure we can take for granted, that lets us try new things very rapidly.\n\nI was talking to Stewart Butterfield about the chat application they're trying here. I said \"Hey, how's that going?\" He said: \"Well, we only had the idea for it two weeks ago. So this is the launch.\" When you can go from \"Hey, I've got an idea\" to \"Let's launch this in front of a few hundred serious geeks and see how it works,\" that suggests that there's a platform there that is letting people do some really interesting things really quickly. It's not that you couldn't have built a similar application a couple of years ago, but the cost would have been much higher. And when you lower costs, interesting new kinds of things happen.\n\nSo the first answer to Why Now? is simply \"Because it's time.\" I can't tell you why it took as long for weblogs to happen as it did, except to say it had absolutely nothing to do with technology. We had every bit of technology we needed to do weblogs the day Mosaic launched the first forms-capable browser. Every single piece of it was right there. Instead, we got Geocities. Why did we get Geocities and not weblogs? We didn't know what we were doing.\n\nOne was a bad idea, the other turns out to be a really good idea. It took a long time to figure out that people talking to one another, instead of simply uploading badly-scanned photos of their cats, would be a useful pattern.\n\nWe got the weblog pattern in around '96 with Drudge. We got weblog platforms starting in '98. The thing really was taking off in 2000. By last year, everyone realized: Omigod, this thing is going mainstream, and it's going to change everything.\n\nThe vertigo moment for me was when Phil Gyford launched the Pepys weblog, Samuel Pepys' diaries of the 1660's turned into a weblog form, with a new post every day from Pepys' diary. What that said to me was: Phil was asserting, and I now believe, that weblogs will be around for at least 10 years, because that's how long Pepys kept a diary. And that was this moment of projecting into the future: This is now infrastructure we can take for granted.\n\nWhy was there an eight-year gap between a forms-capable browser and the Pepys diaries? I don't know. It just takes a while for people to get used to these ideas.\n\nSo, first of all, this is a revolution in part because it is a revolution. We've internalized the ideas and people are now working with them. Second, the things that people are now building are web-native.\n\nWhen you got social software on the web in the mid-Nineties, a lot of it was: \"This is the Giant Lotus Dreadnought, now with New Lightweight Web Interface!\" It never felt like the web. It felt like this hulking thing with a little, you know, \"Here's some icons. Don't look behind the curtain.\"\n\nA weblog is web-native. It's the web all the way in. A wiki is a web-native way of hosting collaboration. It's lightweight, it's loosely coupled, it's easy to extend, it's easy to break down. And it's not just the surface, like oh, you can just do things in a form. It assumes http is transport. It assumes markup in the coding. RSS is a web-native way of doing syndication. So we're taking all of these tools and we're extending them in a way that lets us build new things really quickly.\n\nThird, in David Weinberger's felicitous phrase, we can now start to have a Small Pieces Loosely Joined pattern. It's really worthwhile to look into what Joi Ito is doing with the Emergent Democracy movement, even if you're not interested in the themes of emerging democracy. This started because a conversation was going on, and Ito said \"I am frustrated. I'm sitting here in Japan, and I know all of these people are having these conversations in real-time with one another. I want to have a group conversation, too. I'll start a conference call.\n\n\"But since conference calls are so lousy on their own, I'm going to bring up a chat window at the same time.\" And then, in the first meeting, I think it was Pete Kaminski said \"Well, I've also opened up a wiki, and here's the URL.\" And he posted it in the chat window. And people can start annotating things. People can start adding bookmarks; here are the lists.\n\nSo, suddenly you've got this meeting, which is going on in three separate modes at the same time, two in real-time and one annotated. So you can have the conference call going on, and you know how conference calls are. Either one or two people dominate it, or everyone's like \"Oh, can I -- no, but --\", everyone interrupting and cutting each other off.\n\nIt's very difficult to coordinate a conference call, because people can't see one another, which makes it hard to manage the interrupt logic. In Joi's conference call, the interrupt logic got moved to the chat room. People would type \"Hand,\" and the moderator of the conference call will then type \"You're speaking next,\" in the chat. So the conference call flowed incredibly smoothly.\n\nMeanwhile, in the chat, people are annotating what people are saying. \"Oh, that reminds me of So-and-so's work.\" Or \"You should look at this URL...you should look at that ISBN number.\" In a conference call, to read out a URL, you have to spell it out -- \"No, no, no, it's w w w dot net dash...\" In a chat window, you get it and you can click on it right there. You can say, in the conference call or the chat: \"Go over to the wiki and look at this.\"\n\nThis is a broadband conference call, but it isn't a giant thing. It's just three little pieces of software laid next to each other and held together with a little bit of social glue. This is an incredibly powerful pattern. It's different from: Let's take the Lotus juggernaut and add a web front-end.\n\nAnd finally, and this is the thing that I think is the real freakout, is ubiquity. The web has been growing for a long, long time. And so some people had web access, and then lots of people had web access, and then most people had web access.\n\nBut something different is happening now. In many situations, all people have access to the network. And \"all\" is a different kind of amount than \"most.\" \"All\" lets you start taking things for granted.\n\nNow, the Internet isn't everywhere in the world. It isn't even everywhere in the developed world. But for some groups of people -- students, people in high-tech offices, knowledge workers -- everyone they work with is online. Everyone they're friends with is online. Everyone in their family is online.\n\nAnd this pattern of ubiquity lets you start taking this for granted. Bill Joy once said \"My method is to look at something that seems like a good idea and assume it's true.\" We're starting to see software that simply assumes that all offline groups will have an online component, no matter what.\n\nIt is now possible for every grouping, from a Girl Scout troop on up, to have an online component, and for it to be lightweight and easy to manage. And that's a different kind of thing than the old pattern of \"online community.\" I have this image of two hula hoops, the old two-hula hoop world, where my real life is over here, and my online life is over there, and there wasn't much overlap between them. If the hula hoops are swung together, and everyone who's offline is also online, at least from my point of view, that's a different kind of pattern.\n\nThere's a second kind of ubiquity, which is the kind we're enjoying here thanks to Wifi. If you assume whenever a group of people are gathered together, that they can be both face to face and online at the same time, you can start to do different kinds of things. I now don't run a meeting without either having a chat room or a wiki up and running. Three weeks ago I ran a meeting for the Library of Congress. We had a wiki, set up by Socialtext, to capture a large and very dense amount of technical information on long-term digital preservation.\n\nThe people who organized the meeting had never used a wiki before, and now the Library of Congress is talking as if they always had a wiki for their meetings, and are assuming it's going to be at the next meeting as well -- the wiki went from novel to normal in a couple of days.\n\nIt really quickly becomes an assumption that a group can do things like \"Oh, I took my PowerPoint slides, I showed them, and then I dumped them into the wiki. So now you can get at them.\" It becomes a sort of shared repository for group memory. This is new. These kinds of ubiquity, both everyone is online, and everyone who's in a room can be online together at the same time, can lead to new patterns.\n\nPart Three: What can we take for granted?\n\nIf these assumptions are right, one that a group is its own worst enemy, and two, we're seeing this explosion of social software, what should we do? Is there anything we can say with any certainty about building social software, at least for large and long-lived groups?\n\nI think there is. A little over 10 years ago, I quit my day job, because Usenet was so interesting, I thought: This is really going to be big. And I actually wrote a book about net culture at the time: Usenet, the Well, Echo, IRC and so forth. It launched in April of '95, just as that world was being washed away by the web. But it was my original interest, so I've been looking at this problem in one way or another for 10 years, and I've been looking at it pretty hard for the a year and a half or so.\n\nSo there's this question \"What is required to make a large, long-lived online group successful?\" and I think I can now answer with some confidence: \"It depends.\" I'm hoping to flesh that answer out a little bit in the next ten years.\n\nBut I can at least say some of the things it depends on. The Calvinists had a doctrine of natural grace and supernatural grace. Natural grace was \"You have to do all the right things in the world to get to heaven...\" and supernatural grace was \"...and God has to anoint you.\" And you never knew if you had supernatural grace or not. This was their way of getting around the fact that the Book of Revelations put an upper limit on the number of people who were going to heaven.\n\nSocial software is like that. You can find the same piece of code running in many, many environments. And sometimes it works and sometimes it doesn't. So there is something supernatural about groups being a run-time experience.\n\nThe normal experience of social software is failure. If you go into Yahoo groups and you map out the subscriptions, it is, unsurprisingly, a power law. There's a small number of highly populated groups, a moderate number of moderately populated groups, and this long, flat tail of failure. And the failure is inevitably more than 50% of the total mailing lists in any category. So it's not like a cake recipe. There's nothing you can do to make it come out right every time.\n\nThere are, however, I think, about half a dozen things that are broadly true of all the groups I've looked at and all the online constitutions I've read for software that supports large and long-lived groups. And I'd break that list in half. I'd say, if you are going to create a piece of social software designed to support large groups, you have to accept three things, and design for four things.\n\n1.) Of the things you have to accept, the first is that you cannot completely separate technical and social issues. There are two attractive patterns. One says, we'll handle technology over `here, we'll do social issues there. We'll have separate mailing lists with separate discussion groups, or we'll have one track here and one track there. This doesn't work. It's never been stated more clearly than in the pair of documents called \"LambdaMOO Takes a New Direction.\" I can do no better than to point you to those documents.\n\nBut recently we've had this experience where there was a social software discussion list, and someone said \"I know, let's set up a second mailing list for technical issues.\" And no one moved from the first list, because no one could fork the conversation between social and technical issues, because the conversation can't be forked.\n\nThe other pattern that's very, very attractive -- anybody who looks at this stuff has the same epiphany, which is: \"Omigod, this software is determining what people do!\" And that is true, up to a point. But you cannot completely program social issues either. So you can't separate the two things, and you also can't specify all social issues in technology. The group is going to assert its rights somehow, and you're going to get this mix of social and technological effects.\n\nSo the group is real. It will exhibit emergent effects. It can't be ignored, and it can't be programmed, which means you have an ongoing issue. And the best pattern, or at least the pattern that's worked the most often, is to put into the hands of the group itself the responsibility for defining what value is, and defending that value, rather than trying to ascribe those things in the software upfront.\n\n2.) The second thing you have to accept: Members are different than users. A pattern will arise in which there is some group of users that cares more than average about the integrity and success of the group as a whole. And that becomes your core group, Art Kleiner's phrase for \"the group within the group that matters most.\"\n\nThe core group on Communitree was undifferentiated from the group of random users that came in. They were separate in their own minds, because they knew what they wanted to do, but they couldn't defend themselves against the other users. But in all successful online communities that I've looked at, a core group arises that cares about and gardens effectively. Gardens the environment, to keep it growing, to keep it healthy.\n\nNow, the software does not always allow the core group to express itself, which is why I say you have to accept this. Because if the software doesn't allow the core group to express itself, it will invent new ways of doing so.\n\nOn alt.folklore.urban , the discussion group about urban folklore on Usenet, there was a group of people who hung out there and got to be friends. And they came to care about the existence of AFU, to the point where, because Usenet made no distinction between members in good standing and drive-by users, they set up a mailing list called The Old Hats. The mailing list was for meta-discussion, discussion about AFU, so they could coordinate efforts formally if they were going to troll someone or flame someone or ignore someone, on the mailing list.\n\nAddendum, July 2, 2003: A longtime a.f.u participant says that the Old Hat list was created to allow the Silicon Valley-dwelling members to plan a barbecue, so that they could add a face-to-face dimension to their virtual interaction. The use of the list as a backstage area for discussing the public newsgroup arose after the fact.\n\nThen, as Usenet kept growing, many newcomers came along and seemed to like the environment, because it was well-run. In order to defend themselves from the scaling issues that come from of adding a lot of new members to the Old Hats list, they said \"We're starting a second list, called the Young Hats.\"\n\nSo they created this three-tier system, not dissimilar to the tiers of anonymous cowards, logged-in users, and people with high karma on Slashdot. But because Usenet didn't let them do it in the software, they brought in other pieces of software, these mailing lists, that they needed to build the structure. So you don't get the program users, the members in good standing will find one another and be recognized to one another.\n\n3.) The third thing you need to accept: The core group has rights that trump individual rights in some situations. This pulls against the libertarian view that's quite common on the network, and it absolutely pulls against the one person/one vote notion. But you can see examples of how bad an idea voting is when citizenship is the same as ability to log in.\n\nIn the early Nineties, a proposal went out to create a Usenet news group for discussing Tibetan culture, called soc.culture.tibet. And it was voted down, in large part because a number of Chinese students who had Internet access voted it down, on the logic that Tibet wasn't a country; it was a region of China. And in their view, since Tibet wasn't a country, there oughtn't be any place to discuss its culture, because that was oxymoronic.\n\nNow, everyone could see that this was the wrong answer. The people who wanted a place to discuss Tibetan culture should have it. That was the core group. But because the one person/one vote model on Usenet said \"Anyone who's on Usenet gets to vote on any group,\" sufficiently contentious groups could simply be voted away.\n\nImagine today if, in the United States, Internet users had to be polled before any anti-war group could be created. Or French users had to be polled before any pro-war group could be created. The people who want to have those discussions are the people who matter. And absolute citizenship, with the idea that if you can log in, you are a citizen, is a harmful pattern, because it is the tyranny of the majority.\n\nSo the core group needs ways to defend itself -- both in getting started and because of the effects I talked about earlier -- the core group needs to defend itself so that it can stay on its sophisticated goals and away from its basic instincts.\n\nThe Wikipedia has a similar system today, with a volunteer fire department, a group of people who care to an unusual degree about the success of the Wikipedia. And they have enough leverage, because of the way wikis work, they can always roll back graffiti and so forth, that that thing has stayed up despite repeated attacks. So leveraging the core group is a really powerful system.\n\nNow, when I say these are three things you have to accept, I mean you have to accept them. Because if you don't accept them upfront, they'll happen to you anyway. And then you'll end up writing one of those documents that says \"Oh, we launched this and we tried it, and then the users came along and did all these weird things. And now we're documenting it so future ages won't make this mistake.\" Even though you didn't read the thing that was written in 1978.\n\nAll groups of any integrity have a constitution. The constitution is always partly formal and partly informal. At the very least, the formal part is what's substantiated in code -- \"the software works this way.\"\n\nThe informal part is the sense of \"how we do it around here.\" And no matter how is substantiated in code or written in charter, whatever, there will always be an informal part as well. You can't separate the two.\n\nFour Things to Design For\n\n1.) If you were going to build a piece of social software to support large and long-lived groups, what would you design for? The first thing you would design for is handles the user can invest in.\n\nNow, I say \"handles,\" because I don't want to say \"identity,\" because identity has suddenly become one of those ideas where, when you pull on the little thread you want, this big bag of stuff comes along with it. Identity is such a hot-button issue now, but for the lightweight stuff required for social software, its really just a handle that matters.\n\nIt's pretty widely understood that anonymity doesn't work well in group settings, because \"who said what when\" is the minimum requirement for having a conversation. What's less well understood is that weak pseudonymity doesn't work well, either. Because I need to associate who's saying something to me now with previous conversations.\n\nThe world's best reputation management system is right here, in the brain. And actually, it's right here, in the back, in the emotional part of the brain. Almost all the work being done on reputation systems today is either trivial or useless or both, because reputations aren't linearizable, and they're not portable.\n\nThere are people who cheat on their spouse but not at cards, and vice versa, and both and neither. Reputation is not necessarily portable from one situation to another, and it's not easily expressed.\n\neBay has done us all an enormous disservice, because eBay works in non-iterated atomic transactions, which are the opposite of social situations. eBay's reputation system works incredibly well, because it starts with a linearizable transaction -- \"How much money for how many Smurfs?\" -- and turns that into a metric that's equally linear.\n\nThat doesn't work well in social situations. If you want a good reputation system, just let me remember who you are. And if you do me a favor, I'll remember it. And I won't store it in the front of my brain, I'll store it here, in the back. I'll just get a good feeling next time I get email from you; I won't even remember why. And if you do me a disservice and I get email from you, my temples will start to throb, and I won't even remember why. If you give users a way of remembering one another, reputation will happen, and that requires nothing more than simple and somewhat persistent handles.\n\nUsers have to be able to identify themselves and there has to be a penalty for switching handles. The penalty for switching doesn't have to be total. But if I change my handle on the system, I have to lose some kind of reputation or some kind of context. This keeps the system functioning.\n\nNow, this pulls against the sense that we've had since the early psychological writings about the Internet. \"Oh, on the Internet we're all going to be changing identities and genders like we change our socks.\"\n\nAnd you see things like the Kaycee Nicole story, where a woman in Kansas pretended to be a high school student, and then because the invented high school student's friends got so emotionally involved, she then tried to kill the Kaycee Nicole persona off. \"Oh, she's got cancer and she's dying and it's all very tragic.\" And of course, everyone wanted to fly to meet her. So then she sort of panicked and vanished. And a bunch of places on the Internet, particularly the MetaFilter community, rose up to find out what was going on, and uncovered the hoax. It was sort of a distributed detective movement.\n\nNow a number of people point to this and say \"See, I told you about that identity thing!\" But the Kaycee Nicole story is this: changing your identity is really weird. And when the community understands that you've been doing it and you're faking, that is seen as a huge and violent transgression. And they will expend an astonishing amount of energy to find you and punish you. So identity is much less slippery than the early literature would lead us to believe.\n\n2.) Second, you have to design a way for there to be members in good standing. Have to design some way in which good works get recognized. The minimal way is, posts appear with identity. You can do more sophisticated things like having formal karma or \"member since.\"\n\nI'm on the fence about whether or not this is a design or accepting. Because in a way I think members in good standing will rise. But more and more of the systems I'm seeing launching these days are having some kind of additional accretion so you can tell how much involvement members have with the system.\n\nThere's an interesting pattern I'm seeing among the music-sharing group that operates between Tokyo and Hong Kong. They operate on a mailing list, which they set up for themselves. But when they're trading music, what they're doing is, they're FedExing one another 180-gig hard-drives. So you're getting .wav files and not MP3s, and you're getting them in bulk.\n\nNow, you can imagine that such a system might be a target for organizations that would frown on this activity. So when you join that group, your user name is appended with the user name of the person who is your sponsor. You can't get in without your name being linked to someone else. You can see immediately the reputational effects going on there, just from linking two handles.\n\nSo in that system, you become a member in good standing when your sponsor link goes away and you're there on your own report. If, on the other hand, you defect, not only are you booted, but your sponsor is booted. There are lots and lots of lightweight ways to accept and work with the idea of member in good standing.\n\n3.) Three, you need barriers to participation. This is one of the things that killed Usenet. You have to have some cost to either join or participate, if not at the lowest level, then at higher levels. There needs to be some kind of segmentation of capabilities.\n\nNow, the segmentation can be total -- you're in or you're out, as with the music group I just listed. Or it can be partial -- anyone can read Slashdot, anonymous cowards can post, non-anonymous cowards can post with a higher rating. But to moderate, you really have to have been around for a while.\n\nIt has to be hard to do at least some things on the system for some users, or the core group will not have the tools that they need to defend themselves.\n\nNow, this pulls against the cardinal virtue of ease of use. But ease of use is wrong. Ease of use is the wrong way to look at the situation, because you've got the Necker cube flipped in the wrong direction. The user of social software is the group, not the individual.\n\nI think we've all been to meetings where everyone had a really good time, we're all talking to one another and telling jokes and laughing, and it was a great meeting, except we got nothing done. Everyone was amusing themselves so much that the group's goal was defeated by the individual interventions.\n\nThe user of social software is the group, and ease of use should be for the group. If the ease of use is only calculated from the user's point of view, it will be difficult to defend the group from the \"group is its own worst enemy\" style attacks from within.\n\n4.) And, finally, you have to find a way to spare the group from scale. Scale alone kills conversations, because conversations require dense two-way conversations. In conversational contexts, Metcalfe's law is a drag. The fact that the amount of two-way connections you have to support goes up with the square of the users means that the density of conversation falls off very fast as the system scales even a little bit. You have to have some way to let users hang onto the less is more pattern, in order to keep associated with one another.\n\nThis is an inverse value to scale question. Think about your Rolodex. A thousand contacts, maybe 150 people you can call friends, 30 people you can call close friends, two or three people you'd donate a kidney to. The value is inverse to the size of the group. And you have to find some way to protect the group within the context of those effects.\n\nSometimes you can do soft forking. Live Journal does the best soft forking of any software I've ever seen, where the concepts of \"you\" and \"your group\" are pretty much intertwingled. The average size of a Live Journal group is about a dozen people. And the median size is around five.\n\nBut each user is a little bit connected to other such clusters, through their friends, and so while the clusters are real, they're not completely bounded -- there's a soft overlap which means that though most users participate in small groups, most of the half-million LiveJournal users are connected to one another through some short chain.\n\nIRC channels and mailing lists are self-moderating with scale, because as the signal to noise ratio gets worse, people start to drop off, until it gets better, so people join, and so it gets worse. You get these sort of oscillating patterns. But it's self-correcting.\n\nAnd then my favorite pattern is from MetaFilter, which is: When we start seeing effects of scale, we shut off the new user page. \"Someone mentions us in the press and how great we are? Bye!\" That's a way of raising the bar, that's creating a threshold of participation. And anyone who bookmarks that page and says \"You know, I really want to be in there; maybe I'll go back later,\" that's the kind of user MeFi wants to have.\n\nYou have to find some way to protect your own users from scale. This doesn't mean the scale of the whole system can't grow. But you can't try to make the system large by taking individual conversations and blowing them up like a balloon; human interaction, many to many interaction, doesn't blow up like a balloon. It either dissipates, or turns into broadcast, or collapses. So plan for dealing with scale in advance, because it's going to happen anyway.\n\nNow, those four things are of course necessary but not sufficient conditions. I propose them more as a platform for building the interesting differences off. There are lots and lots and lots of other effects that make different bits of software interesting enough that you would want to keep more than one kind of pattern around. But those are commonalities I'm seeing across a range of social software for large and long-lived groups.\n\nIn addition, you can do all sorts of things with explicit clustering, whether it's guilds in massively multi-player games, or communities on Live Journal or what have you. You can do things with conversational artifacts, where the group participation leaves behind some record. The Wikipedia right now, the group collaborated online encyclopedia is the most interesting conversational artifact I know of, where product is a result of process. Rather than \"We're specifically going to get together and create this presentation\" it's just \"What's left is a record of what we said.\"\n\nThere are all these things, and of course they differ platform to platform. But there is this, I believe, common core of things that will happen whether you plan for them or not, and things you should plan for, that I think are invariant across large communal software.\n\nWriting social software is hard. And, as I said, the act of writing social software is more like the work of an economist or a political scientist. And the act of hosting social software, the relationship of someone who hosts it is more like a relationship of landlords to tenants than owners to boxes in a warehouse.\n\nThe people using your software, even if you own it and pay for it, have rights and will behave as if they have rights. And if you abrogate those rights, you'll hear about it very quickly.\n\nThat's part of the problem that the John Hegel theory of community -- community leads to content, which leads to commerce -- never worked. Because lo and behold, no matter who came onto the Clairol chat boards, they sometimes wanted to talk about things that weren't Clairol products.\n\n\"But we paid for this! This is the Clairol site!\" Doesn't matter. The users are there for one another. They may be there on hardware and software paid for by you, but the users are there for one another.\n\nThe patterns here, I am suggesting, both the things to accept and the things to design for, are givens. Assume these as a kind of social platform, and then you can start going out and building on top of that the interesting stuff that I think is going to be the real result of this period of experimentation with social software.\n\nThank you very much."},
{"url": "https://boingboing.net/2015/10/06/reputation-economy-dystopia-c.html", "link_title": "China's new \u201cCitizen Scores\u201d will rate every person in the country (2015)", "text": "Reputation Economy Dystopia: China's new \"Citizen Scores\" will rate every person in the country\n\nThe Chinese government has announced a new universal reputation score, tied to every person in the country's nation ID number and based on such factors as political compliance, hobbies, shopping, and whether you play videogames.\n\nIt's a perfect storm of terrible: the program will be administered by Alibaba (China's answer to Amazon) and Tencent (the country's huge, government-compliant social network). Your score will be generated not only by your activities, but by the activities of the friends in your social graph -- the people you identify as friends on social media. Your score will be decremented for doing things like mentioning Tienanmen Square or speculating on official corruption, or for participating in activities that the state wishes to \"nudge\" you away from, like playing video-games.\n\nAll scores are public to everyone, and high-scoring individuals will get privileges denied to their less fortunate peers, such as permits to visit (or live) in Singapore (you can't make this shit up).\n\nAlready, some Chinese people are embracing their scores as bragworthy.\n\nPaternalism, surveillance, social control, guilt by association, paternalistic application of behavioral economics and ideology-driven shunning and isolation -- it's like someone took all my novels and blended them together, and turned them into policy (with Chinese characteristics).\n\n* Everybody is measured by a score between 350 and 950, which is linked to their national identity card. While currently supposedly voluntary, the government has announced that it will be mandatory by 2020. * The system is run by two companies, Alibaba and Tencent, which run all the social networks in China and therefore have access to a vast amount of data about people\u2019s social ties and activities and what they say. * In addition to measuring your ability to pay, as in the United States, the scores serve as a measure of political compliance. Among the things that will hurt a citizen\u2019s score are posting political opinions without prior permission, or posting information that the regime does not like, such as about the Tienanmen Square massacre that the government carried out to hold on to power, or the Shanghai stock market collapse. * It will hurt your score not only if you do these things, but if any of your friends do them. Imagine the social pressure against disobedience or dissent that this will create. Anybody can check anyone else\u2019s score online. Among other things, this lets people find out which of their friends may be hurting their scores. * Also used to calculate scores is information about hobbies, lifestyle, and shopping. Buying certain goods will improve your score, while others (such as video games) will lower it. * Those with higher scores are rewarded with concrete benefits. Those who reach 700, for example, get easy access to a Singapore travel permit, while those who hit 750 get an even more valued visa. * Sadly, many Chinese appear to be embracing the score as a measure of social worth, with almost 100,000 people bragging about their scores on the Chinese equivalent of Twitter."},
{"url": "https://www.reuters.com/article/us-usa-stocks-idUSKBN1AN1BG", "link_title": "Tesla seeks $1.5B junk bond issue to fund Model 3 production", "text": "NEW YORK (Reuters) - The Dow edged up to its ninth record closing high in a row while the S&P ended slightly higher on Monday, with consumer and technology sector gains offsetting losses in energy.\n\nYet trading volume was relatively light as investors had little reason to make big bets with the U.S. Congress and President Donald Trump on vacation and a stronger-than-expected earnings season drawing to a close.\n\n\"Today there's a lack of conviction either way. There's no reason to be a seller yet and there's no reason to be a buyer at these levels as earnings season winds down and you don't have much in the way of economic news this week,\" said Robert Pavlik, chief market strategist at Boston Private Wealth in New York.\n\nSome investors were looking into underperforming sectors, including retail, anticipating a lift from in-store back-to-school shopping, according to Rick Meckler, president of LibertyView Capital Management in Jersey City, New Jersey.\n\n\"What you're really seeing is very minor sector rotation,\" said Meckler.\n\nRobust second-quarter earnings have boosted the broader market in recent weeks and a strong July employment report on Friday added to positive sentiment.\n\nAnalysts, on average, expect S&P 500 earnings to have expanded 12 percent in the second quarter and project earnings up 9.3 percent for the September quarter, according to Thomson Reuters I/B/E/S.\n\nHowever, the recent run-up has also sparked concerns about stretched valuations.\n\nThe S&P, which is up about 11 percent this year, is trading at 18 times expected earnings, compared to its 10-year average of 14, according to Thomson Reuters Datastream.\n\nThe Dow Jones Industrial Average .DJI rose 25.61 points, or 0.12 percent, to close at 22,118.42. The last time the Dow had nine straight record closes was in February when it boasted 12 in a row.\n\nThe S&P 500 .SPX climbed 4.08 points, or 0.16 percent, to 2,480.91 and the Nasdaq Composite .IXIC added 32.21 points, or 0.51 percent, to end at 6,383.77.\n\nThe S&P's consumer staples index .SPLRCS, up 0.7 percent, and its technology index .SPLRCT, up 0.6 percent were the benchmark's leading sectors on the day.\n\nIn coming days, investors will scrutinize quarterly results from retailers in light of competition from online retailer Amazon.com (AMZN.O).\n\nWal-Mart (WMT.N) shares were up almost 1 percent. Tyson Foods (TSN.N), one of the consumer staples sector's biggest boosts on the day, rose 5.7 percent after the No. 1 U.S. meat processor reported greater-than-expected quarterly profit and sales.\n\nThe energy sector .SPNY led the laggards with a 0.9 percent drop as oil prices LCOc1 CLc1 edged lower on a rebound in production from Libya's largest oil field, along with worries about higher output from OPEC and the United States. [O/R]\n\nDeclining issues outnumbered advancing ones on the NYSE by a 1.08-to-1 ratio; on Nasdaq, a 1.07-to-1 ratio favored advancers.\n\nAbout 5.29 billion shares changed hands on U.S. exchanges compared with the 6.13 billion average for the last 20 sessions."},
{"url": "https://www.theguardian.com/environment/2017/aug/07/usda-climate-change-language-censorship-emails", "link_title": "USDA is censoring use of term 'climate change', emails reveal", "text": "Staff at the US Department of Agriculture (USDA) have been told to avoid using the term climate change in their work, with the officials instructed to reference \u201cweather extremes\u201d instead.\n\nA series of emails obtained by the Guardian between staff at the Natural Resources Conservation Service (NRCS), a USDA unit that oversees farmers\u2019 land conservation, show that the incoming Trump administration has had a stark impact on the language used by some federal employees around climate change.\n\nA missive from Bianca Moebius-Clune, director of soil health, lists terms that should be avoided by staff and those that should replace them. \u201cClimate change\u201d is in the \u201cavoid\u201d category, to be replaced by \u201cweather extremes\u201d. Instead of \u201cclimate change adaption\u201d, staff are asked to use \u201cresilience to weather extremes\u201d.\n\n\n\nThe primary cause of human-driven climate change is also targeted, with the term \u201creduce greenhouse gases\u201d blacklisted in favor of \u201cbuild soil organic matter, increase nutrient use efficiency\u201d. Meanwhile, \u201csequester carbon\u201d is ruled out and replaced by \u201cbuild soil organic matter\u201d.\n\nIn her email to staff, dated 16 February this year, Moebius-Clune said the new language was given to her staff and suggests it be passed on. She writes that \u201cwe won\u2019t change the modeling, just how we talk about it \u2013 there are a lot of benefits to putting carbon back in the sail [sic], climate mitigation is just one of them\u201d, and that a colleague from USDA\u2019s public affairs team gave advice to \u201ctamp down on discretionary messaging right now\u201d.\n\nIn contrast to these newly contentious climate terms, Moebius-Clune wrote that references to economic growth, emerging business opportunities in the rural US, agro-tourism and \u201cimproved aesthetics\u201d should be \u201ctolerated if not appreciated by all\u201d.\n\nIn a separate email to senior employees on 24 January, just days after Trump\u2019s inauguration, Jimmy Bramblett, deputy chief for programs at the NRCS, said: \u201cIt has become clear one of the previous administration\u2019s priority is not consistent with that of the incoming administration. Namely, that priority is climate change. Please visit with your staff and make them aware of this shift in perspective within the executive branch.\u201d\n\nBramblett added that \u201cprudence\u201d should be used when discussing greenhouse gases and said the agency\u2019s work on air quality regarding these gases could be discontinued.\n\nOther emails show the often agonized discussions between staff unsure of what is forbidden. On 16 February, a staffer named Tim Hafner write to Bramblett: \u201cI would like to know correct terms I should use instead of climate changes and anything to do with carbon ... I want to ensure to incorporate correct terminology that the agency has approved to use.\u201d\n\nOn 5 April, Suzanne Baker, a New York-based NRCS employee, emailed a query as to whether staff are \u201callowed to publish work from outside the USDA that use \u2018climate change\u2019\u201d. A colleague advises that the issue be determined in a phone call.\n\nSome staff weren\u2019t enamored with the new regime, with one employee stating on an email on 5 July that \u201cwe would prefer to keep the language as is\u201d and stressing the need to maintain the \u201cscientific integrity of the work\u201d.\n\n\n\nIn a statement, USDA said that on 23 January it had issued \u201cinterim operating procedures outlining procedures to ensure the new policy team has an opportunity to review policy-related statements, legislation, budgets and regulations prior to issuance\u201d.\n\nThe statement added: \u201cThis guidance, similar to procedures issued by previous administrations, was misinterpreted by some to cover data and scientific publications. This was never the case and USDA interim procedures will allow complete, objective information for the new policy staff reviewing policy decisions.\u201d\n\nKaveh Sadeghzadeh of the Natural Resources Conservation Service added that his organisation \u201chas not received direction from USDA or the administration to modify its communications on climate change or any other topic\u201d.\n\nTrump has repeatedly questioned the veracity of climate change research, infamously suggesting that it is part of an elaborate Chinese hoax. The president has started the process of withdrawing the US from the Paris climate agreement, has instructed the Environmental Protection Agency (EPA) to scrap or amend various regulations aimed at cutting greenhouse gases, and has moved to open up more public land and waters to fossil fuel activity.\n\n\n\nThe nomenclature of the federal government has also shifted as these new priorities have taken hold. Mentions of the dangers of climate change have been removed from the websites of the White House and the Department of the Interior, while the EPA scrapped its entire online climate section in April pending a review that will be \u201cupdating language to reflect the approach of new leadership\u201d.\n\n\u201cThese records reveal Trump\u2019s active censorship of science in the name of his political agenda,\u201d said Meg Townsend, open government attorney at the Center for Biological Diversity.\n\n\u201cTo think that federal agency staff who report about the air, water and soil that sustains the health of our nation must conform their reporting with the Trump administration\u2019s anti-science rhetoric is appalling and dangerous for America and the greater global community.\u201d\n\nThe Center for Biological Diversity is currently suing several government agencies, including the EPA and state department, to force them to release information on the \u201ccensoring\u201d of climate change verbiage.\n\n\n\nWhile some of the changes to government websites may have occurred anyway, the emails from within the USDA are the clearest indication yet that staff have been instructed to steer clear of acknowledging climate change or its myriad consequences.\n\nUS agriculture is a major source of heat-trapping gases, with 15% of the country\u2019s emissions deriving from farming practices. A USDA plan to address the \u201cfar reaching\u201d impacts of climate change is still online.\n\nHowever, Sam Clovis, Trump\u2019s nomination to be the USDA\u2019s chief scientist, has labeled climate research \u201cjunk science\u201d.\n\nLast week it was revealed that Clovis, who is not a scientist, once ran a blog where he called progressives \u201crace traders and race \u2018traitors\u2019\u201d and likened Barack Obama to a \u201ccommunist\u201d."},
{"url": "https://theoutline.com/post/2063/sam-altman-united-slate", "link_title": "A Silicon Valley kingmaker wants to fix what tech did to California", "text": "Sam Altman, 32, is sitting here in a Y Combinator conference room in San Francisco wearing a blue striped shirt and jeans, both knees pulled up to his chest and his shoes on the chair. One arm is wrapped around his left leg, showing off his zebra print Yeezys.\n\nAltman, the president of Y Combinator \u2014 Silicon Valley\u2019s most prestigious startup incubator \u2014 pulled out his iPhone to walk me through the 10 policy goals of The United Slate, his new political advocacy project aimed at introducing new political candidates at the state and federal level. It\u2019s \u201cbasically an effort to find five-ish candidates to run in California in 2018 on the same set of policies,\u201d Altman said.\n\nThrough Y Combinator, Altman funds technology startups and provides mentorship to its founders. Y Combinator's success stories include Dropbox, Airbnb, and Twitch, and it claims the combined valuation of its companies exceeds $80 billion. The acceleration of these companies contributed to rising living costs in the Bay Area, which is rapidly dividing into two classes of techie haves and non-techie have nots. Therefore, Altman seems to have a sense of responsibility \u2014 guilt? noblesse oblige? \u2014 when it comes to tackling inequality in the region. This led to his passion project, The United Slate.\n\nAltman, like many liberal Bay Area techies including Mark Zuckerberg, was shaken by the election of Donald Trump. After November 9, Altman went soul searching. He took a tour of the country and California, interviewing 100 Trump supporters and publishing his findings. He vocally criticized Trump, but at the same time, he refused to cut ties with friend and billionaire Trump surrogate Peter Thiel, who works as a part-time partner at Y Combinator.\n\nThis was the beginning of Altman\u2019s poking into politics. In the kitchen where catered lunch is being served to Y Combinator employees, Altman explained that the cost of housing is one of the biggest problems in California. Leading me into the conference room, he told me it\u2019s something he\u2019d \u201creally like to fix.\u201d The cost of housing has skyrocketed not just in San Francisco where the rise of multi-billion dollar technology companies has driven lower income people out, but in all of California. In January 2012 in Southern California the median home price was $260,000. Today it is half a million dollars.\n\nJust earlier this year, Altman himself was mulling a run for governor, and he\u2019s certainly thinking about what he\u2019d do if he were Governor. \u201cSo like, personally, if I were Governor,\u201d Altman told me, \u201cI would not do the bullet train, and I would take that hundred billion dollars and I would spend it all on local rapid transit.\u201d Altman is talking about the multi-billion dollar California High Speed Rail train that will connect San Francisco and Los Angeles. \u201cThis is another thing we heard from regular people, they don't actually need to go from SF to LA every weekend, but they would really like it if they don\u2019t have to spend 95 minutes in traffic every day, each way.\u201d\n\nSo: how does Altman get people to vote for these candidates? For him, it all comes back to building better tech. \u201cOne thing I can help with is, just, building really good software to help people run really great online campaigns,\u201d he said. \u201cI think that\u2019s been terrible.\u201d\n\nWhen I brought up the fact that Trump and Cambridge Analytica, the online data firm working on behalf of the Trump campaign, seemed to have done a pretty good job online campaigning, he demurred. \u201cHe did pretty good, but I think the sad thing about Trump is I think he just mostly won on message,\u201d he said. \u201cI don\u2019t think he was individually targeting people. I think the era of everyone seeing the same ad for anything is over, and, you know, there\u2019s a lot of people who are really great at targeted online marketing, but not politicians.\u201d\n\nAs for cash, The United Slate isn\u2019t set up as a PAC. It\u2019s more of a branding exercise than a fundraising one, and candidates will run their own campaigns and raise their own funds. Hundreds of candidates have reached out to him expressing their interest in running with The United Slate, he said. The platform encompasses disparate goals, but overall, it\u2019s aimed at reducing inequality: its three guiding principles are personal liberty, prosperity from technology, and economic fairness.\n\nPortions of our interview with Sam Altman are on our daily podcast, The Outline World Dispatch. Subscribe on Apple Podcasts or wherever you listen.\n\nOne of his proposals is Medicare for All, where Altman proposes dropping the age of Medicare availability over time. Another point of the platform is expanding social programs, where Altman suggests researching Universal Basic Income. (Y Combinator has already begun work on a UBI project in Oakland.) \u201cWe should set a goal of eliminating poverty in the country,\u201d Altman writes on his website. \u201cI\u2019m not yet sure what a reasonable timeframe for this goal is, but I do feel a moral obligation to figure out how to do it.\u201d Unions aren\u2019t working, Altman says, and wages are stagnant, so we need something better, even though he isn\u2019t sure what that is yet.\n\nThe one Altman cares about the most, he tells me, is government investment in research and development. \u201cLike, right now we\u2019re set up to fight World War II again, very unlikely that\u2019s what we need to be really good at, and we should really be investing hard in biotechnology, artificial intelligence, cybersecurity.\u201d So Altman has proposed shifting 10 percent of the annual defense budget into \u201cthe research and development of future technologies.\u201d\n\nThere\u2019s also a clean energy target of 90 percent by 2050; Altman says we should get rid of the clean energy subsidies and just impose a carbon tax. His plan includes an increased tax rate for capital gains, calls for an end to corporate tax shelters, and a tax on land ownership, which Altman sees as the fundamental way wealth inequality builds up over time.\n\nIt\u2019s hard to pin down where exactly Altman lies on the political spectrum. He\u2019s decidedly on the left, and has gone so far as to compare Trump to Hitler. He also breaks with Silicon Valley libertarianism by wanting a requirement that the children of California representatives must go to public school. Altman went to a \u201cnot great\u201d public school, he explains, \u201cand I think if the people that are setting the rules don't feel that, they won't fix it.\u201d\n\nA lot of Altman's ideas are broad and vague, he\u2019s not really sure at what rate he\u2019d like the Medicare eligibility rate to drop for example, but that\u2019s probably the point. He\u2019s trying to find candidates that identify with these ideals, and the details can get worked out later.\n\n\u201cA democracy needs everyone's lives to get better every year, and I think prosperity is how we do that,\u201d Altman explains. \u201cWe have this prosperity machine in the U.S., we've created huge amount of wealth and its not distributed very well.\u201d Does that everyone include people who are already rich? \u201cYeah, people who are already rich too,\u201d Altman says.\n\nThe United Slate is just three people: Altman; Matt Krisiloff, who works as the Director of Research at Y Combinator; and his brother Scott Krisiloff, a mutual fund manager, who are helping conduct focus groups around California. Altman isn\u2019t trying to build some big operation he warns, he\u2019s simply interested to see if he can advance the political wish list he\u2019s dreamed up.\n\n\u201cWhat I hope more than anything else is that if there\u2019s a group of people running together, and they say \u2018Hey, all five of us we\u2019re running as a slate and here\u2019s this new set of things we believe, if you want this vote for us all.\u2019 No one has tried that in a while, but I hope there will be this network effect.\u201d"},
{"url": "https://www.theguardian.com/technology/2017/aug/07/silicon-valley-google-diversity-black-women-workers", "link_title": "Segregated Valley: the ugly truth about Google and diversity in tech", "text": "Google has spent much of the past 72 hours insisting its commitment to diversity is \u201cunequivocal\u201d after the internal publication and subsequent leak of an anti-diversity polemic by a Google engineer. The unidentified software engineer argued, among other things, that biological differences between men and women account for the extreme gender imbalance at Google and other technology companies.\n\n\u201cWe are unequivocal in our belief that diversity and inclusion are critical to our success as a company,\u201d said Danielle Brown, Google\u2019s vice-president of diversity, integrity and governance.\n\n\u201cBuilding an open, inclusive environment is core to who we are, and the right thing to do,\u201d added Ari Balogh, the company\u2019s vice-president of engineering, \u201c\u2018Nuff said.\u201d\n\nGoogle might prefer the discussion to end there, but the reality is there is a lot more to say about the company\u2019s commitment to diversity.\n\nThe public relations blitz may be a corporate necessity given the virulent backlash against the document by many of Google\u2019s own employees. On Monday night, Bloomberg reported that the engineer said he had been fired; Google declined to comment on individual employee cases.\n\nBut public commitments to diversity from Google executives do not tally with the company\u2019s workforce data.\n\nGoogle\u2019s workforce is, by its own accounting, 69% male and just 2% African American. Just 20% of technical jobs are held by women. Google may be unequivocal in its \u201cbelief\u201d about diversity, but the figures make its shortcomings clear. The company tends to hire white and Asian men over women and other racial minorities.\n\nLack of diversity in Silicon Valley is an old story. Eighteen years ago, civil rights leader Jesse Jackson first launched a campaign to encourage the region\u2019s tech companies to hire black and Latino workers. At the time, he was accused of \u201cterrorism\u201d by Scott McNealy, the co-founder of early Silicon Valley giant Sun Microsystems.\n\nTech leaders may have changed their tune in the intervening decades \u2013 all the top CEOs today loudly proclaim a commitment to \u201cdiversity and inclusion\u201d \u2013 but in other ways not much has changed in almost two decades.\n\nMcNealy, now the chairman of a digital marketing startup, stands by his statements on Jackson, though he concedes that \u201cterrorism\u201d might have been an overstatement. \u201cProbably the right word is blackmail,\u201d he told the Guardian. \u201cI just don\u2019t have time for race baiters. Stop baiting me.\u201d\n\nGoogle is the subject of an investigation by the US Department of Labor, which has accused the technology corporation of systematically discriminating against women (the company denies the charge.) Much of Uber\u2019s top tier of executives has left the company amid complaints of systematic sexual harassment and gender discrimination. And the tech industry has lately been shaken by allegations that high-profile venture capitalists have abused their position to prey on female startup entrepreneurs.\n\nMeanwhile, the representation of black, Latino, and female employees at top Silicon Valley technology firms remains so disproportionately low that a government report published last year described the problem with the same word that Jackson uses: \u201csegregation\u201d. For all its forward looking technologies, Silicon Valley is in many ways mired in the ugliest practices of the American past.\n\nPicture a technology hub where more than 17% of high-tech workers \u2013 from programmers to security analysts to software and web developers \u2013 are African American.\n\nThis isn\u2019t some kind of utopian diversity thought experiment. It is the greater Washington DC metropolitan area, home to more than 200,000 high tech jobs, many of them with the federal government or government contractors.\n\n\u201cYou\u2019d be hard pressed to have someone out here who thinks that blacks doing computer work is weird,\u201d said William Spriggs, a professor of economics at Howard University. And lest you think that the computing in DC is less advanced than that in Silicon Valley, he adds: \u201cWe don\u2019t do Mickey Mouse stuff out here. This is the number one place if you want to do cyber security.\u201d\n\nThe DC area is a kind of mirror image to Silicon Valley when it comes to hiring African Americans. Overall, blacks make up 14.4% of the workforce nationwide and 7.4% of high-tech employment. In the DC metro area, which includes parts of Virginia, Maryland, and West Virginia, blacks hold 17.3% of the jobs in 12 computing occupations, according to government employment data.\n\nBut cross over to the west coast, and in Silicon Valley African Americans hold just 2.7% of the jobs in the same categories. At premiere employers like Google and Facebook, black representation in technical jobs drops below 2%.\n\nTo Spriggs, there is simply no excuse for Silicon Valley\u2019s failure to hire a more diverse workforce. \u201cThe thing that always irritates me is that they say, \u2018We can\u2019t find them,\u2019\u201d he said. \u201cYou run a freaking search engine!\u201d\n\nSo how did Silicon Valley end up with fewer than 5,000 black people in highly technical jobs, while DC has more than 35,000?\n\nOne obvious difference between northern California and the mid-Atlantic region is the underlying demographics. The DC metro area is approximately 25% black, while Silicon Valley is about 6.5% black.\n\nBut companies like Google, Facebook and Apple are known to recruit aggressively across the country \u2013 and throughout the world. And the fact that northern California\u2019s workforce is heavily Latino (more than 20%) is not reflected in the area\u2019s tech companies (about 6% Latino).\n\nSpriggs argued that a significant difference is that in DC, the tech industry grew up around the federal government. Affirmative action provisions for federal contracting encouraged African Americans to start businesses in computing or data processing in the late 1970s and early 1980s. The first domain name registrar for the internet, for example, was the black-owned company, Network Solutions, which was founded in northern Virginia in 1979.\n\n\u201cHaving black-owned companies helped get people in,\u201d Spriggs said. \u201cIt\u2019s partly entrepreneurship, partly because the federal government does not discriminate, partly because you have to have [security] clearance, which favors American citizens, and partly because the area is heavily black.\u201d\n\nSchools in the region focused on preparing their students for technology jobs with government contractors as well.\n\n\u201cThe industry [has been] reaching out to and working with the historically black colleges and universities in the area,\u201d said Ben Jealous, the former president of the NAACP who is now a partner at venture capital firm Kapor Capital. \u201cMorgan State, Virginia State, the University of Baltimore Maryland \u2013 all of those schools are Stem schools that have focused on providing people to legacy tech companies.\u201d\n\nThe relationships are advanced enough, Jealous said, that companies will inform universities what kind of skills they project needing five-years out, so that curricula can be adapted to ensure a trained workforce.\n\nThe result is a technology hub that looks like what Silicon Valley, a supposed wellspring of innovation, claims to want. Yet it is not just demographics that differ 3,000 miles away \u2013 the politics is different too.\n\nWhere the DC tech industry grew in a symbiotic relationship with government, many of the pioneers of Silicon Valley were techno-libertarians, ideologically opposed to government regulation and oversight.\n\nThe political climate in Silicon Valley in the 1990s was anti-government, anti-affirmative action, and anti-immigrant, said Butch Wing, who has worked with Jesse Jackson on the Rainbow Push Coalition\u2019s Silicon Valley project, an effort to increase the participation of people of color in tech, since 1999.\n\nCalifornia voters approved statewide ballot initiatives to ban affirmative action in college admissions and bar undocumented immigrants from going to public schools or access public services in 1996 and 1994, respectively.\n\nTech leaders like McNealy and Cypress Industries CEO TJ Rodgers were openly hostile to the idea of consciously diverse hiring. Rodgers wrote an op-ed in the San Jose Mercury News in 1999 declaring that \u201cthe only sharecropper I know is my dad\u201d and calling Jackson \u201ca hustler who exploits white shame for his own financial and political ends\u201d.\n\nIt\u2019s a far cry from the Silicon Valley of 2016, where Facebook erected a banner reading \u201cBlack Lives Matter\u201d in the center of its Menlo Park headquarters and all the major employers have hired diversity and inclusion executives.\n\nAnd yet, Wing suspects that the same ideology lingers.\n\nFacebook CEO Mark Zuckerberg, who last year chastised his employees for crossing out \u201cBlack Lives Matter\u201d on the walls and replacing it with \u201cAll Lives Matter\u201d, is also friends with and was reportedly mentored by Peter Thiel, the Pay-Pal co-founder who co-authored an anti-multiculturalism screed, The Diversity Myth, in 1995.\n\n\u201cI\u2019ve not seen a single editorial against diversity since 2014,\u201d said Wing. \u201cCommunications-wise, companies today will all say diversity is in their DNA. There\u2019s a kind of love fest with diversity and inclusion, but few companies are making any actionable, measureable progress on hiring underrepresented minorities and overcoming the \u20182% dilemma\u2019.\u201d\n\nAt the top 75 companies in Silicon Valley, only 3% of employees are black, according to the Equal Employment Opportunity Commission. Premier employers Facebook and Google have yet to crack 2% in technical jobs.\n\nFacing renewed pressure from Jackson and other activists in 2014, the top firms began releasing data about their workforces and publicly committing to diversity programs.\n\nBut as the companies continue to grow, their hiring of underrepresented minorities has remained too small to make a real impact. The numbers of employees they would need to hire to achieve anything close to proportional representation keeps getting larger.\n\nAny discussion about diverse hiring in Silicon Valley inevitably becomes a discussion about \u201cthe pipeline\u201d. The pipeline, the story goes, is the steady stream of able and willing workers that are pumped out of colleges and universities each year, computer science degrees in hand, ready to populate the tech company campuses that dot the suburbs of the Bay Area.\n\nIt\u2019s not the fault of tech companies that the pipeline is overwhelmingly filled with white and Asian people, Silicon Valley\u2019s defenders claim. It\u2019s the fault of the education system. \u201cMinorities are the minority by far in computing programs,\u201d conceded Dr Juan Gilbert, chair of the computer science and engineering department at the University of Florida.\n\nBut there\u2019s a problem with that argument: black students are earning computer science degrees at higher rates than they are being hired by Silicon Valley companies. In 2014, they received 9.7% of the bachelor degrees awarded in computer science, according to the National Science Foundation.\n\n\u201cIf the pipeline doesn\u2019t lead anywhere, then all that work is for nought,\u201d said Catherine Bracy, co-founder of the TechEquity Collaborative. \u201cThe people who come out of those programs need to be able to find jobs in the industry.\u201d\n\nSilicon Valley companies don\u2019t want students with computer science degrees from just anywhere, said Leslie Miley, the director of engineering at Slack. The founders and hiring managers of Silicon Valley companies want students with degrees from the same schools they went to.\n\n\u201cHow difficult do you think it would be to go to an engineering meeting and tell all these people who went to Cal, Stanford, and MIT that the person coming from the University of Texas El Paso or a community college can do their job as well as they can?\u201d Miley asked. \u201cYou will not be able to convince them of that. They don\u2019t want to believe that they\u2019re not special.\u201d\n\nThe preference for an elite resume severely restricts the so-called pipeline \u2013 and results in a much less diverse group of candidates for Silicon Valley jobs.\n\nIn 2014, Wired analyzed LinkedIn profiles to come up with a list of the top five feeder universities for Microsoft, IBM, Google, Apple, Yahoo, Facebook, and Twitter. The thirteen US universities included the elite private schools Stanford, Carnegie Mellon, and MIT, as well as public schools such as UC Berkeley and the University of Washington.\n\nOne thing the schools had in common were student bodies with significantly fewer African-American students than the national average for four-year universities of 14%. Stanford had the highest rate of black students, at 7.8%.\n\nBasic network factors also likely contribute to tech\u2019s failure to find the underrepresented minorities in the pipeline.\n\n\u201cTech is heavily referral-based, not just for jobs, but for funding,\u201d said Y-Vonne Hutchinson, a diversity consultant and one of the founding members of Project Include. \u201cFor white people, 90% of their networks are white. You have this exponential deepening impact of homogeneity.\u201d\n\nGilbert, whose program has more black computer science PhD students than anywhere else in the country, said that he\u2019s also seen black undergraduates self-select out of Silicon Valley, in favor of working for companies on the east coast.\n\n\u201cAmongst the African American students that I engage with, I don\u2019t hear many of them aspiring to work at Google.\u201d he said. \u201cThey hear Silicon Valley and they think, \u2018I\u2019m not going to see people like me.\u2019\u201d"},
{"url": "http://www.apa.org/news/press/releases/2014/08/pushed-back.pdf", "link_title": "Leaning In, but Getting Pushed Back (and Out) (2014) [pdf]", "text": ""},
{"url": "https://www.wired.com/story/army-dji-drone-ban", "link_title": "The Army Grounds Its DJI Drones Over Security Concerns", "text": "The US Army has increasingly used small consumer drones in the field , purchasing them as needed from consumer manufacturers like the well-known Chinese maker DJI. But documents indicate that the Army Aviation Directorate is now enforcing new orders, banning DJI drones \u201cdue to increased awareness of cyber vulnerabilities associated with DJI products.\u201d\n\nThe documents, first obtained by Small UAS News, don\u2019t explain the Army\u2019s security concern, but refer to classified studies about DJI drones that first went out at the end of May. Previously, hackers have been able to jailbreak some DJI drones to control and modify things like safety features on the devices. Some reports have also indicated that DJI can gather location, audio, and even visual data from user flights. It's unclear what data DJI can access without customer consent, but location and media data from an Army drone could potentially reveal extensive information about US military operations. Even if the Army isn't specifically concerned about DJI or the Chinese government accessing this data, it may be worried that other parties could intercept any data linked to DJI.\n\nAn Army spokesperson told WIRED in a statement, \u201cWe can confirm that guidance was issued; however, we are currently reviewing the guidance and cannot comment further at this time.\u201d The guidance points to two US military reports, one from the Army Research Laboratory titled \u201cDJI UAS Technology Threat and User Vulnerabilities\u201d and one from the Navy called \u201cOperational Risks with Regards to DJI Family of Products.\"\n\nDJI has said in the past that it doesn\u2019t track devices, and can\u2019t access unit audio or video feeds. But the company is at least able to make its drones comply with no fly zones around the world, one of the administrative capabilities that has motivated customers to hack the drones in the past. Drone owners have even developed jailbreaks for DJI devices so they can override safety controls like flight elevation maximums. DJI says that how much information it can access about a particular user hinges on what data sharing that customer has granted, particularly through DJI mobile apps. An April 2016 privacy policy notes that \u201cDJI Products and Services connect to servers hosted in the United States, China, and Hong Kong.\u201d\n\n\u201cDJI makes civilian drones for peaceful purposes,\u201d a DJI spokesperson said in a statement. \u201cWe do not market our products for military customers, and if military members choose to buy and use our products as the best way to accomplish their tasks, we have no way of knowing who they are or what they do with them. The US Army has not explained why it suddenly banned the use of DJI drones and components, what \u2018cyber vulnerabilities\u2019 it is concerned about, or whether it has also excluded drones made by other manufacturers.\u201d\n\nThough the Army\u2019s specific concerns about DJI remain unknown, the situation is reminiscent of mounting international suspicion over state use of consumer products developed abroad. These tensions have particularly escalated between the US and Russia in the past few years, with Moscow routinely demanding access to software source code to check security products from US companies like IBM and Cisco. Recently the US government has angled for similar access from the Russian antivirus maker Kaspersky Lab. Cybersecurity defense tools would be a particular liability if they were sabotaged by foreign adversaries, but any digital product that generates or accesses sensitive data\u2014like, say, a military drone\u2014is a potential weakness.\n\nAlthough DJI is a Chinese company, the Army may be more concerned about broad exposure to data hijacking, if the wording of the directive gives any clues. \"Cease all use, uninstall all DJI applications, remove all batteries/storage media from devices, and secure equipment for follow on direction,\" the missive reads. Its comprehensive nature may indicate fears about data interception or spyware, or could stem from a military penetration test into DJI equipment that unearthed a bug the Army doesn't want others finding and exploiting. That wouldn't be without precedent; in the late 2000s, terrorists famously intercepted unencrypted predator drone video feeds.\n\nIt wouldn't necessarily be surprising if the US military concluded that a consumer-grade product was inadequate for military use. As DJI itself notes, the company's drones aren't made with warfare in mind, and mainstream products generally aren't hardened the way military technology is. But the thoroughness of the Army's DJI recall certainly raises questions about what specifically they found\u2014and whether consumer privacy or security could be at risk as well."},
{"url": "http://blog.indeed.com/2017/05/02/what-employers-think-about-coding-bootcamp/", "link_title": "What Do Employers Really Think About Coding Bootcamps?", "text": "Today, technological innovation impacts every industry, creating massive demand for employees with tech skills. But good candidates are notoriously hard to find, and universities just aren\u2019t producing enough STEM grads.\n\nEnter coding bootcamps: an alternative education model that offers a new way to satisfy the tech talent shortage. Does the rapid rise of these fast-track, high-impact courses indicate that employers now view them as a serious alternative to a traditional four-year computer science degree? And just how prepared are bootcamp graduates to compete for those highly paid tech jobs after only two or three months of training?\n\nTo find out the answers, Indeed conducted a survey of over 1000 HR managers and technical recruiters at US companies of all sizes. Here\u2019s what they told us.\n\nSo just what do employers think about bootcamp graduates? It turns out that they hold them in pretty high esteem.\n\nAn impressive 72% of respondents consider bootcamp grads to be just as prepared and just as likely to perform at a high level than computer science grads. Some go further: 12% think they are more prepared and more likely to do better. By contrast, only 17% have doubts.\n\nLittle wonder, then, that 80% of respondents have actually gone ahead and hired a coding bootcamp graduate for a tech role within their company. Meanwhile, satisfaction levels are high: The overwhelming majority (99.8%) say they would do so again.\n\nWith attitudes as favorable as this, it seems undeniable that bootcamps are an idea whose time has come. So for job seekers with the skills and interest, coding bootcamp is \u201cworth it.\u201d But is the message getting through to candidates?\n\nIn fact, awareness of the opportunities provided by these courses is clearly spreading: 86% of respondents say that applications from bootcamp grads have gone up over the last few years. That\u2019s not all: Since 2010, Indeed has seen a doubling of year-over-year growth of job seekers with bootcamp experience in our resume database.\n\nBut bootcamps can do more than just help narrow the supply-demand gap. A recent Indeed survey of diversity in tech found that 77% of respondents considered it \u201cvery or quite important\u201d to have a diverse company. Here bootcamps can also play a role: According to respondents, 51% of surveyed companies said that hiring bootcamp grads is a good way to help job seekers from underrepresented groups find work in the technology sector.\n\nSome bootcamps, such as Ada Developers Academy, are designed specifically to address tech\u2019s lack of diversity. Ada\u2019s tuition-free model offers 6 months of full-time classroom training, followed by five months in a paid industry internship at a top employer.\n\nMeanwhile, half of respondents (50%) said they were a good way to retrain workers who either don\u2019t have college degrees or those who have lost jobs and could benefit from re-training.\n\nIn 2016 there were 91 recognized, full-time bootcamps with an estimated 18,000 graduates, according to the coding bootcamp directory Course Report. However, although they clearly fill a pressing need for employers, the fact remains that bootcamps are not currently regulated or accredited. Meanwhile, despite employer enthusiasm for the model, 98% of respondents want to see regulation.\n\nIt is easy to understand why. Not all bootcamps are created equal, and many factors contribute to the quality of a program and its ability to both provide employers with qualified candidates and gain graduates entry into jobs.\n\nFor instance, some bootcamps are strictly online, which can limit teacher-student interactions. There are bootcamps that only teach certain languages or front-end versus back-end development, while others teach both front- and back-end, and still others that specialize in fields as complicated as data science. Some have promises of 100% job placement post-graduation, while others even throw in salary guarantees for projected job placements.\n\nAwareness of the lack of a common set of standards has led some bootcamps to attempt self-regulation. In March, TechCrunch reported that a group of coding bootcamps had joined together to form the Council on Integrity in Results Reporting, with the intention of improving transparency in salary and graduation data. But will it take off? Only time will tell.\n\nThis lack of a common set of standards may help explain why 